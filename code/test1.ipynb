{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1=\"Extract the words from the following texts: \"\n",
    "prompt2=\"Translate the following text from <source-language> into <target-language>: <source-sentence>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3='''\n",
    "The chained multilingual dictionaries:\n",
    "<word X in source-language> means <word X in target-language> means \n",
    "<word X in auxiliary-language 1> means <word X in auxiliary-language 2>.\n",
    "Translate the following text from <source-language> into <target-language>: <source-sentence>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/flores\", \"ace_Arab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "\n",
    "# model_name = \"princeton-nlp/gemma-2-9b-it-SimPO\"\n",
    "# cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# # Load model and tokenizer with optimization\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     cache_dir=cache_directory,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools import tool\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tool_descriptions}\n",
    "\n",
    "When using a tool, follow this format:\n",
    "Thought: what you want to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "\n",
    "When you have the answer, follow this format:\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the question\n",
    "\n",
    "Begin!\n",
    "\n",
    "{input}\n",
    "\"\"\"\n",
    "custom_prompt = PromptTemplate(template=prompt_template, input_variables=[\"tool_descriptions\", \"tool_names\", \"input\"])\n",
    "\n",
    "# Define a sample tool\n",
    "@tool\n",
    "def calculator(query: str) -> str:\n",
    "    \"\"\"Perform basic arithmetic calculations.\"\"\"\n",
    "    try:\n",
    "        result = eval(query)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Instantiate the LLM\n",
    "# from transformers import pipeline\n",
    "# pipe = pipeline(\"text-generation\",\n",
    "#                 model=model,\n",
    "#                 tokenizer= tokenizer,\n",
    "#                 torch_dtype=torch.bfloat16,\n",
    "#                 device_map=\"auto\",\n",
    "#                 max_new_tokens = 512,\n",
    "#                 do_sample=True,\n",
    "#                 top_k=30,\n",
    "#                 num_return_sequences=1,\n",
    "#                 eos_token_id=tokenizer.eos_token_id\n",
    "#                 )\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "\n",
    "# Define tools\n",
    "tools = [Tool(name=\"Calculator\", func=calculator, description=\"Use to solve math problems\")]\n",
    "\n",
    "# Initialize the agent\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    agent_kwargs={\"custom_prompt\": custom_prompt}\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "# response = agent.run(\"What is 12 plus 8 divided by 2?\")\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSeq2SeqLM\n",
    "# model_name = \"facebook/nllb-200-3.3B\"\n",
    "# cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# # Load model and tokenizer with optimization\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     cache_dir=cache_directory,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import HuggingFacePipeline, pipeline\n",
    "# pipe = pipeline(\"text-generation\",\n",
    "#                 model=model,\n",
    "#                 tokenizer= tokenizer,\n",
    "#                 torch_dtype=torch.bfloat16,\n",
    "#                 device_map=\"auto\",\n",
    "#                 max_new_tokens = 512,\n",
    "#                 do_sample=True,\n",
    "#                 top_k=30,\n",
    "#                 num_return_sequences=1,\n",
    "#                 eos_token_id=tokenizer.eos_token_id\n",
    "#                 )\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template=\"\"\" Given the sentence {sentence} I want to extract important keywords from the sentence. Your answer should contain only list of important keywords.\n",
    "\"\"\"\n",
    "prompt_template=PromptTemplate(\n",
    "    template=template, input_variables=['sentence']\n",
    ")\n",
    "llm_chain = prompt_template | llm\n",
    "ans=llm_chain.invoke(\"Apple is looking at buying U.K. startup for $1 billion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans.content.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template=\"\"\" Given the list of words {sentence}. I want to translate each keywords from the sentence to three languages: Bangla, French, German and Portuguese. Answer will be provided in dictionary format and answer have only dicionary, nothing else and remove extra syntax.\n",
    "\"\"\"\n",
    "prompt_template=PromptTemplate(\n",
    "    template=template, input_variables=['sentence']\n",
    ")\n",
    "llm_chain = prompt_template | llm\n",
    "ans=llm_chain.invoke(['Apple', ' buying', ' U.K.', ' startup', ' $1 billion'])\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "parsed_data = json.loads(ans.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_chained_format(dictionary, src_lang, target_lang):\n",
    "    result = []\n",
    "    for word, translations in dictionary.items():\n",
    "        chain = f\"{word} in {src_lang} means {translations.get(target_lang, 'N/A')} in {target_lang}\"\n",
    "        for aux_lang, aux_translation in translations.items():\n",
    "            if aux_lang != target_lang:\n",
    "                chain += f\" means {aux_translation} in {aux_lang}\"\n",
    "        result.append(chain + \".\")\n",
    "    return result\n",
    "\n",
    "# Convert and print the chained multilingual format\n",
    "src_language = \"English\"\n",
    "target_language = \"Bangla\"\n",
    "chained_output = convert_to_chained_format(parsed_data, src_language, target_language)\n",
    "chained_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3='''\n",
    "The chained multilingual dictionaries:\n",
    "<word X in source-language> means <word X in target-language> means \n",
    "<word X in auxiliary-language 1> means <word X in auxiliary-language 2>.\n",
    "Translate the following text from <source-language> into <target-language>: <source-sentence>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\" \n",
    "{chained_dict}.\n",
    "Translate the following text from {src_lng} into {trg_lng}: {src_sent}\n",
    "\n",
    "\"\"\"\n",
    "input_data = {\n",
    "    'chained_dict': chained_output,\n",
    "    'src_lng': 'English',\n",
    "    'trg_lng': 'Bangla',\n",
    "    'src_sent': 'Apple is looking at buying U.K. startup for $1 billion.'\n",
    "}\n",
    "\n",
    "prompt_template=PromptTemplate(\n",
    "    template=template, input_variables=['chained_dict','src_lng','trg_lng','src_sent']\n",
    ")\n",
    "llm_chain = prompt_template | llm\n",
    "ans=llm_chain.invoke(input_data)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ner(sentence):\n",
    "    # Process the sentence with spaCy\n",
    "    doc = nlp(sentence)\n",
    "    ner_words = [{\"word\": ent.text, \"Entity name\":ent.label_} for ent in doc.ents]\n",
    "    return ner_words\n",
    "extract_ner(\"Apple is looking at buying U.K. startup for $1 billion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import (\n",
    "    create_react_agent,\n",
    "    AgentExecutor,\n",
    ")\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "sent=\"Apple is looking at buying U.K. startup for $1 billion.\"\n",
    "template=\"\"\" Given the sentence {sentence} I want to extract important keywords from the list. Your answer should contain only list of important keywords.\n",
    "\"\"\"\n",
    "prompt_template=PromptTemplate(\n",
    "    template=template, input_variables=['sentence']\n",
    ")\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_for_agent=[\n",
    "    Tool(\n",
    "        name=\"Extract the important keywords\",\n",
    "        func=extract_ner,\n",
    "        description=\"Useful when I want to extract the important keywords from the sentence\"\n",
    "    )\n",
    "]\n",
    "react_prompt=hub.pull(\"hwchase17/react\")\n",
    "agent=create_react_agent(llm=llm,tools=tools_for_agent,prompt=react_prompt)\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools_for_agent,verbose=True)\n",
    "result=agent_executor.invoke(\n",
    "    input={\"input\":prompt_template.format_prompt(sentence=sent)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "keyword_relationships_path = \"/home/mshahidul/project1/all_tran_data/prompt_info/keyword_relationships_umls.json\"\n",
    "with open(keyword_relationships_path, 'r', encoding='utf-8') as keyword_relationships_file:\n",
    "    keyword_relationships = json.load(keyword_relationships_file)\n",
    "\n",
    "def keyword_relationships_umls(sentence):\n",
    "    for keyword_relationship in keyword_relationships:\n",
    "        if keyword_relationship['sentence'] == sentence:\n",
    "            res=keyword_relationship['relationships']\n",
    "            list=[]\n",
    "            for k,v in res.items():\n",
    "                if len(v):\n",
    "                    list.append(f\"{k} is a {v[0]['relation']} to the term '{v[0]['related_concept']}'\")\n",
    "            full_list = ', '.join(list)\n",
    "            return full_list\n",
    "    return None\n",
    "t=keyword_relationships_umls(\"If you have a weakened immune system due to AIDS, cancer, transplantation, or corticosteroid use, call your doctor if you develop a cough, fever, or shortness of breath.\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "keyword_synonymous_path = \"/home/mshahidul/project1/all_tran_data/prompt_info/keyword_synonymous_umls.json\"\n",
    "with open(keyword_synonymous_path, 'r', encoding='utf-8') as keyword_synonymous_file:\n",
    "    keyword_synonymous = json.load(keyword_synonymous_file)\n",
    "def keyword_synonymous_umls(sentence):\n",
    "    for keyword_synonym in keyword_synonymous:\n",
    "        if keyword_synonym['sentence'] == sentence:\n",
    "            res=keyword_synonym['synonymous']\n",
    "            list=[]\n",
    "            for k,v in res.items():\n",
    "                if len(v):\n",
    "                    list.append(f\"The term {k} is synonymous with:\\nPortuguese: '{v['POR']}'; French: '{v['FRE']}'; German: '{v['GER']}'\")\n",
    "            full_list = '\\n'.join(list)\n",
    "            return full_list\n",
    "    return None\n",
    "t=keyword_synonymous_umls(\"If you have a weakened immune system due to AIDS, cancer, transplantation, or corticosteroid use, call your doctor if you develop a cough, fever, or shortness of breath.\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kg_gpt4o_mini_path = \"/home/mshahidul/project1/all_tran_data/prompt_info/kg_gpt4o_mini.json\"\n",
    "with open(kg_gpt4o_mini_path, 'r', encoding='utf-8') as kg_gpt4o_mini_file:\n",
    "    kg_gpt4o_mini = json.load(kg_gpt4o_mini_file)\n",
    "def kg_gpt4o_mini_umls(keyword):\n",
    "    for kg in kg_gpt4o_mini:\n",
    "        if kg['keyword'] == keyword:\n",
    "            list=[]\n",
    "            data=kg['kg']\n",
    "            for k,v in data.items():\n",
    "                if k=='keyword':\n",
    "                    continue\n",
    "                value = v[0] if not isinstance(v, str) else v\n",
    "                list.append(f\"'{keyword}' term {k} is '{value}'\")\n",
    "            full_list = '\\n'.join(list)\n",
    "            return full_list\n",
    "    return None\n",
    "t=kg_gpt4o_mini_umls(\"newborn jaundice\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_gpt4o_diff_lang_path = \"/home/mshahidul/project1/all_tran_data/prompt_info/synonyms_gpt4o_diff_lang.json\"\n",
    "with open(synonyms_gpt4o_diff_lang_path, 'r', encoding='utf-8') as synonyms_gpt4o_diff_lang_file:\n",
    "    synonyms_gpt4o_diff_lang = json.load(synonyms_gpt4o_diff_lang_file)\n",
    "\n",
    "def synonyms_gpt4o_diff_lang_umls(keyword):\n",
    "    for syn in synonyms_gpt4o_diff_lang:\n",
    "        if syn['keyword'] == keyword:\n",
    "            list=[]\n",
    "            list.append(f\"Synonyms of '{keyword}':\")\n",
    "            data=syn['synonyms']\n",
    "            for k,v in data.items():\n",
    "                list.append(f\"{k}: {', '.join(v)}\")\n",
    "            full_list = '\\n'.join(list)\n",
    "            return full_list\n",
    "    return None\n",
    "t=synonyms_gpt4o_diff_lang_umls(\"gauze\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_keywords_path = \"/home/mshahidul/project1/all_tran_data/prompt_info/updated_keywords.json\"\n",
    "\n",
    "with open(updated_keywords_path, 'r', encoding='utf-8') as updated_keywords_file:\n",
    "    updated_keywords = json.load(updated_keywords_file)\n",
    "\n",
    "def extract_keywords(sentence):\n",
    "    for updated_keyword in updated_keywords:\n",
    "        if updated_keyword['sentence'] == sentence:\n",
    "            res=updated_keyword['keywords']\n",
    "            return res\n",
    "    return None\n",
    "t=extract_keywords(\"If you have a weakened immune system due to AIDS, cancer, transplantation, or corticosteroid use, call your doctor if you develop a cough, fever, or shortness of breath.\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_keywords_path = \"/home/mshahidul/project1/all_tran_data/prompt_info/translated_keywords_gpt4o_mini.json\"\n",
    "import json\n",
    "with open(translated_keywords_path, 'r', encoding='utf-8') as translated_keywords_file:\n",
    "    translated_keywords = json.load(translated_keywords_file)\n",
    "gpt4o_mini_tran={}\n",
    "for x in translated_keywords:\n",
    "    gpt4o_mini_tran[x['sentence']]=x['translated_keywords']\n",
    "\n",
    "def gpt4o_mini_tran_func(sentence):\n",
    "    res=gpt4o_mini_tran[sentence]\n",
    "    full_list=[]\n",
    "    for k, v in res.items():\n",
    "        t=f\"'{k}' : {v}\"\n",
    "        full_list.append(t)\n",
    "    return \"\\n\".join(full_list)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'weakened immune system' : {'French': 'système immunitaire affaibli', 'Portuguese': 'sistema imunológico enfraquecido', 'German': 'geschwächtes Immunsystem', 'Spanish': 'sistema inmunológico debilitado'}\n",
      "'AIDS' : {'French': 'SIDA', 'Portuguese': 'SIDA', 'German': 'AIDS', 'Spanish': 'SIDA'}\n",
      "'cancer' : {'French': 'cancer', 'Portuguese': 'câncer', 'German': 'Krebs', 'Spanish': 'cáncer'}\n",
      "'transplantation' : {'French': 'transplantation', 'Portuguese': 'transplante', 'German': 'Transplantation', 'Spanish': 'trasplante'}\n",
      "'corticosteroid' : {'French': 'corticostéroïde', 'Portuguese': 'corticosteróide', 'German': 'Kortikosteroid', 'Spanish': 'corticosteroide'}\n",
      "'doctor' : {'French': 'docteur', 'Portuguese': 'médico', 'German': 'Arzt', 'Spanish': 'médico'}\n",
      "'cough' : {'French': 'toux', 'Portuguese': 'tosse', 'German': 'Husten', 'Spanish': 'tos'}\n",
      "'fever' : {'French': 'fièvre', 'Portuguese': 'febre', 'German': 'Fieber', 'Spanish': 'fiebre'}\n",
      "'shortness of breath' : {'French': 'essoufflement', 'Portuguese': 'falta de ar', 'German': 'Atemnot', 'Spanish': 'dificultad para respirar'}\n"
     ]
    }
   ],
   "source": [
    "print(gpt4o_mini_tran_func(translated_keywords[0]['sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weakened immune system {'French': 'système immunitaire affaibli', 'Portuguese': 'sistema imunológico enfraquecido', 'German': 'geschwächtes Immunsystem', 'Spanish': 'sistema inmunológico debilitado'}\n",
      "AIDS {'French': 'SIDA', 'Portuguese': 'SIDA', 'German': 'AIDS', 'Spanish': 'SIDA'}\n",
      "cancer {'French': 'cancer', 'Portuguese': 'câncer', 'German': 'Krebs', 'Spanish': 'cáncer'}\n",
      "transplantation {'French': 'transplantation', 'Portuguese': 'transplante', 'German': 'Transplantation', 'Spanish': 'trasplante'}\n",
      "corticosteroid {'French': 'corticostéroïde', 'Portuguese': 'corticosteróide', 'German': 'Kortikosteroid', 'Spanish': 'corticosteroide'}\n",
      "doctor {'French': 'docteur', 'Portuguese': 'médico', 'German': 'Arzt', 'Spanish': 'médico'}\n",
      "cough {'French': 'toux', 'Portuguese': 'tosse', 'German': 'Husten', 'Spanish': 'tos'}\n",
      "fever {'French': 'fièvre', 'Portuguese': 'febre', 'German': 'Fieber', 'Spanish': 'fiebre'}\n",
      "shortness of breath {'French': 'essoufflement', 'Portuguese': 'falta de ar', 'German': 'Atemnot', 'Spanish': 'dificultad para respirar'}\n"
     ]
    }
   ],
   "source": [
    "for k, v in translated_keywords[0]['translated_keywords'].items():\n",
    "    print(k,v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
