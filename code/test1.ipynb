{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1=\"Extract the words from the following texts: \"\n",
    "prompt2=\"Translate the following text from <source-language> into <target-language>: <source-sentence>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3='''\n",
    "The chained multilingual dictionaries:\n",
    "<word X in source-language> means <word X in target-language> means \n",
    "<word X in auxiliary-language 1> means <word X in auxiliary-language 2>.\n",
    "Translate the following text from <source-language> into <target-language>: <source-sentence>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"facebook/flores\", \"ace_Arab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "\n",
    "# model_name = \"princeton-nlp/gemma-2-9b-it-SimPO\"\n",
    "# cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# # Load model and tokenizer with optimization\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     cache_dir=cache_directory,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3219141/805226161.py:55: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. See LangGraph documentation for more details: https://langchain-ai.github.io/langgraph/. Refer here for its pre-built ReAct agent: https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools import tool\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tool_descriptions}\n",
    "\n",
    "When using a tool, follow this format:\n",
    "Thought: what you want to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "\n",
    "When you have the answer, follow this format:\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the question\n",
    "\n",
    "Begin!\n",
    "\n",
    "{input}\n",
    "\"\"\"\n",
    "custom_prompt = PromptTemplate(template=prompt_template, input_variables=[\"tool_descriptions\", \"tool_names\", \"input\"])\n",
    "\n",
    "# Define a sample tool\n",
    "@tool\n",
    "def calculator(query: str) -> str:\n",
    "    \"\"\"Perform basic arithmetic calculations.\"\"\"\n",
    "    try:\n",
    "        result = eval(query)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Instantiate the LLM\n",
    "# from transformers import pipeline\n",
    "# pipe = pipeline(\"text-generation\",\n",
    "#                 model=model,\n",
    "#                 tokenizer= tokenizer,\n",
    "#                 torch_dtype=torch.bfloat16,\n",
    "#                 device_map=\"auto\",\n",
    "#                 max_new_tokens = 512,\n",
    "#                 do_sample=True,\n",
    "#                 top_k=30,\n",
    "#                 num_return_sequences=1,\n",
    "#                 eos_token_id=tokenizer.eos_token_id\n",
    "#                 )\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
    "\n",
    "# Define tools\n",
    "tools = [Tool(name=\"Calculator\", func=calculator, description=\"Use to solve math problems\")]\n",
    "\n",
    "# Initialize the agent\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    agent_kwargs={\"custom_prompt\": custom_prompt}\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "# response = agent.run(\"What is 12 plus 8 divided by 2?\")\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSeq2SeqLM\n",
    "# model_name = \"facebook/nllb-200-3.3B\"\n",
    "# cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# # Load model and tokenizer with optimization\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     cache_dir=cache_directory,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import HuggingFacePipeline, pipeline\n",
    "# pipe = pipeline(\"text-generation\",\n",
    "#                 model=model,\n",
    "#                 tokenizer= tokenizer,\n",
    "#                 torch_dtype=torch.bfloat16,\n",
    "#                 device_map=\"auto\",\n",
    "#                 max_new_tokens = 512,\n",
    "#                 do_sample=True,\n",
    "#                 top_k=30,\n",
    "#                 num_return_sequences=1,\n",
    "#                 eos_token_id=tokenizer.eos_token_id\n",
    "#                 )\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template=\"\"\" Given the sentence {sentence} I want to extract important keywords from the sentence. Your answer should contain only list of important keywords.\n",
    "\"\"\"\n",
    "prompt_template=PromptTemplate(\n",
    "    template=template, input_variables=['sentence']\n",
    ")\n",
    "llm_chain = prompt_template | llm\n",
    "ans=llm_chain.invoke(\"Apple is looking at buying U.K. startup for $1 billion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple', ' buying', ' U.K.', ' startup', ' $1 billion']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.content.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n    \"Apple\": {\\n        \"Bangla\": \"আপেল\",\\n        \"French\": \"Pomme\",\\n        \"German\": \"Apfel\",\\n        \"Portuguese\": \"Maçã\"\\n    },\\n    \"buying\": {\\n        \"Bangla\": \"কেনা\",\\n        \"French\": \"achat\",\\n        \"German\": \"Kauf\",\\n        \"Portuguese\": \"compra\"\\n    },\\n    \"U.K.\": {\\n        \"Bangla\": \"যুক্তরাজ্য\",\\n        \"French\": \"Royaume-Uni\",\\n        \"German\": \"Vereinigtes Königreich\",\\n        \"Portuguese\": \"Reino Unido\"\\n    },\\n    \"startup\": {\\n        \"Bangla\": \"স্টার্টআপ\",\\n        \"French\": \"startup\",\\n        \"German\": \"Startup\",\\n        \"Portuguese\": \"startup\"\\n    },\\n    \"$1 billion\": {\\n        \"Bangla\": \"১ বিলিয়ন ডলার\",\\n        \"French\": \"1 milliard de dollars\",\\n        \"German\": \"1 Milliarde Dollar\",\\n        \"Portuguese\": \"1 bilhão de dólares\"\\n    }\\n}', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 231, 'prompt_tokens': 74, 'total_tokens': 305, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None}, id='run-d806eb81-49f5-4a91-bcf1-cc51a701f5cf-0', usage_metadata={'input_tokens': 74, 'output_tokens': 231, 'total_tokens': 305, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "template=\"\"\" Given the list of words {sentence}. I want to translate each keywords from the sentence to three languages: Bangla, French, German and Portuguese. Answer will be provided in dictionary format and answer have only dicionary, nothing else and remove extra syntax.\n",
    "\"\"\"\n",
    "prompt_template=PromptTemplate(\n",
    "    template=template, input_variables=['sentence']\n",
    ")\n",
    "llm_chain = prompt_template | llm\n",
    "ans=llm_chain.invoke(['Apple', ' buying', ' U.K.', ' startup', ' $1 billion'])\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "parsed_data = json.loads(ans.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple in English means আপেল in Bangla means Pomme in French means Apfel in German means Maçã in Portuguese.',\n",
       " 'buying in English means কেনা in Bangla means achat in French means Kauf in German means compra in Portuguese.',\n",
       " 'U.K. in English means যুক্তরাজ্য in Bangla means Royaume-Uni in French means Vereinigtes Königreich in German means Reino Unido in Portuguese.',\n",
       " 'startup in English means স্টার্টআপ in Bangla means startup in French means Startup in German means startup in Portuguese.',\n",
       " '$1 billion in English means ১ বিলিয়ন ডলার in Bangla means 1 milliard de dollars in French means 1 Milliarde Dollar in German means 1 bilhão de dólares in Portuguese.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_chained_format(dictionary, src_lang, target_lang):\n",
    "    result = []\n",
    "    for word, translations in dictionary.items():\n",
    "        chain = f\"{word} in {src_lang} means {translations.get(target_lang, 'N/A')} in {target_lang}\"\n",
    "        for aux_lang, aux_translation in translations.items():\n",
    "            if aux_lang != target_lang:\n",
    "                chain += f\" means {aux_translation} in {aux_lang}\"\n",
    "        result.append(chain + \".\")\n",
    "    return result\n",
    "\n",
    "# Convert and print the chained multilingual format\n",
    "src_language = \"English\"\n",
    "target_language = \"Bangla\"\n",
    "chained_output = convert_to_chained_format(parsed_data, src_language, target_language)\n",
    "chained_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3='''\n",
    "The chained multilingual dictionaries:\n",
    "<word X in source-language> means <word X in target-language> means \n",
    "<word X in auxiliary-language 1> means <word X in auxiliary-language 2>.\n",
    "Translate the following text from <source-language> into <target-language>: <source-sentence>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Apple যুক্তরাজ্যের স্টার্টআপ কেনার জন্য ১ বিলিয়ন ডলার খুঁজছে।', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 183, 'total_tokens': 206, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None}, id='run-5b721490-2208-47e0-8355-b36d69a240d2-0', usage_metadata={'input_tokens': 183, 'output_tokens': 23, 'total_tokens': 206, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template=\"\"\" \n",
    "{chained_dict}.\n",
    "Translate the following text from {src_lng} into {trg_lng}: {src_sent}\n",
    "\n",
    "\"\"\"\n",
    "input_data = {\n",
    "    'chained_dict': chained_output,\n",
    "    'src_lng': 'English',\n",
    "    'trg_lng': 'Bangla',\n",
    "    'src_sent': 'Apple is looking at buying U.K. startup for $1 billion.'\n",
    "}\n",
    "\n",
    "prompt_template=PromptTemplate(\n",
    "    template=template, input_variables=['chained_dict','src_lng','trg_lng','src_sent']\n",
    ")\n",
    "llm_chain = prompt_template | llm\n",
    "ans=llm_chain.invoke(input_data)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ner(sentence):\n",
    "    # Process the sentence with spaCy\n",
    "    doc = nlp(sentence)\n",
    "    ner_words = [{\"word\": ent.text, \"Entity name\":ent.label_} for ent in doc.ents]\n",
    "    return ner_words\n",
    "extract_ner(\"Apple is looking at buying U.K. startup for $1 billion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import (\n",
    "    create_react_agent,\n",
    "    AgentExecutor,\n",
    ")\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "sent=\"Apple is looking at buying U.K. startup for $1 billion.\"\n",
    "template=\"\"\" Given the sentence {sentence} I want to extract important keywords from the list. Your answer should contain only list of important keywords.\n",
    "\"\"\"\n",
    "prompt_template=PromptTemplate(\n",
    "    template=template, input_variables=['sentence']\n",
    ")\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_for_agent=[\n",
    "    Tool(\n",
    "        name=\"Extract the important keywords\",\n",
    "        func=extract_ner,\n",
    "        description=\"Useful when I want to extract the important keywords from the sentence\"\n",
    "    )\n",
    "]\n",
    "react_prompt=hub.pull(\"hwchase17/react\")\n",
    "agent=create_react_agent(llm=llm,tools=tools_for_agent,prompt=react_prompt)\n",
    "agent_executor=AgentExecutor(agent=agent,tools=tools_for_agent,verbose=True)\n",
    "result=agent_executor.invoke(\n",
    "    input={\"input\":prompt_template.format_prompt(sentence=sent)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
