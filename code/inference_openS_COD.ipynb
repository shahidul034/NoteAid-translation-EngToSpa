{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import back_translate\n",
    "from utils import compute_bleu_chrf\n",
    "import json\n",
    "file_path = \"/home/mshahidul/project1/all_tran_data/dataset/Sampled_100_MedlinePlus_eng_spanish_pair.json\"\n",
    "with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "    original_file = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_file_path = \"/home/mshahidul/project1/all_tran_data/dataset/medlineplus_info.json\"\n",
    "with open(info_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    info_data = json.load(json_file)\n",
    "    # Create a dictionary to map Original_English_sentence to synonyms\n",
    "sentence_to_synonyms = {item['Original_English_sentence']: item['synonyms'] for item in info_data}\n",
    "\n",
    "    # Function to find synonyms based on English sentence\n",
    "def find_synonyms(english_sentence):\n",
    "    return sentence_to_synonyms.get(english_sentence, \"Synonyms not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file_path = \"/home/mshahidul/project1/results_new/Medline/medlineplus_gpt4_mini_COD_back_translation.json\"\n",
    "with open(results_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    results_data = json.load(json_file)\n",
    "\n",
    "# Create a dictionary to map Original_English_sentence to COD_prompt\n",
    "sentence_to_prompt = {item['Original_English_sentence']: item['COD_prompt'] for item in results_data}\n",
    "\n",
    "# Function to find COD_prompt based on English sentence\n",
    "def find_cod_prompt(english_sentence):\n",
    "    return sentence_to_prompt.get(english_sentence, \"Prompt not found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "                    ### Instruction:\n",
    "                    {}\n",
    "\n",
    "                    ### Input:\n",
    "                    {}\n",
    "\n",
    "                    ### Response:\n",
    "                    {}\"\"\"\n",
    "max_seq_length=2048\n",
    "model_name = \"unsloth/Qwen2.5-14B-Instruct\"\n",
    "model_name2=model_name.split(\"/\")[1]\n",
    "from unsloth import FastLanguageModel\n",
    "model2, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_name, # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = None,\n",
    "        load_in_4bit = False,\n",
    "    )\n",
    "FastLanguageModel.for_inference(model2) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score=[]\n",
    "FastLanguageModel.for_inference(model2) # Enable native 2x faster inference\n",
    "\n",
    "def inference_without_prompt(ques):\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            f\"{find_cod_prompt(ques)}\\nUsing the above context translate the input into English to Spanish:\", # instruction\n",
    "            ques, # input\n",
    "            \"\", # output - leave this blank for generation!\n",
    "        )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model2.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "    ans=tokenizer.batch_decode(outputs)\n",
    "    start_marker = '### Response:\\n'\n",
    "    end_marker = '<|end_of_text|>'\n",
    "\n",
    "    # Find the start and end positions\n",
    "    start_index = ans[0].find(start_marker) + len(start_marker)\n",
    "    end_index = ans[0].find(end_marker)\n",
    "    response = ans[0][start_index:end_index].strip()\n",
    "    try:\n",
    "        ans=response.split(\"\\n\\n\")[1]\n",
    "    except:\n",
    "        ans=response\n",
    "    return ans  \n",
    "\n",
    "\n",
    "def inference_with_prompt(ques):\n",
    "    # {find_synonyms(ques)}\\n\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            f\"{find_cod_prompt(ques)}\\nUse this context to translate the input into English to Spanish:\", # instruction\n",
    "            ques, # input\n",
    "            \"\", # output - leave this blank for generation!\n",
    "        )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model2.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "    ans=tokenizer.batch_decode(outputs)\n",
    "    start_marker = '### Response:\\n'\n",
    "    end_marker = '<|end_of_text|>'\n",
    "\n",
    "    # Find the start and end positions\n",
    "    start_index = ans[0].find(start_marker) + len(start_marker)\n",
    "    end_index = ans[0].find(end_marker)\n",
    "    response = ans[0][start_index:end_index].strip()\n",
    "    return response.split(\"\\n\\n\")[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_prompt(ques):\n",
    "    # {find_synonyms(ques)}\\n\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            f\"{find_cod_prompt(ques)}\\nUse this context to translate the input into English to Spanish:\", # instruction\n",
    "            ques, # input\n",
    "            \"\", # output - leave this blank for generation!\n",
    "        )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model2.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "    ans=tokenizer.batch_decode(outputs)\n",
    "    start_marker = '### Response:\\n'\n",
    "    end_marker = '<|end_of_text|>'\n",
    "\n",
    "    # Find the start and end positions\n",
    "    start_index = ans[0].find(start_marker) + len(start_marker)\n",
    "    end_index = ans[0].find(end_marker)\n",
    "    response = ans[0][start_index:end_index].strip()\n",
    "    return response.split(\"\\n\\n\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_with_prompt(\"In January 2001, the FDA's Center for Food Safety and Applied Nutrition proposed that developers of bioengineered food submit scientific and safety information to the FDA at least 120 days before the food is marketed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_without_prompt(\"In January 2001, the FDA's Center for Food Safety and Applied Nutrition proposed that developers of bioengineered food submit scientific and safety information to the FDA at least 120 days before the food is marketed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation reference text spanish(real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tqdm\n",
    "for line in tqdm.tqdm(original_file):\n",
    "    try:\n",
    "        spa_tran_prompt = inference_with_prompt(line['english'])\n",
    "        \n",
    "\n",
    "        reference_text = [line['spanish']]\n",
    "        hypothesis_text = spa_tran_prompt\n",
    "        score1=compute_bleu_chrf(reference_text, hypothesis_text)  \n",
    "\n",
    "        spa_tran_direct=inference_without_prompt(line['english'])\n",
    "    \n",
    "        reference_text = [line['spanish']]\n",
    "        hypothesis_text = spa_tran_direct\n",
    "        score2=compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "\n",
    "        total_score.append({\n",
    "            \"Original_English_sentence\": line['english'],\n",
    "            \"Original_Spanish_sentence\": line['spanish'],\n",
    "            \"spanish_translation_prompt\": spa_tran_prompt,\n",
    "            \"spanish_translation_direct\": spa_tran_direct,\n",
    "            \"scores_cod_prompt(bleu and chrf)\": score1,\n",
    "            \"scores_direct(bleu and chrf)\": score2\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing line: {line}\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation back translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import tqdm\n",
    "# for line in tqdm.tqdm(original_file):\n",
    "#     try:\n",
    "#         spa_tran_prompt = inference_with_prompt(line['english'])\n",
    "#         back_tran_prompt=back_translate(spa_tran_prompt)\n",
    "\n",
    "#         reference_text = [line['english']]\n",
    "#         hypothesis_text = back_tran_prompt\n",
    "#         score1=compute_bleu_chrf(reference_text, hypothesis_text)  \n",
    "\n",
    "#         spa_tran_direct=inference_without_prompt(line['english'])\n",
    "#         back_tran_direct=back_translate(spa_tran_direct)\n",
    "#         reference_text = [line['english']]\n",
    "#         hypothesis_text = back_tran_direct\n",
    "#         score2=compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "\n",
    "#         total_score.append({\n",
    "#             \"Original_English_sentence\": line['english'],\n",
    "#             \"Original_Spanish_sentence\": line['spanish'],\n",
    "#             \"spanish_translation_prompt\": spa_tran_prompt,\n",
    "#             \"back_translation_prompt\": back_tran_prompt,\n",
    "#             \"spanish_translation_direct\": spa_tran_direct,\n",
    "#             \"back_translation_direct\": back_tran_direct,\n",
    "#             \"scores_cod_prompt(bleu and chrf)\": score1,\n",
    "#             \"scores_direct(bleu and chrf)\": score2\n",
    "#         })\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing line: {line}\")\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = f\"/home/mshahidul/project1/results_new/medline_{model_name2}_direct_and_COD_translation.json\"\n",
    "with open(output_file_path, 'w', encoding='utf-8') as json_file: \n",
    "    json.dump(total_score, json_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to store the sum of scores\n",
    "total_bleu_cod_prompt = 0\n",
    "\n",
    "total_bleu_direct = 0\n",
    "\n",
    "\n",
    "# Iterate through the total_score list to sum up the scores\n",
    "for score in total_score:\n",
    "    total_bleu_cod_prompt += score[\"scores_cod_prompt(bleu and chrf)\"]['bleu_score']\n",
    "    \n",
    "    total_bleu_direct += score[\"scores_direct(bleu and chrf)\"]['bleu_score']\n",
    "\n",
    "\n",
    "# Calculate the average scores\n",
    "num_entries = len(total_score)\n",
    "avg_bleu_cod_prompt = total_bleu_cod_prompt / num_entries\n",
    "\n",
    "avg_bleu_direct = total_bleu_direct / num_entries\n",
    "\n",
    "print(model_name)\n",
    "# Print the average scores\n",
    "print(f\"{model_name2} (COD prompt): {avg_bleu_cod_prompt}\")\n",
    "\n",
    "print(f\"{model_name2} (Direct): {avg_bleu_direct}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
