{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    import torch\n",
    "except: \n",
    "    raise ImportError('Install torch via `pip install torch`')\n",
    "from packaging.version import Version as V\n",
    "v = V(torch.__version__)\n",
    "cuda = str(torch.version.cuda)\n",
    "is_ampere = torch.cuda.get_device_capability()[0] >= 8\n",
    "if cuda != \"12.1\" and cuda != \"11.8\" and cuda != \"12.4\": \n",
    "    raise RuntimeError(f\"CUDA = {cuda} not supported!\")\n",
    "if   v <= V('2.1.0'): \n",
    "    raise RuntimeError(f\"Torch = {v} too old!\")\n",
    "elif v <= V('2.1.1'): \n",
    "    x = 'cu{}{}-torch211'\n",
    "elif v <= V('2.1.2'): \n",
    "    x = 'cu{}{}-torch212'\n",
    "elif v  < V('2.3.0'): \n",
    "    x = 'cu{}{}-torch220'\n",
    "elif v  < V('2.4.0'): \n",
    "    x = 'cu{}{}-torch230'\n",
    "elif v  < V('2.5.0'): \n",
    "    x = 'cu{}{}-torch240'\n",
    "elif v  < V('2.6.0'): \n",
    "    x = 'cu{}{}-torch250'\n",
    "else: \n",
    "    raise RuntimeError(f\"Torch = {v} too new!\")\n",
    "x = x.format(cuda.replace(\".\", \"\"), \"-ampere\" if is_ampere else \"\")\n",
    "print(f'pip install --upgrade pip && pip install \"unsloth[{x}] @ git+https://github.com/unslothai/unsloth.git\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n",
    "\n",
    "def search_umls_by_language(keyword):\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        if connection.is_connected():\n",
    "            print(\"Connected to the UMLS database\")\n",
    "\n",
    "            cursor = connection.cursor(dictionary=True)\n",
    "\n",
    "            # Step 1: Find the CUI for the keyword\n",
    "            cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{keyword}%\",))\n",
    "            result = cursor.fetchone()\n",
    "            if not result:\n",
    "                print(f\"No results found for keyword: {keyword}\")\n",
    "                return \"not found\"\n",
    "            cui = result[\"CUI\"]\n",
    "            print(f\"Found CUI: {cui}\")\n",
    "\n",
    "            # Step 2: Query for STR values for specific languages\n",
    "            languages = ['FRE', 'POR', 'GER']\n",
    "            cursor.execute(\n",
    "                \"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                (cui, *languages)\n",
    "            )\n",
    "            rows = cursor.fetchall()\n",
    "\n",
    "            # Step 3: Format the output in JSON\n",
    "            language_data = {row['LAT']: row['STR'] for row in rows}\n",
    "            output_json = json.dumps(language_data, indent=4, ensure_ascii=False)\n",
    "            print(\"\\nResults in JSON format:\")\n",
    "            print(output_json)\n",
    "\n",
    "            return output_json\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "    finally:\n",
    "        if 'connection' in locals() and connection.is_connected():\n",
    "            connection.close()\n",
    "            print(\"Database connection closed\")\n",
    "\n",
    "# Use the function to search for a keyword\n",
    "keyword = input(\"Enter a keyword to search: \")\n",
    "search_umls_by_language(keyword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n",
    "\n",
    "output_file = \"umls_tables_info.txt\"\n",
    "\n",
    "try:\n",
    "    # Connect to the MySQL database\n",
    "    connection = mysql.connector.connect(**db_config)\n",
    "    \n",
    "    if connection.is_connected():\n",
    "        print(\"Connected to the UMLS database\")\n",
    "\n",
    "        # Create a cursor object to execute queries\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Open the file to write\n",
    "        with open(output_file, \"w\") as file:\n",
    "            # List all tables in the database\n",
    "            cursor.execute(\"SHOW TABLES;\")\n",
    "            tables = cursor.fetchall()\n",
    "            file.write(\"Tables in the database:\\n\")\n",
    "            for table in tables:\n",
    "                file.write(f\"{table[0]}\\n\")\n",
    "            file.write(\"\\nTable structures:\\n\")\n",
    "\n",
    "            # Get the structure of each table\n",
    "            for table in tables:\n",
    "                table_name = table[0]\n",
    "                file.write(f\"\\nStructure of table '{table_name}':\\n\")\n",
    "                cursor.execute(f\"DESCRIBE {table_name};\")\n",
    "                structure = cursor.fetchall()\n",
    "                for column in structure:\n",
    "                    file.write(f\"  Column: {column[0]}, Type: {column[1]}, Null: {column[2]}, Key: {column[3]}, Default: {column[4]}\\n\")\n",
    "        \n",
    "        print(f\"Table information saved to '{output_file}'\")\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Error: {err}\")\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    if 'connection' in locals() and connection.is_connected():\n",
    "        connection.close()\n",
    "        print(\"Database connection closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n",
    "\n",
    "# Define the NLLB-200 model and its configuration\n",
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# List of languages and their codes\n",
    "languages = {\n",
    "    \"French\": \"fra_Latn\",\n",
    "    \"German\": \"deu_Latn\",\n",
    "    \"Portuguese\": \"por_Latn\",\n",
    "    \"Bangla\": 'ben_Beng'\n",
    "}\n",
    "\n",
    "# Load model and tokenizer for translation\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_directory, torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def search_umls_by_language(keyword):\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        if connection.is_connected():\n",
    "            print(\"Connected to the UMLS database\")\n",
    "\n",
    "            cursor = connection.cursor(dictionary=True)\n",
    "\n",
    "            # Step 1: Find the CUI for the keyword\n",
    "            cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{keyword}%\",))\n",
    "            result = cursor.fetchone()\n",
    "            if not result:\n",
    "                print(f\"No results found for keyword: {keyword}\")\n",
    "                return translate_keyword(keyword)  # Call translation fallback\n",
    "            cui = result[\"CUI\"]\n",
    "            print(f\"Found CUI: {cui}\")\n",
    "\n",
    "            # Step 2: Query for STR values for specific languages\n",
    "            languages_query = ['FRE', 'POR', 'GER']\n",
    "            cursor.execute(\n",
    "                \"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                (cui, *languages_query)\n",
    "            )\n",
    "            rows = cursor.fetchall()\n",
    "\n",
    "            # Step 3: Format the output in JSON\n",
    "            language_data = {row['LAT']: row['STR'] for row in rows}\n",
    "            output_json = json.dumps(language_data, indent=4, ensure_ascii=False)\n",
    "            print(\"\\nResults in JSON format:\")\n",
    "            print(output_json)\n",
    "\n",
    "            return output_json\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error: {err}\")\n",
    "    finally:\n",
    "        if 'connection' in locals() and connection.is_connected():\n",
    "            connection.close()\n",
    "            print(\"Database connection closed\")\n",
    "\n",
    "\n",
    "def translate_keyword(keyword):\n",
    "    print(f\"Translating keyword: {keyword}\")\n",
    "    multi_tran = {}\n",
    "    multi_tran[keyword] = {}\n",
    "    for language, lang_code in languages.items():\n",
    "        output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "        multi_tran[keyword][language] = output[0]['translation_text']\n",
    "\n",
    "    output_json = json.dumps(multi_tran, indent=4, ensure_ascii=False)\n",
    "    print(\"\\nTranslated results in JSON format:\")\n",
    "    print(output_json)\n",
    "    return output_json\n",
    "\n",
    "\n",
    "# Use the function to search for a keyword\n",
    "keyword = input(\"Enter a keyword to search: \")\n",
    "search_umls_by_language(keyword)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import OpenAI  # Assuming OpenAI API is set up\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n",
    "\n",
    "# Define the NLLB-200 model\n",
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# List of target languages\n",
    "languages = {\n",
    "    \"French\": \"fra_Latn\",\n",
    "    \"German\": \"deu_Latn\",\n",
    "    \"Portuguese\": \"por_Latn\",\n",
    "    \"Bangla\": 'ben_Beng'\n",
    "}\n",
    "\n",
    "# Load translation model\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_directory, torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# OpenAI API Client\n",
    "client = OpenAI(api_key=\"YOUR_OPENAI_API_KEY\")\n",
    "\n",
    "def extract_keywords(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"Extract medical and non-medical keywords from the given sentence.\"},\n",
    "                  {\"role\": \"user\", \"content\": sentence}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    keywords = json.loads(response.choices[0].message.content)\n",
    "    return keywords  # Expected format: {\"medical\": [\"keyword1\", \"keyword2\"], \"non_medical\": [\"keyword3\", \"keyword4\"]}\n",
    "\n",
    "def search_umls(keyword):\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{keyword}%\",))\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            return None\n",
    "        cui = result[\"CUI\"]\n",
    "\n",
    "        cursor.execute(\"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                       (cui, 'FRE', 'POR', 'GER'))\n",
    "        rows = cursor.fetchall()\n",
    "        return {row['LAT']: row['STR'] for row in rows}\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Database error: {err}\")\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            connection.close()\n",
    "\n",
    "def translate_non_medical(keyword):\n",
    "    translations = {}\n",
    "    for language, lang_code in languages.items():\n",
    "        output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "        translations[language] = output[0]['translation_text']\n",
    "    return translations\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    keywords = extract_keywords(sentence)\n",
    "    medical_translations = {}\n",
    "    non_medical_translations = {}\n",
    "    \n",
    "    for keyword in keywords[\"medical\"]:\n",
    "        translation = search_umls(keyword)\n",
    "        if translation:\n",
    "            medical_translations[keyword] = translation\n",
    "    \n",
    "    for keyword in keywords[\"non_medical\"]:\n",
    "        non_medical_translations[keyword] = translate_non_medical(keyword)\n",
    "    \n",
    "    return json.dumps({\n",
    "        \"medical\": medical_translations,\n",
    "        \"non_medical\": non_medical_translations\n",
    "    }, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Example usage\n",
    "sentence = input(\"Enter a sentence: \")\n",
    "result_json = process_sentence(sentence)\n",
    "print(result_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Define the path to the tar.gz file\n",
    "tar_gz_path = \"/data/data_user/annotations/machine_translation/eng_spa_pairs.tar.gz\"\n",
    "\n",
    "# Extract the contents of the tar.gz file\n",
    "with tarfile.open(tar_gz_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=\"/home/mshahidul/project1/data2/extracted_files\")\n",
    "\n",
    "# List the extracted files\n",
    "extracted_files_path = \"/home/mshahidul/project1/data2/extracted_files\"\n",
    "extracted_files = os.listdir(extracted_files_path)\n",
    "print(extracted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import OpenAI  # Assuming OpenAI API is set up\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n",
    "\n",
    "# Define the NLLB-200 model\n",
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# List of target languages\n",
    "languages = {\n",
    "    \"French\": \"fra_Latn\",\n",
    "    \"German\": \"deu_Latn\",\n",
    "    \"Portuguese\": \"por_Latn\",\n",
    "    \"Bangla\": 'ben_Beng'\n",
    "}\n",
    "\n",
    "# Load translation model\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_directory, torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# OpenAI API Client\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def extract_keywords(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"Extract medical and non-medical keywords from the given sentence. return it as json format without extra things.\"},\n",
    "                  {\"role\": \"user\", \"content\": sentence}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    print(response.choices[0].message.content)\n",
    "    keywords = json.loads(response.choices[0].message.content)\n",
    "    return keywords  # Expected format: {\"medical\": [\"keyword1\", \"keyword2\"], \"non_medical\": [\"keyword3\", \"keyword4\"]}\n",
    "\n",
    "def search_umls(keyword):\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{keyword}%\",))\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            return None\n",
    "        cui = result[\"CUI\"]\n",
    "\n",
    "        cursor.execute(\"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                       (cui, 'FRE', 'POR', 'GER'))\n",
    "        rows = cursor.fetchall()\n",
    "        return {row['LAT']: row['STR'] for row in rows}\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Database error: {err}\")\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            connection.close()\n",
    "\n",
    "def translate_non_medical(keyword):\n",
    "    translations = {}\n",
    "    for language, lang_code in languages.items():\n",
    "        output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "        translations[language] = output[0]['translation_text']\n",
    "    return translations\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    keywords = extract_keywords(sentence)\n",
    "    medical_translations = {}\n",
    "    non_medical_translations = {}\n",
    "    \n",
    "    for keyword in keywords[\"medical_keywords\"]:\n",
    "        translation = search_umls(keyword)\n",
    "        if translation:\n",
    "            medical_translations[keyword] = translation\n",
    "    \n",
    "    for keyword in keywords[\"non_medical_keywords\"]:\n",
    "        non_medical_translations[keyword] = translate_non_medical(keyword)\n",
    "    \n",
    "    return json.dumps({\n",
    "        \"medical\": medical_translations,\n",
    "        \"non_medical\": non_medical_translations\n",
    "    }, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Bariatric surgery is done when diet and exercise haven't worked or when you have serious health problems because of your weight.\"\n",
    "result_json = process_sentence(sentence)\n",
    "print(result_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unigram_hits': ['the', 'cat', 'is', 'on', 'mat'], 'unigram_misses': [], 'bigram_hits': [('the', 'cat'), ('cat', 'is'), ('on', 'the'), ('the', 'mat')], 'bigram_misses': [('is', 'on')]}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def ngram_hits(reference: str, hypothesis: str):\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    \n",
    "    # Compute unigram hits\n",
    "    ref_unigrams = Counter(ref_tokens)\n",
    "    hyp_unigrams = Counter(hyp_tokens)\n",
    "    unigram_hits = [word for word in hyp_unigrams if word in ref_unigrams]\n",
    "    unigram_misses = [word for word in hyp_unigrams if word not in ref_unigrams]\n",
    "    \n",
    "    # Compute bigram hits\n",
    "    ref_bigrams = Counter(ngrams(ref_tokens, 2))\n",
    "    hyp_bigrams = Counter(ngrams(hyp_tokens, 2))\n",
    "    bigram_hits = [bigram for bigram in hyp_bigrams if bigram in ref_bigrams]\n",
    "    bigram_misses = [bigram for bigram in hyp_bigrams if bigram not in ref_bigrams]\n",
    "    \n",
    "    return {\n",
    "        \"unigram_hits\": unigram_hits,\n",
    "        \"unigram_misses\": unigram_misses,\n",
    "        \"bigram_hits\": bigram_hits,\n",
    "        \"bigram_misses\": bigram_misses\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "reference_sentence = \"the cat is sitting on the mat\"\n",
    "hypothesis_sentence = \"the cat is on the mat\"\n",
    "print(ngram_hits(reference_sentence, hypothesis_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* Running on public URL: https://43f3e64d1f9c07f9ba.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://43f3e64d1f9c07f9ba.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import gradio as gr\n",
    "reference=\"the cat is on the mat\" \n",
    "hypothesis=\"the cat is sitting on the mat\"\n",
    "\n",
    "def ngram_hits():\n",
    "    ref_tokens = reference.split()\n",
    "    hyp_tokens = hypothesis.split()\n",
    "    \n",
    "    # Compute unigram hits\n",
    "    ref_unigrams = Counter(ref_tokens)\n",
    "    hyp_unigrams = Counter(hyp_tokens)\n",
    "    unigram_hits = [word for word in hyp_unigrams if word in ref_unigrams]\n",
    "    unigram_misses = [word for word in hyp_unigrams if word not in ref_unigrams]\n",
    "    \n",
    "    # Compute bigram hits\n",
    "    ref_bigrams = Counter(ngrams(ref_tokens, 2))\n",
    "    hyp_bigrams = Counter(ngrams(hyp_tokens, 2))\n",
    "    bigram_hits = [bigram for bigram in hyp_bigrams if bigram in ref_bigrams]\n",
    "    bigram_misses = [bigram for bigram in hyp_bigrams if bigram not in ref_bigrams]\n",
    "    \n",
    "    # Format output with HTML\n",
    "    formatted_text = \"\"\n",
    "    for word in hyp_tokens:\n",
    "        if word in unigram_hits:\n",
    "            formatted_text += f'<span style=\"color: green;\">{word}</span> '\n",
    "        else:\n",
    "            formatted_text += f'<span style=\"color: red;\">{word}</span> '\n",
    "    \n",
    "    return formatted_text.strip()\n",
    "\n",
    "def processing():\n",
    "    txt=ngram_hits()\n",
    "    print(txt)\n",
    "    return txt\n",
    "# Gradio UI\n",
    "demo = gr.Interface(\n",
    "    fn=processing,\n",
    "    inputs=[],\n",
    "    outputs=gr.HTML(label=\"Highlighted Output\")\n",
    ")\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
