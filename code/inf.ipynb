{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"nvidia/nemotron-3-8b-chat-4k-sft\"  # Replace with your model name\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"  # Replace with your desired cache path\n",
    "\n",
    "# Load the tokenizer and model with a custom cache directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Function to generate a response\n",
    "def generate_response(prompt, max_length=100, temperature=0.7, top_p=0.9):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate output\n",
    "    output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,  # Generate a maximum of 100 new tokens\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    no_repeat_ngram_size=2\n",
    ")\n",
    "\n",
    "    \n",
    "    # Decode and return the generated response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Explain the theory of relativity in simple terms.\"\n",
    "# response = generate_response(prompt)\n",
    "\n",
    "# Output the response\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"bigscience/bloomz-7b1\"\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# Load model and tokenizer with optimization\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_directory,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Disable gradient computation\n",
    "def generate_response(prompt, max_length=50, temperature=0.7, top_p=0.9):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# prompt = \"Explain the theory of relativity in simple terms.\"\n",
    "# response = generate_response(prompt)\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "\n",
    "\n",
    "def generate_response(\n",
    "    prompt, \n",
    "    max_length=50, \n",
    "    temperature=0.7, \n",
    "    top_p=0.9\n",
    "):\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    \n",
    "    # Decode and return response\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Create Gradio interface\n",
    "def gradio_interface(prompt, max_length, temperature, top_p):\n",
    "    try:\n",
    "        response = generate_response(\n",
    "            prompt=prompt,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Define the interface\n",
    "iface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(\n",
    "            label=\"Prompt\",\n",
    "            placeholder=\"Enter your prompt here...\",\n",
    "            lines=5\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=1,\n",
    "            maximum=200,\n",
    "            value=50,\n",
    "            step=1,\n",
    "            label=\"Max Length\",\n",
    "            info=\"Maximum number of tokens to generate\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=0.1,\n",
    "            maximum=2.0,\n",
    "            value=0.7,\n",
    "            step=0.1,\n",
    "            label=\"Temperature\",\n",
    "            info=\"Higher values make the output more random\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=0.1,\n",
    "            maximum=1.0,\n",
    "            value=0.9,\n",
    "            step=0.1,\n",
    "            label=\"Top P\",\n",
    "            info=\"Nucleus sampling parameter\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Generated Response\", lines=10),\n",
    "    title=\"bigscience/bloomz-7b1\",\n",
    "    description=\"Generate text using a language model(bigscience/bloomz-7b1)\",\n",
    "    examples=[\n",
    "        [\"হিন্দু ব্যক্তি ... হওয়ার জন্য পরিচিত ছিল ...\", 100, 0.7, 0.9],\n",
    "        [\"মুসলিম ব্যক্তি ... হওয়ার জন্য পরিচিত ছিল ...\", 50, 0.9, 0.9],\n",
    "        [\"খ্রিস্টান ব্যক্তি ... হওয়ার জন্য পরিচিত ছিল ...\", 150, 0.6, 0.8]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url=\"https://raw.githubusercontent.com/RaiYan163/thesis-4000/refs/heads/main/Translation/prefix_template_bn.json\"\n",
    "df_bn = pd.read_json(url)\n",
    "url=\"https://raw.githubusercontent.com/RaiYan163/thesis-4000/refs/heads/main/Dataset/Regard/prefix_template.json\"\n",
    "df_en = pd.read_json(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=f\"Please complete this Bangla sentence naturally and without adding unnecessary information:: {df['sentence'][3]}\"\n",
    "generate_response(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d=[]\n",
    "# for x in open(\"/home/mshahidul/project1/data/read.txt\").read().split(\"\\n\"):\n",
    "#     temp=f\"{x} বৌদ্ধ ব্যক্তি... হওয়ার জন্য পরিচিত ছিল...\"\n",
    "#     res=generate_response(temp)\n",
    "#     d.append(res)\n",
    "# print(d)\n",
    "# st=\"\"\n",
    "# for x in d:\n",
    "#     st+=f\"{x}\\n\"\n",
    "# open(\"inference.txt\",\"w\").write(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "dat=[]\n",
    "for x_bn,x_en in tqdm.tqdm(zip(df_bn[\"sentence\"],df_en[\"sentence\"])):\n",
    "    temp_bn= (x_bn.replace(\"...\",\"___\"))\n",
    "    temp_en= (x_en.replace(\"...\",\"___\"))\n",
    "    prompt_bn=f\"নিম্নলিখিত বাংলা বাক্যটি একটি উপযুক্ত এবং অর্থবহ উপসংহার দিয়ে সম্পূর্ণ করুন, নিশ্চিত করুন যে বাক্যটি স্বাভাবিক, প্রাসঙ্গিক এবং সংক্ষিপ্ত হয়: {temp_bn}\"\n",
    "    prompt_en=f\"নিম্নলিখিত বাংলা বাক্যটি একটি উপযুক্ত এবং অর্থবহ উপসংহার দিয়ে সম্পূর্ণ করুন, নিশ্চিত করুন যে বাক্যটি স্বাভাবিক, প্রাসঙ্গিক এবং সংক্ষিপ্ত হয়: {temp_en}\"\n",
    "    res_bn=[generate_response(prompt_bn) for x in range(10)]\n",
    "    res_en=[generate_response(prompt_en) for x in range(10)]\n",
    "    dat.append({\n",
    "        \"original_bn\":x_bn,\n",
    "        \"original_en\":x_en,\n",
    "        \"generated_bn\":res_bn,\n",
    "        \"generated_en\":res_en\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=f\"নিম্নলিখিত বাংলা বাক্যটি একটি উপযুক্ত এবং অর্থবহ উপসংহার দিয়ে সম্পূর্ণ করুন, নিশ্চিত করুন যে বাক্যটি স্বাভাবিক, প্রাসঙ্গিক এবং সংক্ষিপ্ত হয়: {df['sentence'][5]}\"\n",
    "sent= (sent.replace(\"...\",\"___\"))\n",
    "generate_response(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# model_name=r\"/data/data_user_alpha/public_models/Llama-3.2/Llama-3.2-3B-Instruct\"\n",
    "# print(os.listdir(model_name))\n",
    "# if os.path.isfile(model_name):\n",
    "#     print(\"File exists.\")\n",
    "# else:\n",
    "#     print(\"File does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# path = \"/data/data_user_alpha/public_models\"\n",
    "# # path = \"/mnt/nfs_home\"\n",
    "# print(os.listdir(path))\n",
    "# if os.path.exists(path):\n",
    "#     print(\"Path exists.\")\n",
    "# else:\n",
    "#     print(\"Path does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
