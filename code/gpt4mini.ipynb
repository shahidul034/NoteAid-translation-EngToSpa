{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt3='''\n",
    "# The chained multilingual dictionaries:\n",
    "# <word X in source-language> means <word X in target-language> means \n",
    "# <word X in auxiliary-language 1> means <word X in auxiliary-language 2>.\n",
    "# Translate the following text from <source-language> into <target-language>: <source-sentence>\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import OpenAI  # Assuming OpenAI API is set up\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n",
    "\n",
    "# Define the NLLB-200 model\n",
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# List of target languages\n",
    "languages = {\n",
    "    \"French\": \"fra_Latn\",\n",
    "    \"German\": \"deu_Latn\",\n",
    "    \"Portuguese\": \"por_Latn\",\n",
    "    \"Spanish\": 'spa_Latn'\n",
    "}\n",
    "\n",
    "# Load translation model\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_directory, torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# OpenAI API Client\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def extract_keywords(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract medical and non-medical keywords from the given sentence. return it as json format without extra things.\"},\n",
    "                  {\"role\": \"user\", \"content\": f\"{sentence}\"}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    # print(response.choices[0].message.content)\n",
    "    keywords = json.loads(response.choices[0].message.content)\n",
    "    return keywords  # Expected format: {\"medical\": [\"keyword1\", \"keyword2\"], \"non_medical\": [\"keyword3\", \"keyword4\"]}\n",
    "\n",
    "def search_umls(keyword):\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{keyword}%\",))\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            return None\n",
    "        cui = result[\"CUI\"]\n",
    "\n",
    "        cursor.execute(\"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                       (cui, 'FRE', 'POR', 'GER'))\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        translations = {row['LAT']: row['STR'] for row in rows}\n",
    "        return translations\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Database error: {err}\")\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            connection.close()\n",
    "\n",
    "def translate_non_medical(keyword):\n",
    "    translations = {}\n",
    "    for language, lang_code in languages.items():\n",
    "        output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "        translations[language] = output[0]['translation_text']\n",
    "    return translations\n",
    "\n",
    "def convert_to_chained_format(dictionary, src_lang, target_lang):\n",
    "    data={}\n",
    "    for x, y in dictionary.items():\n",
    "        for word,_ in dictionary[x].items():\n",
    "            data[word]=[]\n",
    "            data[word].append(word)\n",
    "            for _, tran2 in dictionary[x][word].items():\n",
    "                data[word].append(tran2)\n",
    "    chain=[]\n",
    "    for word, translations in data.items():\n",
    "        ch=\"\"\n",
    "        for i,x in enumerate(translations):\n",
    "            if i!=len(translations)-1:\n",
    "                ch+=f\"'{x}' means \"\n",
    "            else:\n",
    "                ch+=f\"'{x}'\"\n",
    "        chain.append(ch)\n",
    "    chain=\". \".join(chain)\n",
    "    chain+=\". \"\n",
    "    # chain=f\"{chain}\\nTranslate the following text from {src_lang} into {target_lang}:\"\n",
    "    return chain\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    keywords = extract_keywords(sentence)\n",
    "    medical_translations = {}\n",
    "    non_medical_translations = {}\n",
    "\n",
    "    # Process medical keywords\n",
    "    for keyword in keywords[\"medical_keywords\"]:\n",
    "        translation = search_umls(keyword)\n",
    "        if not translation:  # If keyword not found in UMLS, translate using NLLB\n",
    "            translation = {lang: translator(keyword, src_lang=\"eng_Latn\", tgt_lang=code, max_length=400)[0]['translation_text'] \n",
    "                           for lang, code in languages.items()}\n",
    "        elif \"SPA\" not in translation:  # If Spanish missing in UMLS, use NLLB for Spanish\n",
    "            translation[\"SPA\"] = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=\"spa_Latn\", max_length=400)[0]['translation_text']\n",
    "        \n",
    "        medical_translations[keyword] = translation\n",
    "\n",
    "    # Process non-medical keywords (always translated via NLLB)\n",
    "    for keyword in keywords[\"non_medical_keywords\"]:\n",
    "        non_medical_translations[keyword] = translate_non_medical(keyword)\n",
    "\n",
    "    result_json_temp = {\n",
    "        \"medical\": medical_translations,\n",
    "        \"non_medical\": non_medical_translations\n",
    "    }\n",
    "\n",
    "    src_language = \"English\"\n",
    "    target_language = \"Spanish\"\n",
    "    chained_output = convert_to_chained_format(result_json_temp, src_language, target_language)\n",
    "\n",
    "    return chained_output, result_json_temp\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Bariatric surgery is done when diet and exercise haven't worked or when you have serious health problems because of your weight.\"\n",
    "COD_prompt,result_json_temp = process_sentence(sentence)\n",
    "# print(COD_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate using COD prompt in Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_cod_prompt(COD_prompt,sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"{COD_prompt}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from English into Spanish: {sentence}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    ans=(response.choices[0].message.content)\n",
    "    return ans\n",
    "spa_tran=translate_cod_prompt(COD_prompt,sentence)\n",
    "spa_tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct translate in Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_translate(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from English into Spanish: {sentence}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    return (response.choices[0].message.content)\n",
    "spa_tran_direct=direct_translate(sentence)\n",
    "spa_tran_direct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back translate in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate(spa_tran):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                # {\"role\": \"system\", \"content\": f\"\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from Spanish into English: {spa_tran}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    return (response.choices[0].message.content)\n",
    "back_tran=back_translate(spa_tran)\n",
    "back_tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back translation in English without COD prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_tran_direct=back_translate(spa_tran_direct)\n",
    "back_tran_direct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "def compute_bleu_chrf(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Computes the BLEU and chrF++ scores for a given reference and hypothesis.\n",
    "    \n",
    "    :param reference: List of reference translations (list of strings)\n",
    "    :param hypothesis: The hypothesis translation (a single string)\n",
    "    :return: A dictionary containing BLEU and chrF++ scores\n",
    "    \"\"\"\n",
    "    # Ensure reference is wrapped in a list as sacrebleu expects a list of references\n",
    "    # bleu_score = sacrebleu.corpus_bleu(hypothesis, [reference]).score\n",
    "    # bleu_score = sacrebleu.corpus_bleu(hypothesis, [reference], tokenize=\"13a\", lowercase=True).score\n",
    "    bleu_score=metric.compute(predictions=[hypothesis], references=[reference])\n",
    "    chrf_score = sacrebleu.corpus_chrf(hypothesis, [reference]).score\n",
    "\n",
    "    return {\"bleu_score\": bleu_score['score'],\"chrF++\": chrf_score}\n",
    "\n",
    "# Example usage\n",
    "# reference_text = [sentence]\n",
    "# hypothesis_text = back_tran\n",
    "# scores = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "# print(\"COD prompt: \",scores)\n",
    "# reference_text = [sentence]\n",
    "# hypothesis_text = back_tran_direct\n",
    "# scores = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "# print(\"Direct translation: \",scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation\n",
    "output_data=[]\n",
    "import tqdm\n",
    "path=\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs.txt\"\n",
    "f=open(path, \"r\").read().split(\"\\n\")\n",
    "for x in tqdm.tqdm(f[:100]):\n",
    "    xx=x.split(\"\\t\")\n",
    "    sentence_eng=xx[0]\n",
    "    sentence_spa=xx[1]\n",
    "    COD_prompt,result_json_temp = process_sentence(sentence_eng)\n",
    "    spa_tran=translate_cod_prompt(COD_prompt,sentence_eng)\n",
    "    spa_tran_direct=direct_translate(sentence_eng)\n",
    "    back_tran=back_translate(spa_tran)\n",
    "    back_tran_direct=back_translate(spa_tran_direct)\n",
    "    reference_text = [sentence_eng]\n",
    "    hypothesis_text = back_tran\n",
    "    scores_cod_prompt = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    hypothesis_text = back_tran_direct\n",
    "    scores_direct = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    output_data.append({\n",
    "        \"Original_English_sentence\": sentence_eng,\n",
    "        \"Original_Spanish_sentence\": sentence_spa,\n",
    "        \"COD_prompt\": COD_prompt,\n",
    "        \"spanish_translation\": spa_tran,\n",
    "        \"spanish_translation_direct\": spa_tran_direct,\n",
    "        \"back_translation\": back_tran,\n",
    "        \"back_translation_direct\": back_tran_direct,\n",
    "        \"scores_cod_prompt(bleu and chrf)\": scores_cod_prompt,\n",
    "        \"scores_direct(bleu and chrf)\": scores_direct\n",
    "    })\n",
    "    # print(scores_cod_prompt,scores_direct)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(output_data)\n",
    "df.to_csv(\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs_gpt4_mini.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs_gpt4_mini.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs.txt\"\n",
    "f=open(path, \"r\").read().split(\"\\n\")\n",
    "for x in f[:100]:\n",
    "    xx=x.split(\"\\t\")\n",
    "    sentence_eng=xx[0]\n",
    "    sentence_spa=xx[1]\n",
    "    COD_prompt,result_json_temp = process_sentence(sentence_eng)\n",
    "    spa_tran=translate_cod_prompt(COD_prompt, sentence_eng)\n",
    "    reference_text = [sentence_spa]\n",
    "    hypothesis_text = spa_tran\n",
    "    scores_cod_prompt = compute_bleu_chrf(reference_text, hypothesis_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COD + syn (test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.49it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain of dictionary: 'stress fracture' means 'fractura por esfuerzo' means 'Fractures de fatigue' means 'Frakturen, Streß-' means 'Fraturas de Estresse'. 'bone' means 'hueso' means 'Maladies osseuses' means 'Knochenkrankheiten' means 'Doenças Ósseas'.\n",
      "\n",
      "Synonyms: 'stress fracture' synonyms are [stress, fracture]. 'hairline crack' synonyms are [hairline, crack]. 'bone' synonyms are [bone]. 'repeated forces' synonyms are [repeated, forces]. 'prolonged forces' synonyms are [prolonged, forces].\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import OpenAI  # Assuming OpenAI API is set up\n",
    "from utils import get_synonyms, back_translate, compute_bleu_chrf\n",
    "from openai import OpenAI \n",
    "import os\n",
    "client = OpenAI(api_key=\"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\")\n",
    "import json\n",
    "def translate_using_prompt(prompt,sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from English into Spanish: {sentence}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    ans=(response.choices[0].message.content)\n",
    "    return ans\n",
    "\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n",
    "\n",
    "# Define the NLLB-200 model\n",
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# List of target languages\n",
    "languages = {\n",
    "    \"French\": \"fra_Latn\",\n",
    "    \"German\": \"deu_Latn\",\n",
    "    \"Portuguese\": \"por_Latn\",\n",
    "    \"Spanish\": 'spa_Latn'\n",
    "}\n",
    "\n",
    "# Load translation model\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_directory, torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def extract_keywords(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract medical and non-medical keywords from the given sentence. return it as json format without extra things.\"},\n",
    "                  {\"role\": \"user\", \"content\": f\"{sentence}\"}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    # print(response.choices[0].message.content)\n",
    "    keywords = json.loads(response.choices[0].message.content)\n",
    "    return keywords  # Expected format: {\"medical\": [\"keyword1\", \"keyword2\"], \"non_medical\": [\"keyword3\", \"keyword4\"]}\n",
    "\n",
    "def search_umls(keyword):\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{keyword}%\",))\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            return None\n",
    "        cui = result[\"CUI\"]\n",
    "\n",
    "        cursor.execute(\"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                       (cui, 'FRE', 'POR', 'GER'))\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        translations = {row['LAT']: row['STR'] for row in rows}\n",
    "        return translations\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Database error: {err}\")\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            connection.close()\n",
    "\n",
    "# def translate_non_medical(keyword):\n",
    "#     translations = {}\n",
    "#     for language, lang_code in languages.items():\n",
    "#         output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "#         translations[language] = output[0]['translation_text']\n",
    "#     return translations\n",
    "\n",
    "def convert_to_chained_format(dictionary, src_lang, target_lang):\n",
    "    chain = []\n",
    "    \n",
    "    for category, words in dictionary.items():\n",
    "        for word, translations in words.items():\n",
    "            formatted_translations = []\n",
    "            \n",
    "            # Ensure the source language is first, target language second, then others\n",
    "            ordered_languages = [\"ENG\", \"SPA\", \"FRE\", \"GER\", \"POR\"]\n",
    "            \n",
    "            for lang in ordered_languages:\n",
    "                if lang in translations:\n",
    "                    formatted_translations.append(f\"'{translations[lang]}'\")\n",
    "\n",
    "            chain.append(\" means \".join(formatted_translations))\n",
    "    \n",
    "    chained_text = \". \".join(chain) + \".\"\n",
    "    return chained_text\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    keywords = extract_keywords(sentence)\n",
    "    medical_translations = {}\n",
    "    output=[]\n",
    "    # Process medical keywords\n",
    "    for keyword in keywords[\"medical_keywords\"]:\n",
    "        translation={}\n",
    "        translation = search_umls(keyword)\n",
    "        synonyms=get_synonyms1(keyword)\n",
    "        # print(keyword)\n",
    "        if synonyms:\n",
    "            output.append(f\"'{keyword}' synonyms are [{', '.join(synonyms)}].\")\n",
    "        if translation and \"SPA\" not in translation:  # If Spanish missing in UMLS, use NLLB for Spanish\n",
    "            translation[\"SPA\"] = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=\"spa_Latn\", max_length=400)[0]['translation_text']\n",
    "        \n",
    "        if translation:\n",
    "            translation['ENG'] = keyword\n",
    "            medical_translations[keyword] = translation\n",
    "\n",
    "    result_json_temp = {\n",
    "        \"medical\": medical_translations\n",
    "    }\n",
    "\n",
    "    src_language = \"English\"\n",
    "    target_language = \"Spanish\"\n",
    "    output2=\" \".join(output)\n",
    "    if medical_translations:\n",
    "        chained_output = convert_to_chained_format(result_json_temp, src_language, target_language)\n",
    "        full_prompt=\"Chain of dictionary: \"+chained_output+\"\\n\\nSynonyms: \"+output2\n",
    "    else:\n",
    "        full_prompt=\"Synonyms: \"+output2\n",
    "\n",
    "    return full_prompt, medical_translations, keywords,output\n",
    "\n",
    "# Example usage\n",
    "sentence = \"A stress fracture is a hairline crack in the bone that develops because of repeated or prolonged forces against the bone.\"\n",
    "full_prompt, medical_translations, keywords,output = process_sentence(sentence)\n",
    "print(full_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HEAD INJ CLOSED',\n",
       " 'Nonpenetrating Head Injury',\n",
       " 'close head injuries',\n",
       " 'INJ CLOSED HEAD',\n",
       " 'HEAD INJURY CLOSED',\n",
       " 'Closed injury of head (disorder)',\n",
       " 'Injury;closed head',\n",
       " 'Nonpenetrating Head Injuries',\n",
       " 'Injuries, Closed Head',\n",
       " 'CLOSED HEAD INJ',\n",
       " 'Closed Head Traumas',\n",
       " 'Head Injury, Closed',\n",
       " 'Head Injuries, Nonpenetrating',\n",
       " 'Closed Head Trauma',\n",
       " 'Head Injuries, Closed',\n",
       " 'Head Traumas, Closed',\n",
       " 'Head Injury, Nonpenetrating',\n",
       " 'Closed Head Injury',\n",
       " 'Closed head injuries',\n",
       " 'Head Trauma, Closed',\n",
       " 'Closed injury of head',\n",
       " 'HEAD INJ NONPENETRATING',\n",
       " 'close head injury']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_synonyms1\n",
    "synonyms=get_synonyms1(\"head injury\")\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use orginal spanish text as reference text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tqdm\n",
    "# total_score=[]\n",
    "# file_path = \"/home/mshahidul/project1/all_tran_data/Sampled_100_MedlinePlus_eng_spanish_pair.json\"\n",
    "# from utils import compute_bleu_chrf\n",
    "# with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "#     original_file = json.load(json_file)\n",
    "\n",
    "# for line in tqdm.tqdm(original_file):\n",
    "#     full_prompt, medical_translations, keywords,output = process_sentence(line['english'])\n",
    "#     hypothesis_text=translate_using_prompt(full_prompt,sentence_eng)\n",
    "#     reference_text = line['spanish']\n",
    "#     score=compute_bleu_chrf(reference_text, hypothesis_text)  \n",
    "#     total_score.append({\n",
    "#         \"original_english\": line['english'],\n",
    "#         \"original_spanish\": line['spanish'],\n",
    "#         \"translated_spanish\": hypothesis_text,\n",
    "#         \"bleu_score\": score\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:54<14:08,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unread result found!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:59<12:03,  7.78s/it]"
     ]
    }
   ],
   "source": [
    "## Evaluation\n",
    "from utils import translate_using_prompt\n",
    "output_data = []\n",
    "file_path = \"/home/mshahidul/project1/all_tran_data/dataset/Sampled_100_MedlinePlus_eng_spanish_pair.json\"\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "    sampled_medlineplus_data = json.load(json_file)\n",
    "\n",
    "for x in tqdm.tqdm(sampled_medlineplus_data):\n",
    "    sentence_eng = x['english']\n",
    "    sentence_spa = x['spanish']\n",
    "    \n",
    "    try:\n",
    "        full_prompt, medical_translations, keywords, output = process_sentence(sentence_eng)\n",
    "\n",
    "        output_data.append({\n",
    "            \"Original_English_sentence\": sentence_eng,\n",
    "            \"Original_Spanish_sentence\": sentence_spa,\n",
    "            \"COD_prompt\": full_prompt,\n",
    "            \"medical_translations\": medical_translations,\n",
    "            \"keywords\": keywords,\n",
    "            \"synonyms\": output\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}!!!!\")\n",
    "        continue\n",
    "\n",
    "json_path = \"/home/mshahidul/project1/all_tran_data/dataset/medlineplus_info.json\"\n",
    "with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(output_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Filtered data saved to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use back translation to evaluation the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:27<45:23, 27.51s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 100/100 [23:42<00:00, 14.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /home/mshahidul/project1/results_new/medlineplus_gpt4_mini_COD_(syn)_back_translation.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Evaluation\n",
    "from utils import translate_using_prompt\n",
    "output_data=[]\n",
    "file_path = \"/home/mshahidul/project1/all_tran_data/dataset/Sampled_100_MedlinePlus_eng_spanish_pair.json\"\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "    sampled_medlineplus_data = json.load(json_file)\n",
    "\n",
    "for x in tqdm.tqdm(sampled_medlineplus_data):\n",
    "    sentence_eng = x['english']\n",
    "    sentence_spa = x['spanish']\n",
    "    \n",
    "    try:\n",
    "        full_prompt, medical_translations, keywords,output = process_sentence(sentence_eng)\n",
    "        spa_tran_prompt = translate_using_prompt(full_prompt, sentence_eng)\n",
    "        back_tran_prompt = back_translate(spa_tran_prompt)\n",
    "        \n",
    "        reference_text = [sentence_eng]\n",
    "        hypothesis_text = back_tran_prompt\n",
    "        scores_cod_prompt = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "\n",
    "        output_data.append({\n",
    "            \"Original_English_sentence\": sentence_eng,\n",
    "            \"Original_Spanish_sentence\": sentence_spa,\n",
    "            \"keywords\":keywords,\n",
    "            \"COD_prompt\": full_prompt,\n",
    "            \"spanish_translation_prompt\": spa_tran_prompt,\n",
    "            \"back_translation_prompt\": back_tran_prompt,\n",
    "            \"scores_cod_prompt(bleu and chrf)\": scores_cod_prompt\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}!!!!\")\n",
    "        continue\n",
    "\n",
    "json_path = \"/home/mshahidul/project1/results_new/medlineplus_gpt4_mini_COD_(syn)_back_translation.json\"\n",
    "with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(output_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Data saved to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 60.5105\n"
     ]
    }
   ],
   "source": [
    "avg_bleu_score = sum([x['scores_cod_prompt(bleu and chrf)']['bleu_score'] for x in output_data]) / len(output_data)\n",
    "\n",
    "print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ehr data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "108it [35:59, 19.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /home/mshahidul/project1/results_new/ehr_gpt4_mini_COD_(syn)_back_translation.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Evaluation\n",
    "from utils import translate_using_prompt\n",
    "output_data = []\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "ehr_data = pd.read_excel('/home/mshahidul/project1/all_tran_data/dataset/EHR_data.xlsx')\n",
    "\n",
    "for eng, spa in tqdm.tqdm(zip(ehr_data[\"english\"], ehr_data[\"spain\"])):\n",
    "    sentence_eng = eng\n",
    "    sentence_spa = spa\n",
    "    try:\n",
    "        full_prompt, medical_translations, keywords, output = process_sentence(sentence_eng)\n",
    "        spa_tran_prompt = translate_using_prompt(full_prompt, sentence_eng)\n",
    "        back_tran_prompt = back_translate(spa_tran_prompt)\n",
    "\n",
    "        reference_text = [sentence_eng]\n",
    "        hypothesis_text = back_tran_prompt\n",
    "        scores_cod_prompt = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "\n",
    "        output_data.append({\n",
    "            \"Original_English_sentence\": sentence_eng,\n",
    "            \"Original_Spanish_sentence\": sentence_spa,\n",
    "            \"COD_prompt\": full_prompt,\n",
    "            \"spanish_translation_prompt\": spa_tran_prompt,\n",
    "            \"back_translation_prompt\": back_tran_prompt,\n",
    "            \"scores_cod_prompt(bleu and chrf)\": scores_cod_prompt\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}!!!!\")\n",
    "        continue\n",
    "\n",
    "json_path = \"/home/mshahidul/project1/results_new/ehr_gpt4_mini_COD_(syn)_back_translation.json\"\n",
    "with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(output_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Data saved to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COD + syn (test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import Open\n",
    "AI  # Assuming OpenAI API is set up\n",
    "from utils import get_synonyms, back_translate, compute_bleu_chrf\n",
    "from openai import OpenAI \n",
    "client = OpenAI(api_key=\"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\")\n",
    "import json\n",
    "def translate_using_prompt(prompt,sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from English into Spanish based on above context: {sentence}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    ans=(response.choices[0].message.content)\n",
    "    return ans\n",
    "def back_translate(spa_tran):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                # {\"role\": \"system\", \"content\": f\"\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from Spanish into English: {spa_tran}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    return (response.choices[0].message.content)\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n",
    "\n",
    "# Define the NLLB-200 model\n",
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# List of target languages\n",
    "languages = {\n",
    "    \"FRE\": \"fra_Latn\",\n",
    "    \"GER\": \"deu_Latn\",\n",
    "    \"POR\": \"por_Latn\",\n",
    "    \"SPA\": 'spa_Latn'\n",
    "}\n",
    "\n",
    "# Load translation model\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_directory, torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import ast\n",
    "def extract_keywords(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract medical keywords from the given sentence. return it as python list format without extra things.\"},\n",
    "                  {\"role\": \"user\", \"content\": f\"{sentence}\"}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    keywords = (response.choices[0].message.content)\n",
    "    words_list = ast.literal_eval(keywords)\n",
    "    return words_list  # Expected format: {\"medical\": [\"keyword1\", \"keyword2\"], \"non_medical\": [\"keyword3\", \"keyword4\"]}\n",
    "\n",
    "def search_umls(keyword):\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        \n",
    "        def find_cui(term):\n",
    "            cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{term}%\",))\n",
    "            return cursor.fetchone()\n",
    "        \n",
    "        def find_translations(cui):\n",
    "            cursor.execute(\"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                           (cui, 'FRE', 'POR', 'GER'))\n",
    "            return {row['LAT']: row['STR'] for row in cursor.fetchall()}\n",
    "        \n",
    "        results = {}\n",
    "        words = keyword.split()\n",
    "        for word in words:\n",
    "            result = find_cui(word)\n",
    "            if result:\n",
    "                results[word] = find_translations(result[\"CUI\"])\n",
    "        \n",
    "        return results if results else None\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Database error: {err}\")\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            connection.close()\n",
    "    \n",
    "\n",
    "# def translate_non_medical(keyword):\n",
    "#     translations = {}\n",
    "#     for language, lang_code in languages.items():\n",
    "#         output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "#         translations[language] = output[0]['translation_text']\n",
    "#     return translations\n",
    "\n",
    "def convert_to_chained_format(dictionary, src_lang, target_lang):\n",
    "    chain = []\n",
    "    \n",
    "    for category, words in dictionary.items():\n",
    "        for word, translations in words.items():\n",
    "            formatted_translations = []\n",
    "            \n",
    "            # Ensure the source language is first, target language second, then others\n",
    "            ordered_languages = [\"ENG\", \"SPA\", \"FRE\", \"GER\", \"POR\"]\n",
    "            \n",
    "            for lang in ordered_languages:\n",
    "                if lang in translations:\n",
    "                    formatted_translations.append(f\"'{word}' in {lang} is '{translations[lang]}'\")\n",
    "\n",
    "            chain.append(\". \".join(formatted_translations))\n",
    "    \n",
    "    chained_text = \". \".join(chain) + \".\"\n",
    "    return chained_text\n",
    "def translate_keywords_NLLB(keyword):\n",
    "    translations = {\"ENG\": keyword}  # Ensure English is first\n",
    "    for language, lang_code in languages.items():\n",
    "        output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "        translations[language] = output[0]['translation_text']\n",
    "    return translations\n",
    "\n",
    "def get_keywords(sentence):\n",
    "    df = pd.read_excel(\"/home/mshahidul/project1/testing_dataset_modified.xlsx\")\n",
    "    result = df.loc[df['sentence'] == sentence, 'keywords']\n",
    "    return result.iloc[0] if not result.empty else None\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    # keywords = extract_keywords(sentence)\n",
    "    keywords=get_keywords(sentence)\n",
    "    import ast\n",
    "    keywords=ast.literal_eval(keywords)\n",
    "    # print(keywords)\n",
    "    medical_translations = {}\n",
    "    synonyms_list=[]\n",
    "    # Process medical keywords\n",
    "    for keyword in keywords:\n",
    "        translation = search_umls(keyword)\n",
    "        translation={} if translation is None else translation\n",
    "        synonyms=get_synonyms(keyword)\n",
    "        # print(keyword)\n",
    "        if synonyms:\n",
    "            synonyms_list.append(f\"'{keyword}' synonyms are [{', '.join(synonyms)}].\")\n",
    "        if translation:  # If Spanish missing in UMLS, use NLLB for Spanish\n",
    "            translation=translate_keywords_NLLB(keyword)\n",
    "        else:\n",
    "            translation[\"SPA\"] = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=\"spa_Latn\", max_length=400)[0]['translation_text']\n",
    "        \n",
    "        if translation:\n",
    "            translation['ENG'] = keyword\n",
    "            medical_translations[keyword] = translation\n",
    "\n",
    "    result_json_temp = {\n",
    "        \"medical\": medical_translations\n",
    "    }\n",
    "\n",
    "    src_language = \"English\"\n",
    "    target_language = \"Spanish\"\n",
    "    output2=\" \".join(synonyms_list)\n",
    "    if medical_translations:\n",
    "        chained_output = convert_to_chained_format(result_json_temp, src_language, target_language)\n",
    "        full_prompt=\"Chain of dictionary: \"+chained_output+\"\\n\\nSynonyms: \"+output2\n",
    "    else:\n",
    "        full_prompt=\"Synonyms: \"+output2\n",
    "\n",
    "    return full_prompt, medical_translations, output2\n",
    "\n",
    "# Example usage\n",
    "# sentence = \"If the broken bone punctures the skin, it is called an open fracture (compound fracture).\"\n",
    "# full_prompt,result_json_temp,_1 = process_sentence(sentence)\n",
    "# print(full_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data=[]\n",
    "import tqdm\n",
    "path=\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs.txt\"\n",
    "f=open(path, \"r\").read().split(\"\\n\")\n",
    "for x in tqdm.tqdm(f[:100]):\n",
    "    xx=x.split(\"\\t\")\n",
    "    sentence_eng=xx[0]\n",
    "    sentence_spa=xx[1]\n",
    "    full_prompt,result_json_temp,_ = process_sentence(sentence_eng)\n",
    "    spa_tran=translate_using_prompt(full_prompt,sentence_eng)\n",
    "    # spa_tran_direct=direct_translate(sentence_eng)\n",
    "    back_tran=back_translate(spa_tran)\n",
    "    # back_tran_direct=back_translate(spa_tran_direct)\n",
    "    reference_text = [sentence_eng]\n",
    "    hypothesis_text = back_tran\n",
    "    scores_cod_prompt = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    # hypothesis_text = back_tran_direct\n",
    "    # scores_direct = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    output_data.append({\n",
    "        \"Original_English_sentence\": sentence_eng,\n",
    "        \"Original_Spanish_sentence\": sentence_spa,\n",
    "        \"COD_prompt\": full_prompt,\n",
    "        \"spanish_translation\": spa_tran,\n",
    "        # \"spanish_translation_direct\": spa_tran_direct,\n",
    "        \"back_translation\": back_tran,\n",
    "        # \"back_translation_direct\": back_tran_direct,\n",
    "        \"scores_cod_prompt(bleu and chrf)\": scores_cod_prompt,\n",
    "        # \"scores_direct(bleu and chrf)\": scores_direct\n",
    "    })\n",
    "    # print(scores_cod_prompt,scores_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average scores for COD-prompt-based and direct translations\n",
    "total_bleu_cod = 0\n",
    "total_chrf_cod = 0\n",
    "total_bleu_direct = 0\n",
    "total_chrf_direct = 0\n",
    "count = len(output_data)\n",
    "\n",
    "for entry in output_data:\n",
    "    total_bleu_cod += entry[\"scores_cod_prompt(bleu and chrf)\"][\"bleu_score\"]\n",
    "    total_chrf_cod += entry[\"scores_cod_prompt(bleu and chrf)\"][\"chrF++\"]\n",
    "\n",
    "\n",
    "# Calculate averages\n",
    "avg_bleu_cod = total_bleu_cod / count\n",
    "avg_chrf_cod = total_chrf_cod / count\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average BLEU Score (COD-Prompt-Based Translation): {avg_bleu_cod:.2f}\")\n",
    "print(f\"Average chrF++ Score (COD-Prompt-Based Translation): {avg_chrf_cod:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import OpenAI  # Assuming OpenAI API is set up\n",
    "from utils import *\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n",
    "\n",
    "# Define the NLLB-200 model\n",
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# List of target languages\n",
    "languages = {\n",
    "    \"French\": \"fra_Latn\",\n",
    "    \"German\": \"deu_Latn\",\n",
    "    \"Portuguese\": \"por_Latn\",\n",
    "    \"Spanish\": 'spa_Latn'\n",
    "}\n",
    "\n",
    "# Load translation model\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_directory, torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# OpenAI API Client\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "\n",
    "\n",
    "def search_umls(keyword):\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{keyword}%\",))\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            return None\n",
    "        cui = result[\"CUI\"]\n",
    "\n",
    "        cursor.execute(\"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                       (cui, 'FRE', 'POR', 'GER'))\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        translations = {row['LAT']: row['STR'] for row in rows}\n",
    "        return translations\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Database error: {err}\")\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            connection.close()\n",
    "\n",
    "def translate_non_medical(keyword):\n",
    "    translations = {}\n",
    "    for language, lang_code in languages.items():\n",
    "        output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "        translations[language] = output[0]['translation_text']\n",
    "    return translations\n",
    "\n",
    "\n",
    "def convert_to_chained_format(dictionary, src_lang, target_lang):\n",
    "    chain = []\n",
    "    \n",
    "    for category, words in dictionary.items():\n",
    "        for word, translations in words.items():\n",
    "            formatted_translations = []\n",
    "            \n",
    "            # Ensure the source language is first, target language second, then others\n",
    "            ordered_languages = [\"English\", \"Spanish\", \"French\", \"German\", \"Portuguese\"]\n",
    "            \n",
    "            for lang in ordered_languages:\n",
    "                if lang in translations:\n",
    "                    formatted_translations.append(f\"{word} in {lang} is '{translations[lang]}'\")\n",
    "\n",
    "            chain.append(\". \".join(formatted_translations))\n",
    "    \n",
    "    chained_text = \". \".join(chain) + \".\"\n",
    "    return chained_text\n",
    "\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    keywords = extract_keywords(sentence)\n",
    "    medical_translations = {}\n",
    "    non_medical_translations = {}\n",
    "\n",
    "    # Process medical keywords\n",
    "    for keyword in keywords[\"medical_keywords\"]:\n",
    "        translation = search_umls(keyword)\n",
    "        if \"SPA\" not in translation and translation:  # If Spanish missing in UMLS, use NLLB for Spanish\n",
    "            translation[\"SPA\"] = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=\"spa_Latn\", max_length=400)[0]['translation_text']\n",
    "        if translation:\n",
    "            medical_translations[keyword] = translation\n",
    "\n",
    "    # Process non-medical keywords (always translated via NLLB)\n",
    "    for keyword in keywords[\"non_medical_keywords\"]:\n",
    "        non_medical_translations[keyword] = translate_non_medical(keyword)\n",
    "\n",
    "    result_json_temp = {\n",
    "        \"medical\": medical_translations,\n",
    "        \"non_medical\": non_medical_translations\n",
    "    }\n",
    "\n",
    "    src_language = \"English\"\n",
    "    target_language = \"Spanish\"\n",
    "    chained_output = convert_to_chained_format(result_json_temp, src_language, target_language)\n",
    "\n",
    "    return chained_output, result_json_temp\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Bariatric surgery is done when diet and exercise haven't worked or when you have serious health problems because of your weight.\"\n",
    "COD_prompt,result_json_temp = process_sentence(sentence)\n",
    "print(COD_prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
