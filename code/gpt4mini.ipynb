{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt3='''\n",
    "# The chained multilingual dictionaries:\n",
    "# <word X in source-language> means <word X in target-language> means \n",
    "# <word X in auxiliary-language 1> means <word X in auxiliary-language 2>.\n",
    "# Translate the following text from <source-language> into <target-language>: <source-sentence>\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import OpenAI  # Assuming OpenAI API is set up\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n",
    "\n",
    "# Define the NLLB-200 model\n",
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# List of target languages\n",
    "languages = {\n",
    "    \"French\": \"fra_Latn\",\n",
    "    \"German\": \"deu_Latn\",\n",
    "    \"Portuguese\": \"por_Latn\",\n",
    "    \"Spanish\": 'spa_Latn'\n",
    "}\n",
    "\n",
    "# Load translation model\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_directory, torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# OpenAI API Client\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def extract_keywords(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract medical and non-medical keywords from the given sentence. return it as json format without extra things.\"},\n",
    "                  {\"role\": \"user\", \"content\": f\"{sentence}\"}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    # print(response.choices[0].message.content)\n",
    "    keywords = json.loads(response.choices[0].message.content)\n",
    "    return keywords  # Expected format: {\"medical\": [\"keyword1\", \"keyword2\"], \"non_medical\": [\"keyword3\", \"keyword4\"]}\n",
    "\n",
    "def search_umls(keyword):\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{keyword}%\",))\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            return None\n",
    "        cui = result[\"CUI\"]\n",
    "\n",
    "        cursor.execute(\"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                       (cui, 'FRE', 'POR', 'GER'))\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        translations = {row['LAT']: row['STR'] for row in rows}\n",
    "        return translations\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Database error: {err}\")\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            connection.close()\n",
    "\n",
    "def translate_non_medical(keyword):\n",
    "    translations = {}\n",
    "    for language, lang_code in languages.items():\n",
    "        output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "        translations[language] = output[0]['translation_text']\n",
    "    return translations\n",
    "\n",
    "def convert_to_chained_format(dictionary, src_lang, target_lang):\n",
    "    data={}\n",
    "    for x, y in dictionary.items():\n",
    "        for word,_ in dictionary[x].items():\n",
    "            data[word]=[]\n",
    "            data[word].append(word)\n",
    "            for _, tran2 in dictionary[x][word].items():\n",
    "                data[word].append(tran2)\n",
    "    chain=[]\n",
    "    for word, translations in data.items():\n",
    "        ch=\"\"\n",
    "        for i,x in enumerate(translations):\n",
    "            if i!=len(translations)-1:\n",
    "                ch+=f\"'{x}' means \"\n",
    "            else:\n",
    "                ch+=f\"'{x}'\"\n",
    "        chain.append(ch)\n",
    "    chain=\". \".join(chain)\n",
    "    chain+=\". \"\n",
    "    # chain=f\"{chain}\\nTranslate the following text from {src_lang} into {target_lang}:\"\n",
    "    return chain\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    keywords = extract_keywords(sentence)\n",
    "    medical_translations = {}\n",
    "    non_medical_translations = {}\n",
    "\n",
    "    # Process medical keywords\n",
    "    for keyword in keywords[\"medical_keywords\"]:\n",
    "        translation = search_umls(keyword)\n",
    "        if not translation:  # If keyword not found in UMLS, translate using NLLB\n",
    "            translation = {lang: translator(keyword, src_lang=\"eng_Latn\", tgt_lang=code, max_length=400)[0]['translation_text'] \n",
    "                           for lang, code in languages.items()}\n",
    "        elif \"SPA\" not in translation:  # If Spanish missing in UMLS, use NLLB for Spanish\n",
    "            translation[\"SPA\"] = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=\"spa_Latn\", max_length=400)[0]['translation_text']\n",
    "        \n",
    "        medical_translations[keyword] = translation\n",
    "\n",
    "    # Process non-medical keywords (always translated via NLLB)\n",
    "    for keyword in keywords[\"non_medical_keywords\"]:\n",
    "        non_medical_translations[keyword] = translate_non_medical(keyword)\n",
    "\n",
    "    result_json_temp = {\n",
    "        \"medical\": medical_translations,\n",
    "        \"non_medical\": non_medical_translations\n",
    "    }\n",
    "\n",
    "    src_language = \"English\"\n",
    "    target_language = \"Spanish\"\n",
    "    chained_output = convert_to_chained_format(result_json_temp, src_language, target_language)\n",
    "\n",
    "    return chained_output, result_json_temp\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Bariatric surgery is done when diet and exercise haven't worked or when you have serious health problems because of your weight.\"\n",
    "COD_prompt,result_json_temp = process_sentence(sentence)\n",
    "# print(COD_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate using COD prompt in Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_cod_prompt(COD_prompt,sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"{COD_prompt}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from English into Spanish: {sentence}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    ans=(response.choices[0].message.content)\n",
    "    return ans\n",
    "spa_tran=translate_cod_prompt(COD_prompt,sentence)\n",
    "spa_tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct translate in Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_translate(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from English into Spanish: {sentence}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    return (response.choices[0].message.content)\n",
    "spa_tran_direct=direct_translate(sentence)\n",
    "spa_tran_direct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back translate in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate(spa_tran):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                # {\"role\": \"system\", \"content\": f\"\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from Spanish into English: {spa_tran}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    return (response.choices[0].message.content)\n",
    "back_tran=back_translate(spa_tran)\n",
    "back_tran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back translation in English without COD prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_tran_direct=back_translate(spa_tran_direct)\n",
    "back_tran_direct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "def compute_bleu_chrf(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Computes the BLEU and chrF++ scores for a given reference and hypothesis.\n",
    "    \n",
    "    :param reference: List of reference translations (list of strings)\n",
    "    :param hypothesis: The hypothesis translation (a single string)\n",
    "    :return: A dictionary containing BLEU and chrF++ scores\n",
    "    \"\"\"\n",
    "    # Ensure reference is wrapped in a list as sacrebleu expects a list of references\n",
    "    # bleu_score = sacrebleu.corpus_bleu(hypothesis, [reference]).score\n",
    "    # bleu_score = sacrebleu.corpus_bleu(hypothesis, [reference], tokenize=\"13a\", lowercase=True).score\n",
    "    bleu_score=metric.compute(predictions=[hypothesis], references=[reference])\n",
    "    chrf_score = sacrebleu.corpus_chrf(hypothesis, [reference]).score\n",
    "\n",
    "    return {\"bleu_score\": bleu_score['score'],\"chrF++\": chrf_score}\n",
    "\n",
    "# Example usage\n",
    "# reference_text = [sentence]\n",
    "# hypothesis_text = back_tran\n",
    "# scores = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "# print(\"COD prompt: \",scores)\n",
    "# reference_text = [sentence]\n",
    "# hypothesis_text = back_tran_direct\n",
    "# scores = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "# print(\"Direct translation: \",scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation\n",
    "output_data=[]\n",
    "import tqdm\n",
    "path=\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs.txt\"\n",
    "f=open(path, \"r\").read().split(\"\\n\")\n",
    "for x in tqdm.tqdm(f[:100]):\n",
    "    xx=x.split(\"\\t\")\n",
    "    sentence_eng=xx[0]\n",
    "    sentence_spa=xx[1]\n",
    "    COD_prompt,result_json_temp = process_sentence(sentence_eng)\n",
    "    spa_tran=translate_cod_prompt(COD_prompt,sentence_eng)\n",
    "    spa_tran_direct=direct_translate(sentence_eng)\n",
    "    back_tran=back_translate(spa_tran)\n",
    "    back_tran_direct=back_translate(spa_tran_direct)\n",
    "    reference_text = [sentence_eng]\n",
    "    hypothesis_text = back_tran\n",
    "    scores_cod_prompt = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    hypothesis_text = back_tran_direct\n",
    "    scores_direct = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    output_data.append({\n",
    "        \"Original_English_sentence\": sentence_eng,\n",
    "        \"Original_Spanish_sentence\": sentence_spa,\n",
    "        \"COD_prompt\": COD_prompt,\n",
    "        \"spanish_translation\": spa_tran,\n",
    "        \"spanish_translation_direct\": spa_tran_direct,\n",
    "        \"back_translation\": back_tran,\n",
    "        \"back_translation_direct\": back_tran_direct,\n",
    "        \"scores_cod_prompt(bleu and chrf)\": scores_cod_prompt,\n",
    "        \"scores_direct(bleu and chrf)\": scores_direct\n",
    "    })\n",
    "    # print(scores_cod_prompt,scores_direct)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(output_data)\n",
    "df.to_csv(\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs_gpt4_mini.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs_gpt4_mini.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs.txt\"\n",
    "f=open(path, \"r\").read().split(\"\\n\")\n",
    "for x in f[:100]:\n",
    "    xx=x.split(\"\\t\")\n",
    "    sentence_eng=xx[0]\n",
    "    sentence_spa=xx[1]\n",
    "    COD_prompt,result_json_temp = process_sentence(sentence_eng)\n",
    "    spa_tran=translate_cod_prompt(COD_prompt, sentence_eng)\n",
    "    reference_text = [sentence_spa]\n",
    "    hypothesis_text = spa_tran\n",
    "    scores_cod_prompt = compute_bleu_chrf(reference_text, hypothesis_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COD + syn (test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import OpenAI  # Assuming OpenAI API is set up\n",
    "from utils import get_synonyms, back_translate, compute_bleu_chrf\n",
    "from openai import OpenAI \n",
    "client = OpenAI(api_key=\"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\")\n",
    "import json\n",
    "def translate_using_prompt(prompt,sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from English into Spanish: {sentence}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    ans=(response.choices[0].message.content)\n",
    "    return ans\n",
    "def back_translate(spa_tran):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                # {\"role\": \"system\", \"content\": f\"\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from Spanish into English: {spa_tran}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    return (response.choices[0].message.content)\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n",
    "\n",
    "# Define the NLLB-200 model\n",
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# List of target languages\n",
    "languages = {\n",
    "    \"French\": \"fra_Latn\",\n",
    "    \"German\": \"deu_Latn\",\n",
    "    \"Portuguese\": \"por_Latn\",\n",
    "    \"Spanish\": 'spa_Latn'\n",
    "}\n",
    "\n",
    "# Load translation model\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_directory, torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# OpenAI API Client\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def extract_keywords(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract medical and non-medical keywords from the given sentence. return it as json format without extra things.\"},\n",
    "                  {\"role\": \"user\", \"content\": f\"{sentence}\"}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    # print(response.choices[0].message.content)\n",
    "    keywords = json.loads(response.choices[0].message.content)\n",
    "    return keywords  # Expected format: {\"medical\": [\"keyword1\", \"keyword2\"], \"non_medical\": [\"keyword3\", \"keyword4\"]}\n",
    "\n",
    "def search_umls(keyword):\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{keyword}%\",))\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            return None\n",
    "        cui = result[\"CUI\"]\n",
    "\n",
    "        cursor.execute(\"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                       (cui, 'FRE', 'POR', 'GER'))\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        translations = {row['LAT']: row['STR'] for row in rows}\n",
    "        return translations\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Database error: {err}\")\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            connection.close()\n",
    "\n",
    "# def translate_non_medical(keyword):\n",
    "#     translations = {}\n",
    "#     for language, lang_code in languages.items():\n",
    "#         output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "#         translations[language] = output[0]['translation_text']\n",
    "#     return translations\n",
    "\n",
    "def convert_to_chained_format(dictionary, src_lang, target_lang):\n",
    "    chain = []\n",
    "    \n",
    "    for category, words in dictionary.items():\n",
    "        for word, translations in words.items():\n",
    "            formatted_translations = []\n",
    "            \n",
    "            # Ensure the source language is first, target language second, then others\n",
    "            ordered_languages = [\"ENG\", \"SPA\", \"FRE\", \"GER\", \"POR\"]\n",
    "            \n",
    "            for lang in ordered_languages:\n",
    "                if lang in translations:\n",
    "                    formatted_translations.append(f\"'{word}' in {lang} is '{translations[lang]}'\")\n",
    "\n",
    "            chain.append(\". \".join(formatted_translations))\n",
    "    \n",
    "    chained_text = \". \".join(chain) + \".\"\n",
    "    return chained_text\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    keywords = extract_keywords(sentence)\n",
    "    medical_translations = {}\n",
    "    output=[]\n",
    "    # Process medical keywords\n",
    "    for keyword in keywords[\"medical_keywords\"]:\n",
    "        translation={}\n",
    "        translation = search_umls(keyword)\n",
    "        synonyms=get_synonyms(keyword)\n",
    "        # print(keyword)\n",
    "        if synonyms:\n",
    "            output.append(f\"'{keyword}' synonyms are [{', '.join(synonyms)}].\")\n",
    "        if translation and \"SPA\" not in translation:  # If Spanish missing in UMLS, use NLLB for Spanish\n",
    "            translation[\"SPA\"] = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=\"spa_Latn\", max_length=400)[0]['translation_text']\n",
    "        \n",
    "        if translation:\n",
    "            translation['ENG'] = keyword\n",
    "            medical_translations[keyword] = translation\n",
    "\n",
    "    result_json_temp = {\n",
    "        \"medical\": medical_translations\n",
    "    }\n",
    "\n",
    "    src_language = \"English\"\n",
    "    target_language = \"Spanish\"\n",
    "    output=\" \".join(output)\n",
    "    if medical_translations:\n",
    "        chained_output = convert_to_chained_format(result_json_temp, src_language, target_language)\n",
    "        full_prompt=\"Chain of dictionary: \"+chained_output+\"\\n\\nSynonyms: \"+output\n",
    "    else:\n",
    "        full_prompt=\"Synonyms: \"+output\n",
    "\n",
    "    return full_prompt, result_json_temp, output\n",
    "\n",
    "# Example usage\n",
    "# sentence = \"A stress fracture is a hairline crack in the bone that develops because of repeated or prolonged forces against the bone.\"\n",
    "# full_prompt,result_json_temp,_1 = process_sentence(sentence)\n",
    "# print(full_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data=[]\n",
    "import tqdm\n",
    "path=\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs.txt\"\n",
    "f=open(path, \"r\").read().split(\"\\n\")\n",
    "for x in tqdm.tqdm(f[:100]):\n",
    "    print(x)\n",
    "    xx=x.split(\"\\t\")\n",
    "    sentence_eng=xx[0]\n",
    "    sentence_spa=xx[1]\n",
    "    full_prompt,result_json_temp,_ = process_sentence(sentence_eng)\n",
    "    spa_tran=translate_using_prompt(full_prompt,sentence_eng)\n",
    "    # spa_tran_direct=direct_translate(sentence_eng)\n",
    "    back_tran=back_translate(spa_tran)\n",
    "    # back_tran_direct=back_translate(spa_tran_direct)\n",
    "    reference_text = [sentence_eng]\n",
    "    hypothesis_text = back_tran\n",
    "    scores_cod_prompt = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    # hypothesis_text = back_tran_direct\n",
    "    # scores_direct = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    output_data.append({\n",
    "        \"Original_English_sentence\": sentence_eng,\n",
    "        \"Original_Spanish_sentence\": sentence_spa,\n",
    "        \"COD_prompt\": full_prompt,\n",
    "        \"spanish_translation\": spa_tran,\n",
    "        # \"spanish_translation_direct\": spa_tran_direct,\n",
    "        \"back_translation\": back_tran,\n",
    "        # \"back_translation_direct\": back_tran_direct,\n",
    "        \"scores_cod_prompt(bleu and chrf)\": scores_cod_prompt,\n",
    "        # \"scores_direct(bleu and chrf)\": scores_direct\n",
    "    })\n",
    "    # print(scores_cod_prompt,scores_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average scores for COD-prompt-based and direct translations\n",
    "total_bleu_cod = 0\n",
    "total_chrf_cod = 0\n",
    "total_bleu_direct = 0\n",
    "total_chrf_direct = 0\n",
    "count = len(output_data)\n",
    "\n",
    "for entry in output_data:\n",
    "    total_bleu_cod += entry[\"scores_cod_prompt(bleu and chrf)\"][\"bleu_score\"]\n",
    "    total_chrf_cod += entry[\"scores_cod_prompt(bleu and chrf)\"][\"chrF++\"]\n",
    "\n",
    "\n",
    "# Calculate averages\n",
    "avg_bleu_cod = total_bleu_cod / count\n",
    "avg_chrf_cod = total_chrf_cod / count\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average BLEU Score (COD-Prompt-Based Translation): {avg_bleu_cod:.2f}\")\n",
    "print(f\"Average chrF++ Score (COD-Prompt-Based Translation): {avg_chrf_cod:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COD + syn (test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import OpenAI  # Assuming OpenAI API is set up\n",
    "from utils import get_synonyms, back_translate, compute_bleu_chrf\n",
    "from openai import OpenAI \n",
    "client = OpenAI(api_key=\"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\")\n",
    "import json\n",
    "def translate_using_prompt(prompt,sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"{prompt}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from English into Spanish based on above context: {sentence}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    ans=(response.choices[0].message.content)\n",
    "    return ans\n",
    "def back_translate(spa_tran):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                # {\"role\": \"system\", \"content\": f\"\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from Spanish into English: {spa_tran}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    return (response.choices[0].message.content)\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n",
    "\n",
    "# Define the NLLB-200 model\n",
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# List of target languages\n",
    "languages = {\n",
    "    \"FRE\": \"fra_Latn\",\n",
    "    \"GER\": \"deu_Latn\",\n",
    "    \"POR\": \"por_Latn\",\n",
    "    \"SPA\": 'spa_Latn'\n",
    "}\n",
    "\n",
    "# Load translation model\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_directory, torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain of dictionary: 'broken bone' in ENG is 'broken bone'. 'broken bone' in SPA is 'hueso roto'. 'broken bone' in FRE is 'fracture d'os'. 'broken bone' in GER is 'Knochenbruch'. 'broken bone' in POR is 'ossos quebrados'. 'punctures' in ENG is 'punctures'. 'punctures' in SPA is 'pinchazos'. 'punctures' in FRE is 'points'. 'punctures' in GER is 'Punktionen'. 'punctures' in POR is 'Punções'. 'skin' in ENG is 'skin'. 'skin' in SPA is 'piel y'. 'skin' in FRE is 'la peau'. 'skin' in GER is 'Haut und Haut'. 'skin' in POR is 'pele'. 'open fracture' in ENG is 'open fracture'. 'open fracture' in SPA is 'fractura abierta'. 'open fracture' in FRE is 'fracture ouverte'. 'open fracture' in GER is 'offene Fraktur'. 'open fracture' in POR is 'fractura aberta'. 'compound fracture' in ENG is 'compound fracture'. 'compound fracture' in SPA is 'fractura compuesta'. 'compound fracture' in FRE is 'fracture composée'. 'compound fracture' in GER is 'Verbundfraktur'. 'compound fracture' in POR is 'fractura composta'.\n",
      "\n",
      "Synonyms: 'broken bone' synonyms are [Fracture of unspecified bones (disorder), Fracture of bone (disorder), Fracture (morphologic abnormality), bone fractures, Fracture, cause NOS, Bone, Broken, bone fracture, Broken bone, fractured bone, bone; fracture, Fracture of bone, Fractures, FRACTURE BONE, Fracture - lesion, Fracture - lesion (disorder), Fracture, cause unspecified (disorder), Bones, Broken, Fracture: [of unspecified bones] or [NOS] (disorder), Fracture, bone, Fracture, cause unspecified, Fracture NOS, Fractured, Fractures, Bone, bones fractured, broken bones, Fracture, Fracture of bones NOS, fracture; bone, Fracture - lesion (morphologic abnormality), Diaphyseal fracture, 12 FRACTURES, Fracture, NOS, Fracture, cause unspecified (finding), Fracture NOS (disorder), bone fractured, Fracture of unspecified bones, Fracture (disorder), Fracture of bones NOS (disorder), bone fracture (diagnosis), Fracture: [of unspecified bones] or [NOS]]. 'punctures' synonyms are [Diagnostic spinal puncture (& lumbar) (procedure), diagnostic spinal puncture, Diagnostic lumbar spinal puncture (procedure), Diagnostic spinal puncture NOS, Diagnostic lumbar spinal puncture (procedure) [Ambiguous], DX LMBR SPI PNXR, LP, Spinal Puncture, Spinal tap, Puncture, Spinal, Spinal Punctures, Lumbar Punctures, Punctures, Spinal, Spinal puncture and aspiration, Spinal puncture, lumbar, diagnostic, Rachicentesis, Lumbar puncture, NOS, spinal tapping, diagnostic lumbar puncture (lab test), Puncture, Lumbar, Diagnostic lumbar puncture, Taps, Spinal, Tap, Spinal, Punctures, Lumbar, Diagnostic spinal tap, Diagnostic lumbar tap, Diagnostic lumbar puncture (procedure), Diagnostic spinal puncture NOS (procedure), Diagnostic lumbar spinal puncture, Lumbar puncture, Diagnostic spinal puncture (& lumbar), spinal taps]. 'skin' synonyms are [VESICULOBULLOUS SKIN DIS, Skin Diseases, Vesiculobullous, Skin Disease, Vesiculobullous, Dermatoses, Vesiculobullous, Vesiculobullous Skin Disease, Vesiculobullous Skin Diseases, SKIN DIS VESICULOBULLOUS, Vesiculobullous Dermatoses, Vesiculobullous disease of skin, Vesiculobullous dermatosis]. 'open fracture' synonyms are [Open fracture of bones, unspecified (disorder), Fracture, compound, NOS, Fracture;compound, Open fracture (disorder), open fracture (diagnosis), fractures open, Open fracture of bones, unspecified, Open Fracture, 122 OPEN FRACTURES, Fracture, open, NOS, Fracture, open (morphologic abnormality), Fracture, compound, fracture compound, Fracture NOS-open, Open Fractures, fracture opened, Fractures, Compound, Compound Fractures, Open #bones unspecified, FRACTURE OPEN, Fracture-open, Fracture of unspecified bone, open, Fracture, open, Compound Fracture, Fractures, Open]. 'compound fracture' synonyms are [Open fracture of bones, unspecified (disorder), Fracture, compound, NOS, Fracture;compound, Open fracture (disorder), open fracture (diagnosis), fractures open, Open fracture of bones, unspecified, Open Fracture, 122 OPEN FRACTURES, Fracture, open, NOS, Fracture, open (morphologic abnormality), Fracture, compound, fracture compound, Fracture NOS-open, Open Fractures, fracture opened, Fractures, Compound, Compound Fractures, Open #bones unspecified, FRACTURE OPEN, Fracture-open, Fracture of unspecified bone, open, Fracture, open, Compound Fracture, Fractures, Open].\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import ast\n",
    "def extract_keywords(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract medical keywords from the given sentence. return it as python list format without extra things.\"},\n",
    "                  {\"role\": \"user\", \"content\": f\"{sentence}\"}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    keywords = (response.choices[0].message.content)\n",
    "    words_list = ast.literal_eval(keywords)\n",
    "    return words_list  # Expected format: {\"medical\": [\"keyword1\", \"keyword2\"], \"non_medical\": [\"keyword3\", \"keyword4\"]}\n",
    "\n",
    "def search_umls(keyword):\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        \n",
    "        def find_cui(term):\n",
    "            cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{term}%\",))\n",
    "            return cursor.fetchone()\n",
    "        \n",
    "        def find_translations(cui):\n",
    "            cursor.execute(\"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                           (cui, 'FRE', 'POR', 'GER'))\n",
    "            return {row['LAT']: row['STR'] for row in cursor.fetchall()}\n",
    "        \n",
    "        results = {}\n",
    "        words = keyword.split()\n",
    "        for word in words:\n",
    "            result = find_cui(word)\n",
    "            if result:\n",
    "                results[word] = find_translations(result[\"CUI\"])\n",
    "        \n",
    "        return results if results else None\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Database error: {err}\")\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            connection.close()\n",
    "    \n",
    "\n",
    "# def translate_non_medical(keyword):\n",
    "#     translations = {}\n",
    "#     for language, lang_code in languages.items():\n",
    "#         output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "#         translations[language] = output[0]['translation_text']\n",
    "#     return translations\n",
    "\n",
    "def convert_to_chained_format(dictionary, src_lang, target_lang):\n",
    "    chain = []\n",
    "    \n",
    "    for category, words in dictionary.items():\n",
    "        for word, translations in words.items():\n",
    "            formatted_translations = []\n",
    "            \n",
    "            # Ensure the source language is first, target language second, then others\n",
    "            ordered_languages = [\"ENG\", \"SPA\", \"FRE\", \"GER\", \"POR\"]\n",
    "            \n",
    "            for lang in ordered_languages:\n",
    "                if lang in translations:\n",
    "                    formatted_translations.append(f\"'{word}' in {lang} is '{translations[lang]}'\")\n",
    "\n",
    "            chain.append(\". \".join(formatted_translations))\n",
    "    \n",
    "    chained_text = \". \".join(chain) + \".\"\n",
    "    return chained_text\n",
    "def translate_keywords_NLLB(keyword):\n",
    "    translations = {\"ENG\": keyword}  # Ensure English is first\n",
    "    for language, lang_code in languages.items():\n",
    "        output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "        translations[language] = output[0]['translation_text']\n",
    "    return translations\n",
    "\n",
    "def get_keywords(sentence):\n",
    "    df = pd.read_excel(\"/home/mshahidul/project1/testing_dataset_modified.xlsx\")\n",
    "    result = df.loc[df['sentence'] == sentence, 'keywords']\n",
    "    return result.iloc[0] if not result.empty else None\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    # keywords = extract_keywords(sentence)\n",
    "    keywords=get_keywords(sentence)\n",
    "    import ast\n",
    "    keywords=ast.literal_eval(keywords)\n",
    "    # print(keywords)\n",
    "    medical_translations = {}\n",
    "    synonyms_list=[]\n",
    "    # Process medical keywords\n",
    "    for keyword in keywords:\n",
    "        translation = search_umls(keyword)\n",
    "        translation={} if translation is None else translation\n",
    "        synonyms=get_synonyms(keyword)\n",
    "        # print(keyword)\n",
    "        if synonyms:\n",
    "            synonyms_list.append(f\"'{keyword}' synonyms are [{', '.join(synonyms)}].\")\n",
    "        if translation:  # If Spanish missing in UMLS, use NLLB for Spanish\n",
    "            translation=translate_keywords_NLLB(keyword)\n",
    "        else:\n",
    "            translation[\"SPA\"] = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=\"spa_Latn\", max_length=400)[0]['translation_text']\n",
    "        \n",
    "        if translation:\n",
    "            translation['ENG'] = keyword\n",
    "            medical_translations[keyword] = translation\n",
    "\n",
    "    result_json_temp = {\n",
    "        \"medical\": medical_translations\n",
    "    }\n",
    "\n",
    "    src_language = \"English\"\n",
    "    target_language = \"Spanish\"\n",
    "    output2=\" \".join(synonyms_list)\n",
    "    if medical_translations:\n",
    "        chained_output = convert_to_chained_format(result_json_temp, src_language, target_language)\n",
    "        full_prompt=\"Chain of dictionary: \"+chained_output+\"\\n\\nSynonyms: \"+output2\n",
    "    else:\n",
    "        full_prompt=\"Synonyms: \"+output2\n",
    "\n",
    "    return full_prompt, medical_translations, output2\n",
    "\n",
    "# Example usage\n",
    "# sentence = \"If the broken bone punctures the skin, it is called an open fracture (compound fracture).\"\n",
    "# full_prompt,result_json_temp,_1 = process_sentence(sentence)\n",
    "# print(full_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:14<08:17,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n",
      "No CUI found for the keyword.\n",
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [01:42<20:47, 13.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [02:03<17:00, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [02:31<24:32, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [03:34<17:09, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [04:11<15:03, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [04:40<11:26,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [05:29<07:48,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n",
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [06:08<19:38, 16.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [06:39<12:46, 10.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [07:38<11:16, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [08:20<16:27, 15.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [09:04<13:32, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [09:48<09:02,  9.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [10:11<12:32, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [11:49<03:50,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [12:37<06:11,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [13:34<08:28, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [14:12<04:19,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n",
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [16:22<05:34, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n",
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [17:12<07:27, 16.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n",
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [18:51<03:30, 10.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [19:59<04:15, 14.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [20:26<05:03, 17.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [20:51<05:19, 19.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUI found for the keyword.\n",
      "No CUI found for the keyword.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [22:04<00:00, 13.24s/it]\n"
     ]
    }
   ],
   "source": [
    "output_data=[]\n",
    "import tqdm\n",
    "path=\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs.txt\"\n",
    "f=open(path, \"r\").read().split(\"\\n\")\n",
    "for x in tqdm.tqdm(f[:100]):\n",
    "    xx=x.split(\"\\t\")\n",
    "    sentence_eng=xx[0]\n",
    "    sentence_spa=xx[1]\n",
    "    full_prompt,result_json_temp,_ = process_sentence(sentence_eng)\n",
    "    spa_tran=translate_using_prompt(full_prompt,sentence_eng)\n",
    "    # spa_tran_direct=direct_translate(sentence_eng)\n",
    "    back_tran=back_translate(spa_tran)\n",
    "    # back_tran_direct=back_translate(spa_tran_direct)\n",
    "    reference_text = [sentence_eng]\n",
    "    hypothesis_text = back_tran\n",
    "    scores_cod_prompt = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    # hypothesis_text = back_tran_direct\n",
    "    # scores_direct = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    output_data.append({\n",
    "        \"Original_English_sentence\": sentence_eng,\n",
    "        \"Original_Spanish_sentence\": sentence_spa,\n",
    "        \"COD_prompt\": full_prompt,\n",
    "        \"spanish_translation\": spa_tran,\n",
    "        # \"spanish_translation_direct\": spa_tran_direct,\n",
    "        \"back_translation\": back_tran,\n",
    "        # \"back_translation_direct\": back_tran_direct,\n",
    "        \"scores_cod_prompt(bleu and chrf)\": scores_cod_prompt,\n",
    "        # \"scores_direct(bleu and chrf)\": scores_direct\n",
    "    })\n",
    "    # print(scores_cod_prompt,scores_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score (COD-Prompt-Based Translation): 51.98\n",
      "Average chrF++ Score (COD-Prompt-Based Translation): 1.84\n"
     ]
    }
   ],
   "source": [
    "# Compute average scores for COD-prompt-based and direct translations\n",
    "total_bleu_cod = 0\n",
    "total_chrf_cod = 0\n",
    "total_bleu_direct = 0\n",
    "total_chrf_direct = 0\n",
    "count = len(output_data)\n",
    "\n",
    "for entry in output_data:\n",
    "    total_bleu_cod += entry[\"scores_cod_prompt(bleu and chrf)\"][\"bleu_score\"]\n",
    "    total_chrf_cod += entry[\"scores_cod_prompt(bleu and chrf)\"][\"chrF++\"]\n",
    "\n",
    "\n",
    "# Calculate averages\n",
    "avg_bleu_cod = total_bleu_cod / count\n",
    "avg_chrf_cod = total_chrf_cod / count\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average BLEU Score (COD-Prompt-Based Translation): {avg_bleu_cod:.2f}\")\n",
    "print(f\"Average chrF++ Score (COD-Prompt-Based Translation): {avg_chrf_cod:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import OpenAI  # Assuming OpenAI API is set up\n",
    "from utils import *\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n",
    "\n",
    "# Define the NLLB-200 model\n",
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# List of target languages\n",
    "languages = {\n",
    "    \"French\": \"fra_Latn\",\n",
    "    \"German\": \"deu_Latn\",\n",
    "    \"Portuguese\": \"por_Latn\",\n",
    "    \"Spanish\": 'spa_Latn'\n",
    "}\n",
    "\n",
    "# Load translation model\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_directory, torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# OpenAI API Client\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "\n",
    "\n",
    "def search_umls(keyword):\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{keyword}%\",))\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            return None\n",
    "        cui = result[\"CUI\"]\n",
    "\n",
    "        cursor.execute(\"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                       (cui, 'FRE', 'POR', 'GER'))\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        translations = {row['LAT']: row['STR'] for row in rows}\n",
    "        return translations\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Database error: {err}\")\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            connection.close()\n",
    "\n",
    "def translate_non_medical(keyword):\n",
    "    translations = {}\n",
    "    for language, lang_code in languages.items():\n",
    "        output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "        translations[language] = output[0]['translation_text']\n",
    "    return translations\n",
    "\n",
    "\n",
    "def convert_to_chained_format(dictionary, src_lang, target_lang):\n",
    "    chain = []\n",
    "    \n",
    "    for category, words in dictionary.items():\n",
    "        for word, translations in words.items():\n",
    "            formatted_translations = []\n",
    "            \n",
    "            # Ensure the source language is first, target language second, then others\n",
    "            ordered_languages = [\"English\", \"Spanish\", \"French\", \"German\", \"Portuguese\"]\n",
    "            \n",
    "            for lang in ordered_languages:\n",
    "                if lang in translations:\n",
    "                    formatted_translations.append(f\"{word} in {lang} is '{translations[lang]}'\")\n",
    "\n",
    "            chain.append(\". \".join(formatted_translations))\n",
    "    \n",
    "    chained_text = \". \".join(chain) + \".\"\n",
    "    return chained_text\n",
    "\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    keywords = extract_keywords(sentence)\n",
    "    medical_translations = {}\n",
    "    non_medical_translations = {}\n",
    "\n",
    "    # Process medical keywords\n",
    "    for keyword in keywords[\"medical_keywords\"]:\n",
    "        translation = search_umls(keyword)\n",
    "        if \"SPA\" not in translation and translation:  # If Spanish missing in UMLS, use NLLB for Spanish\n",
    "            translation[\"SPA\"] = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=\"spa_Latn\", max_length=400)[0]['translation_text']\n",
    "        if translation:\n",
    "            medical_translations[keyword] = translation\n",
    "\n",
    "    # Process non-medical keywords (always translated via NLLB)\n",
    "    for keyword in keywords[\"non_medical_keywords\"]:\n",
    "        non_medical_translations[keyword] = translate_non_medical(keyword)\n",
    "\n",
    "    result_json_temp = {\n",
    "        \"medical\": medical_translations,\n",
    "        \"non_medical\": non_medical_translations\n",
    "    }\n",
    "\n",
    "    src_language = \"English\"\n",
    "    target_language = \"Spanish\"\n",
    "    chained_output = convert_to_chained_format(result_json_temp, src_language, target_language)\n",
    "\n",
    "    return chained_output, result_json_temp\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Bariatric surgery is done when diet and exercise haven't worked or when you have serious health problems because of your weight.\"\n",
    "COD_prompt,result_json_temp = process_sentence(sentence)\n",
    "print(COD_prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
