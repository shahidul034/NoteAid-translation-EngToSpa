{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Translate the text English to Spanish: Hello, my name is Jemmy.\",\n",
    "    \"Translate the text English to Spanish: The president of the United States is Kane.\",\n",
    "    \"Translate the text English to Spanish: The capital of France is Paris\",\n",
    "    \"Translate the text English to Spanish: The future of AI is bright.\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=1.0, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=\"unsloth/Qwen2.5-32B-Instruct\",dtype=\"half\",tensor_parallel_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COD_plus={\n",
    "    \"prompt1.1\":\"Expand this chain of dictionary translations by adding missing languages where appropriate: \",\n",
    "    \"prompt1.2\": \"Identify any missing translations in this language chain and complete them: \",\n",
    "    \"prompt2.1\":\"For each word in this chain of dictionary translations, add appropriate synonyms in parentheses: \",\n",
    "    \"prompt2.2\": \"Enhance this dictionary chain by inserting synonyms for each term in the respective language: \",\n",
    "    \"prompt3.1\":\"Expand this dictionary chain by adding one-hop related concepts from a knowledge graph. Include medical, biological, or psychological terms where relevant: \",\n",
    "    \"prompt3.2\": \"Enhance this chain of dictionary translations by inserting related concepts (such as medical conditions, biological processes, or symptoms) at each step: \"\n",
    "}\n",
    "for x in COD_plus:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = '/home/mshahidul/project1/results_new/prompt_extention_testing.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data[2]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_medline = '/home/mshahidul/project1/results_new/Medline/Qwen2.5-14B-Instruct_without_finetuned_medline.json'\n",
    "import json\n",
    "with open(file_path_medline, 'r') as file:\n",
    "    medline_data = json.load(file)\n",
    "    columns = medline_data[0].keys()\n",
    "    print(columns)\n",
    "sum([x['bleu_score']['bleu_score'] for x in (medline_data)])/len(medline_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "# from google import genai\n",
    "import json\n",
    "client = OpenAI(api_key=json.load(open('/home/mshahidul/project1/api.json', 'r'))['openai_api'])\n",
    "# client = genai.Client(api_key=json.load(open('/home/mshahidul/project1/api.json', 'r'))['gemini_api'])\n",
    "url = \"http://sirchus.com/gpu_status.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    text = response.text\n",
    "    txt=f'''\n",
    "        {text}\n",
    "\n",
    "        Find the free available GPU (GPU Memory usage<=10%) using above context. Sort available GPU based on high configuration to low configuration. Show this table format.\n",
    "        Add gpu information:\n",
    "        GPU server 1 (omega):IP: 172.16.34.1\n",
    "        GPU server 2 (alpha):IP: 172.16.34.21\n",
    "        GPU server 3 (beta): IP: 172.16.34.22 \n",
    "        GPU server 4 (gamma): IP: 172.16.34.29\n",
    "        '''\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"{txt}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    ans=(response.choices[0].message.content)\n",
    "    \n",
    "    print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "client = genai.Client(api_key=json.load(open('/home/mshahidul/project1/api.json', 'r'))['gemini_api'])\n",
    "url = \"http://sirchus.com/gpu_status.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    text = response.text\n",
    "    txt=f'''\n",
    "        {text}\n",
    "\n",
    "        Find the free available GPU (GPU Memory usage<=10%) using above context. Sort available GPU based on high configuration to low configuration. Show this table format.\n",
    "        Add gpu information:\n",
    "        GPU server 1 (omega):IP: 172.16.34.1\n",
    "        GPU server 2 (alpha):IP: 172.16.34.21\n",
    "        GPU server 3 (beta): IP: 172.16.34.22 \n",
    "        GPU server 4 (gamma): IP: 172.16.34.29\n",
    "        '''\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=f\"{txt}\",\n",
    "    )\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "import requests\n",
    "client = Groq(\n",
    "    api_key=json.load(open('/home/mshahidul/project1/api.json', 'r'))['groq_api'],\n",
    ")\n",
    "url = \"http://sirchus.com/gpu_status.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    text = response.text\n",
    "    txt=f'''\n",
    "        {text}\n",
    "\n",
    "        Find the free available GPU (GPU Memory usage<=10%) using above context. Sort available GPU based on high configuration to low configuration. Show this table format.\n",
    "        Add gpu information:\n",
    "        GPU server 1 (omega):IP: 172.16.34.1\n",
    "        GPU server 2 (alpha):IP: 172.16.34.21\n",
    "        GPU server 3 (beta): IP: 172.16.34.22 \n",
    "        GPU server 4 (gamma): IP: 172.16.34.29\n",
    "        '''\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"{txt}\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "    )\n",
    "\n",
    "    print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = \"/home/mshahidul/project1/all_tran_data/dataset/Sampled_100_MedlinePlus_eng_spanish_pair.json\"\n",
    "with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "    original_file = json.load(json_file)\n",
    "with open('/home/mshahidul/project1/all_tran_data/updated_keywords.json', 'r', encoding='utf-8') as keywords_file:\n",
    "    keywords = json.load(keywords_file)\n",
    "ans=0\n",
    "not_exist_count = 0\n",
    "xx=[x['sentence'] for x in (keywords)]\n",
    "for x in original_file:\n",
    "    if x['english'] not in xx:\n",
    "        print(x['english'])\n",
    "        not_exist_count+=1\n",
    "# print(not_exist_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load original dataset\n",
    "file_path = \"/home/mshahidul/project1/all_tran_data/dataset/Sampled_100_MedlinePlus_eng_spanish_pair.json\"\n",
    "with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "    original_file = json.load(json_file)\n",
    "\n",
    "# Load keywords dataset\n",
    "keywords_path = \"/home/mshahidul/project1/all_tran_data/keywords.json\"\n",
    "with open(keywords_path, 'r', encoding='utf-8') as keywords_file:\n",
    "    keywords = json.load(keywords_file)\n",
    "\n",
    "# Extract sentences\n",
    "original_sentences = [x['english'] for x in original_file]\n",
    "keyword_sentences = [x['sentence'] for x in keywords]\n",
    "\n",
    "# Compute embeddings\n",
    "original_embeddings = model.encode(original_sentences, convert_to_tensor=True)\n",
    "keyword_embeddings = model.encode(keyword_sentences, convert_to_tensor=True)\n",
    "\n",
    "# Perform similarity check and replace if similarity > 90%\n",
    "threshold = 0.90\n",
    "for i, kw_emb in enumerate(keyword_embeddings):\n",
    "    similarities = util.cos_sim(kw_emb, original_embeddings)\n",
    "    max_sim_idx = similarities.argmax().item()\n",
    "    max_sim_value = similarities[0, max_sim_idx].item()\n",
    "\n",
    "    if max_sim_value > threshold:\n",
    "        keywords[i]['sentence'] = original_sentences[max_sim_idx]  # Replace sentence\n",
    "\n",
    "# Save updated keywords file\n",
    "updated_keywords_path = \"/home/mshahidul/project1/all_tran_data/updated_keywords.json\"\n",
    "with open(updated_keywords_path, 'w', encoding='utf-8') as updated_file:\n",
    "    json.dump(keywords, updated_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Updated keyword sentences saved to {updated_keywords_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import OpenAI  # Assuming OpenAI API is set up\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword=\"congenital heart surgery\"\n",
    "connection = mysql.connector.connect(**db_config)\n",
    "cursor = connection.cursor(dictionary=True)\n",
    "cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{keyword}%\",))\n",
    "result = cursor.fetchone()\n",
    "if result is None:\n",
    "    print(f\"No CUI found for keyword: {keyword}\")\n",
    "\n",
    "cui = result[\"CUI\"]\n",
    "\n",
    "print(f\"Keyword: {keyword}, CUI: {cui}\")\n",
    "cursor.execute(\"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                       (cui, 'FRE', 'POR', 'GER'))\n",
    "rows = cursor.fetchall()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword=\"heart surgery\"\n",
    "connection = mysql.connector.connect(**db_config)\n",
    "cursor = connection.cursor(dictionary=True)\n",
    "cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s\", (f\"%{keyword}%\",))\n",
    "result = cursor.fetchone()\n",
    "cui = result[\"CUI\"]\n",
    "print(f\"Keyword: {keyword}, CUI: {cui}\")\n",
    "cursor.execute(\"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                       (cui, 'FRE', 'POR', 'GER'))\n",
    "rows = cursor.fetchall()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"SELECT DEF FROM MRDEF WHERE CUI = %s\", (cui,))\n",
    "rows = cursor.fetchall()\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cui_for_keyword(cursor, keyword):\n",
    "    cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{keyword}%\",))\n",
    "    return cursor.fetchone()\n",
    "\n",
    "def get_translations(cursor, cui):\n",
    "    cursor.execute(\"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                   (cui, 'FRE', 'POR', 'GER'))\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def find_and_merge_translations(cursor, keyword):\n",
    "    words = keyword.split()\n",
    "    translations = {}\n",
    "    \n",
    "    for word in words:\n",
    "        result = get_cui_for_keyword(cursor, word)\n",
    "        if result:\n",
    "            cui = result[\"CUI\"]\n",
    "            translated_rows = get_translations(cursor, cui)\n",
    "            \n",
    "            for row in translated_rows:\n",
    "                lang = row[\"LAT\"]\n",
    "                translations.setdefault(lang, []).append(row[\"STR\"])\n",
    "    \n",
    "    merged_translations = {lang: \" \".join(trans) for lang, trans in translations.items()}\n",
    "    return merged_translations\n",
    "def get_translation(keyword):\n",
    "    import mysql.connector\n",
    "    import json\n",
    "\n",
    "    # Database connection details\n",
    "    db_config = {\n",
    "        'host': '172.16.34.1',\n",
    "        'port': 3307,\n",
    "        'user': 'umls',\n",
    "        'password': 'umls',\n",
    "        'database': 'umls2024'\n",
    "    }\n",
    "    connection = mysql.connector.connect(**db_config)\n",
    "    cursor = connection.cursor(dictionary=True)\n",
    "    result = get_cui_for_keyword(cursor, keyword)\n",
    "        \n",
    "    if result is None:\n",
    "        translations = find_and_merge_translations(cursor, keyword)\n",
    "        output = {\"Keyword\": keyword, \"synonymous\": translations}\n",
    "    else:\n",
    "        cui = result[\"CUI\"]\n",
    "        translated_rows = get_translations(cursor, cui)\n",
    "        translations = {row[\"LAT\"]: row[\"STR\"] for row in translated_rows}\n",
    "        output = {\"Keyword\": keyword, \"synonymous\": translations}\n",
    "    return output\n",
    "        \n",
    "get_translation(\"erwe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the updated keywords file\n",
    "updated_keywords_path = '/home/mshahidul/project1/all_tran_data/prompt info/updated_keywords.json'\n",
    "with open(updated_keywords_path, 'r', encoding='utf-8') as file:\n",
    "    updated_keywords = json.load(file)\n",
    "\n",
    "\n",
    "# Process each sentence and its keywords\n",
    "translations_list = []\n",
    "import tqdm\n",
    "for item in tqdm.tqdm(updated_keywords):\n",
    "    sentence = item['sentence']\n",
    "    keywords = item['keywords']\n",
    "    dict_keywords={}\n",
    "    for keyword in keywords:\n",
    "        translation = get_translation(keyword)\n",
    "        dict_keywords[translation['Keyword']]=translation['synonymous']\n",
    "        # translations_list.append(translation)\n",
    "        # print(translations_list)\n",
    "    translations_list.append({\n",
    "        \"sentence\": sentence,\n",
    "        \"synonymous\": dict_keywords\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a new JSON file\n",
    "output_path = '/home/mshahidul/project1/all_tran_data/prompt info/keyword_synonymous.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(translations_list, output_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Translations saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hop_relationships(keyword):\n",
    "    rel_meanings = {\n",
    "        \"RB\": \"Broader concept\",\n",
    "        \"RN\": \"Narrower concept\",\n",
    "        \"RO\": \"Other related concept\",\n",
    "        \"SY\": \"Synonym\",\n",
    "        \"PAR\": \"Parent concept\",\n",
    "        \"CHD\": \"Child concept\",\n",
    "        \"AQ\": \"Allowed qualifier\",\n",
    "        \"QB\": \"Qualifier of broader concept\",\n",
    "        \"QB\": \"Qualifier of narrower concept\",\n",
    "        \"SIB\": \"Sibling concept\",\n",
    "        \"DEL\": \"Deleted concept\",\n",
    "        \"RQ\": \"Related and possibly synonymous concept\"\n",
    "    }\n",
    "    \"\"\"Retrieve one-hop relationships for a given keyword using UMLS MRREL table.\"\"\"\n",
    "    import mysql.connector\n",
    "    import json\n",
    "\n",
    "    # Database connection details\n",
    "    db_config = {\n",
    "        'host': '172.16.34.1',\n",
    "        'port': 3307,\n",
    "        'user': 'umls',\n",
    "        'password': 'umls',\n",
    "        'database': 'umls2024'\n",
    "    }\n",
    "    connection = mysql.connector.connect(**db_config)\n",
    "    cursor = connection.cursor(dictionary=True)\n",
    "    cui = get_cui_for_keyword(cursor, keyword)\n",
    "    if not cui:\n",
    "        return None\n",
    "    else:\n",
    "        cui=cui['CUI']\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        \n",
    "        query = \"\"\"\n",
    "        SELECT a.CUI1, a.CUI2, a.REL, a.RELA, b.STR AS CUI2_Name\n",
    "        FROM MRREL a \n",
    "        JOIN MRCONSO b ON a.CUI2 = b.CUI\n",
    "        WHERE a.CUI1 = %s \n",
    "          AND a.STYPE1 = 'CUI'\n",
    "          AND b.TS = 'P'\n",
    "          AND b.STT = 'PF'\n",
    "          AND b.ISPREF = 'Y'\n",
    "          AND b.LAT = 'ENG'\n",
    "        \"\"\"\n",
    "        cursor.execute(query, (cui,))\n",
    "        relations = cursor.fetchall()\n",
    "        unique_relations = []\n",
    "        seen = set()\n",
    "\n",
    "        for row in relations:\n",
    "            relation_tuple = (row[\"REL\"],row[\"CUI2_Name\"])\n",
    "            if relation_tuple not in seen:\n",
    "                seen.add(relation_tuple)\n",
    "                unique_relations.append({\"relation\": rel_meanings[row[\"REL\"]],\"related_concept\": row[\"CUI2_Name\"]})\n",
    "        return { \n",
    "            \"keyword\": keyword,\n",
    "            \"relationships\": unique_relations  }\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Database error: {err}\")\n",
    "        return { \n",
    "            \"keyword\": keyword,\n",
    "            \"relationships\": []  }\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            connection.close()\n",
    "\n",
    "get_one_hop_relationships(\"heart surgery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the file path\n",
    "file_path = '/home/mshahidul/project1/all_tran_data/prompt info/updated_keywords.json'\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    updated_keywords = json.load(file)\n",
    "full_res=[]\n",
    "import tqdm\n",
    "for x in tqdm.tqdm(updated_keywords):\n",
    "    res={}\n",
    "    for x2 in (x['keywords']):\n",
    "        t=get_one_hop_relationships(x2)\n",
    "        if t is None:\n",
    "            res[x2]=[]\n",
    "        else:\n",
    "            res[x2]=t['relationships']\n",
    "    full_res.append({\n",
    "        \"sentence\": x['sentence'],\n",
    "        \"relationships\": res\n",
    "    })\n",
    "# Save the results to a new JSON file\n",
    "output_path = '/home/mshahidul/project1/all_tran_data/prompt info/keyword_relationships.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(full_res, output_file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI \n",
    "client = OpenAI(api_key=json.load(open('/home/mshahidul/project1/api.json', 'r'))['openai_api'])\n",
    "import json\n",
    "def translate(text):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                # {\"role\": \"system\", \"content\": f\"\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate each keyword in (French, Portuguese, German, and Spanish). return this in json format(keyword, translated_keyword(French, Portuguese, German, and Spanish)) and remove extra things and keep only json data: {text}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    return (response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=(translate(\"heart surgery\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI \n",
    "\n",
    "# Load API key\n",
    "with open('/home/mshahidul/project1/api.json', 'r') as f:\n",
    "    api_key = json.load(f)['openai_api']\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def translate(text, max_retries=3):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Translate the given keyword into French, Portuguese, German, and Spanish. Provide the output strictly in JSON format without any extra text: {{'keyword': '{text}', 'translated_keywords': {{'French': '...', 'Portuguese': '...', 'German': '...', 'Spanish': '...'}}}}\"}\n",
    "            ],\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "    try:    \n",
    "        t=json.loads((response.choices[0].message.content).replace(\"```\",\"\").replace(\"json\",\"\"))\n",
    "        return t\n",
    "    except:\n",
    "        print(\"ok\")\n",
    "        from utils import save_to_json\n",
    "        save_to_json(\"/home/mshahidul/project1/all_tran_data/prompt_info//errors_keywords.json\",response.choices[0].message.content)\n",
    "          \n",
    "\n",
    "# Example usage\n",
    "translated_data = translate(\"heart surgery\")\n",
    "print(translated_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI \n",
    "\n",
    "# Load API key\n",
    "with open('/home/mshahidul/project1/api.json', 'r') as f:\n",
    "    api_key = json.load(f)['openai_api']\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "def KG(text, max_retries=3):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"create knowledge graph for the given keyword. Provide the output strictly in JSON format without any extra text: {text}\"}\n",
    "            ],\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "    try:    \n",
    "        t=json.loads((response.choices[0].message.content).replace(\"```\",\"\").replace(\"json\",\"\"))\n",
    "        return t\n",
    "    except:\n",
    "        print(\"ok\")\n",
    "        from utils import save_to_json\n",
    "        save_to_json(\"/home/mshahidul/project1/all_tran_data/prompt_info//KG_errors.json\",response.choices[0].message.content)\n",
    "          \n",
    "\n",
    "# Example usage\n",
    "# translated_data = translate(\"heart surgery\")\n",
    "# print(translated_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 312/434 [38:02<14:09,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 336/434 [41:09<14:26,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 434/434 [53:56<00:00,  7.46s/it]\n"
     ]
    }
   ],
   "source": [
    "syn_gpt4o=[]\n",
    "import tqdm\n",
    "for x in tqdm.tqdm(keywords):\n",
    "    t=KG(x)\n",
    "    syn_gpt4o.append({\n",
    "        \"keyword\": x,\n",
    "        \"kg\": t\n",
    "    })\n",
    "import json\n",
    "output_path = '/home/mshahidul/project1/all_tran_data/prompt_info/kg_gpt4o_mini.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(syn_gpt4o, output_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_bleu_chrf\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "from utils2 import data_import\n",
    "from unsloth import FastLanguageModel\n",
    "from utils2 import keyword_relationships_umls, keyword_synonymous_umls\n",
    "from utils2 import kg_gpt4o_mini_umls, synonyms_gpt4o_diff_lang_umls,extract_keywords,gpt4o_mini_tran_func\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "original_file,results_data,sentence_to_prompt=data_import()\n",
    "def find_cod_prompt(english_sentence):\n",
    "    return sentence_to_prompt.get(english_sentence, \"Prompt not found\")\n",
    "###########################\n",
    "inference_proc=\"prompt\"\n",
    "def return_prompt_data(text):\n",
    "    return keyword_synonymous_umls(text)\n",
    "max_seq_length,dtype,load_in_4bit,model_name = 2048,None ,True, \"unsloth/phi-4\"\n",
    "txt=\"without_finetune_keyword_synonymous_umls\"\n",
    "###########################\n",
    "\n",
    "\n",
    "print(f'''\n",
    "###############################               \n",
    "#  {model_name}\n",
    "#  {txt}                         \n",
    "###############################\n",
    "''')\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = False)\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"qwen-2.5\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) \n",
    "import re\n",
    "def extract_translation(text):\n",
    "    match = re.search(r\"Spanish: (.*?)<\\|im_end\\|>\", text)\n",
    "    if match:\n",
    "        extracted_text = match.group(1)\n",
    "        return extracted_text\n",
    "    else:\n",
    "        text=text.split(\"\\n\")\n",
    "        text=text[len(text)-1]\n",
    "        text=text.split(\"<|im_end|>\")[0]\n",
    "        return text\n",
    "    \n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"qwen-2.5\",\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) \n",
    "\n",
    "def inference(text,context):\n",
    "    # print (f\"{keyword_relationships_umls(text)}\\nUsing above context translate the input into English to Spanish: \\n\\nEnglish: {text}\\n\\nSpanish: \")\n",
    "    # exit()\n",
    "    prompt=f'''\n",
    "        {context}\n",
    "        Using above context translate the following text into Spanish:\n",
    "        {text}\n",
    "        Spanish: \"\n",
    "        '''\n",
    "    # prompt=f''' \n",
    "    #     Translate the following text into Spanish, given this context:\n",
    "    #     {keyword_relationships_umls(text)}\n",
    "    #     Text: {text}\n",
    "    #     Spanish: \"\n",
    "    #     '''\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt,}]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,temperature = 1.0, min_p = 1.0)\n",
    "    return extract_translation(tokenizer.batch_decode(outputs)[0])\n",
    "\n",
    "def back_inference(text,context):\n",
    "    prompt=f'''\n",
    "        {context}\n",
    "        Using above context translate the following text into English:\n",
    "        {text}\n",
    "        English: \"\n",
    "        '''\n",
    "    # prompt=f'''\n",
    "    #     Translate the following text into English, given this context:\n",
    "    #     {keyword_relationships_umls(text)}\n",
    "    #     Text: {text}\n",
    "    #     English: \"\n",
    "    #     '''\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 100, use_cache = True,temperature = 1.0, min_p = 1.0)\n",
    "    return extract_translation(tokenizer.batch_decode(outputs)[0])\n",
    "\n",
    "##################################################\n",
    "\n",
    "\n",
    "def inference_direct(text):\n",
    "    prompt=f'''\n",
    "            Translate the following text into Spanish: \n",
    "            {text}\n",
    "            Spanish:\n",
    "            '''\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,temperature = 1.0, min_p = 1.0)\n",
    "    return extract_translation(tokenizer.batch_decode(outputs)[0])\n",
    "\n",
    "def back_inference_direct(text):\n",
    "    prompt=f'''\n",
    "            Translate the following text into English: \n",
    "            {text}\n",
    "            English:\n",
    "            '''\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,temperature = 1.0, min_p = 1.0)\n",
    "    return extract_translation(tokenizer.batch_decode(outputs)[0])\n",
    "\n",
    "##################################################\n",
    "\n",
    "total_score=[]\n",
    "\n",
    "for line in tqdm.tqdm(original_file):\n",
    "    try:\n",
    "        # context=return_prompt_data(line['english'])\n",
    "        context=keyword_synonymous_umls(line[\"english\"])\n",
    "        # if inference_proc==\"prompt\":\n",
    "        hypothesis_text = inference(line['english'],context)\n",
    "            # reference_text = line['spanish']\n",
    "        back_translate = back_inference(hypothesis_text,context)\n",
    "        # elif inference_proc==\"direct\":\n",
    "        #     hypothesis_text = inference_direct(line['english'])\n",
    "        #     # reference_text = line['spanish']\n",
    "        #     back_translate = back_inference_direct(hypothesis_text)\n",
    "        # else:\n",
    "        #     exit()\n",
    "\n",
    "        # print(f\"yes: {back_translate}\")\n",
    "        score=compute_bleu_chrf(line['english'].lower(), back_translate.lower())  \n",
    "        total_score.append({\n",
    "            \"original_english\": line['english'],\n",
    "            \"original_spanish\": line['spanish'],\n",
    "            \"translated_spanish\": hypothesis_text,\n",
    "            \"back_translated_english\": back_translate,\n",
    "            \"bleu_score\": score\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(\"Error: \",e)\n",
    "        continue\n",
    "\n",
    "tt=model_name.split(\"/\")[1]\n",
    "avg_bleu_score = sum([x['bleu_score']['bleu_score'] for x in total_score]) / len(total_score)\n",
    "# txt=\"without_finetune_and_prompt\"\n",
    "\n",
    "print(f\"{tt}_{txt}: {avg_bleu_score:.4f}\")\n",
    "\n",
    "with open(f\"/home/mshahidul/project1/all_tran_data/sample/{tt}_{txt}.json\", 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(total_score, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# with open(f\"/home/mshahidul/project1/all_tran_data/sample/{tt}_without_finetune_and_prompt.json\", 'w', encoding='utf-8') as json_file:\n",
    "#     json.dump(total_score, json_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = '/home/mshahidul/project1/all_tran_data/prompt_info/updated_keywords.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    updated_keywords = json.load(file)\n",
    "full_res=[]\n",
    "import tqdm\n",
    "for x in tqdm.tqdm(updated_keywords):\n",
    "    list={}\n",
    "    for x2 in x['keywords']:\n",
    "        t=translate(x2)\n",
    "        try:\n",
    "            list[x2]=t['translated_keywords']\n",
    "        except:\n",
    "            print(t)\n",
    "    full_res.append({\n",
    "        \"sentence\": x['sentence'],\n",
    "        \"keywords\":x['keywords'],\n",
    "        \"translated_keywords\": list\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a new JSON file\n",
    "output_path = '/home/mshahidul/project1/all_tran_data/prompt_info/translated_keywords_gpt4o_mini.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(full_res, output_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = '/home/mshahidul/project1/all_tran_data/prompt_info/updated_keywords.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    updated_keywords = json.load(file)\n",
    "keywords = set()\n",
    "import tqdm\n",
    "for x in tqdm.tqdm(updated_keywords):\n",
    "    for x2 in (x['keywords']):\n",
    "        keywords.add(x2)\n",
    "keywords=list(keywords)\n",
    "len(keywords)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "client = OpenAI(api_key=json.load(open('/home/mshahidul/project1/api.json', 'r'))['openai_api'])\n",
    "def synonyms_gpt4o(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Provide synonyms for the given keyword in different languages (English, French, Portuguese, German, Spanish). Output strictly in JSON format without any extra text: {text}\"}\n",
    "        ],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    # print(response.choices[0].message.content)\n",
    "    try:\n",
    "        t = json.loads(response.choices[0].message.content.replace(\"```\", \"\").replace(\"json\", \"\"))\n",
    "        return t\n",
    "    except:\n",
    "        print(\"Error in parsing JSON response\")\n",
    "        from utils import save_to_json\n",
    "        save_to_json(\"/home/mshahidul/project1/all_tran_data/prompt_info/synonyms_gpt4o.json\", response.choices[0].message.content)\n",
    "synonyms_gpt4o(\"heart surgery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_gpt4o=[]\n",
    "import tqdm\n",
    "for x in tqdm.tqdm(keywords):\n",
    "    t=synonyms_gpt4o(x)\n",
    "    syn_gpt4o.append({\n",
    "        \"keyword\": x,\n",
    "        \"synonyms\": t\n",
    "    })\n",
    "import json\n",
    "output_path = '/home/mshahidul/project1/all_tran_data/prompt_info/synonyms_gpt4o_diff_lang.json'\n",
    "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(syn_gpt4o, output_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'If you have a weakened immune system due to AIDS, cancer, transplantation, or corticosteroid use, call your doctor if you develop a cough, fever, or shortness of breath.', 'synonymous': {'weakened immune system': {'POR': 'Anemia Hemolítica Autoimune Anemias hemolíticas auto-imunes Anemias hemolíticas auto-imunes Anemia hemolítica auto-imune Anemia hemolítica auto-imune Anemia hemolítica auto-imune Anemia hemolítica auto-imune Anemia hemolítica auto-imune NE Anemia hemolítica auto-imune NE Anemia hemolítica auto-imune Anemia Hemolítica Autoimunitária Anemia hemolítica autoimune (SOE) Anemia hemolítica autoimune Anemia hemolítica autoimune Anemia hemolítica autoimune Anemia hemolítica autoimune Anemia hemolítica autoimune Anemias hemolíticas autoimunes Anemia hemolítica autoimune SOE Anemias hemolíticas autoimunes Esclerodermia sistémica Esclerose sistémica Esclerose sistémica progressiva Esclerodermia sistémica Esclerose Sistêmica Esclerodermia sistêmica Esclerose sistêmica Esclerose sistêmica progressiva Esclerodermia sistêmica Escleroderma Sistêmico', 'FRE': 'Anémies hémolytiques autoimmunes Anémies hémolytiques autoimmunes Anémie hémolytique auto-immune Anémie hémolytique auto-immune SAI Anémie hémolytique auto immune SAI Anémie hémolytique auto-immune Anémie hémolytique auto-immune Anémie hémolytique auto-immune Anémie hémolytique auto-immune Anémie hémolytique autoimmune AHLAI AHAI Anémie hémolytique auto-immune Sclérodermie systémique Sclérose systémique progressive Sclérodermie systémique Sclérodermie systémique Sclérose systémique Sclérose générale Sclérodermie généralisée Sclérodermie systémique', 'GER': 'autoimmunhaemolytische Anaemien autoimmunhaemolytische Anaemien Anaemie autoimmunhaemolytisch (NNB) Anaemie autoimmunhaemolytisch NNB Autoimmunhaemolytische Anaemie Autoimmunhaemolytische Anaemie Haemolytische Anaemie autoimmun Haemolytische Anaemie autoimmun Autoimmunhaemolytische Anaemie Autoimmunhämolytische Anämie Hämolytische Anämie, autoimmune Anämie, autoimmunhämolytische Progressive systemische Sklerose Systemische Sklerose, nicht naeher bezeichnet Systemische Sklerose Systemische Sklerodermie Systemische Sklerodermie Systemische Sklerodermie Systemische Sklerose Progressive systemische Sklerose Sklerodermie, systemische'}, 'AIDS': {'FRE': \"Syndrome d'immunodéficience acquise\", 'GER': 'Erworbenes Immundefektsyndrom', 'POR': 'Síndromes de imunodeficiência adquirida'}, 'cancer': {'FRE': 'ACS (American Cancer Society)', 'GER': 'American Cancer Society', 'POR': 'American Cancer Society'}, 'transplantation': {'FRE': 'Homoplastie', 'POR': 'Transplante Homólogo', 'GER': 'Transplantation, homologe'}, 'corticosteroid': {'POR': '17-Hidroxicorticoesteroides', 'FRE': '17-Hydroxycorticostéroïdes', 'GER': '17-Hydroxycorticosteroide'}, 'doctor': {'FRE': 'Assistants de médecin', 'GER': 'Ärztliche Assistenten', 'POR': 'Assistentes Médicos'}, 'cough': {'FRE': 'Toux chronique', 'POR': 'Tosse Persistente', 'GER': 'Chronischer Husten'}, 'fever': {'GER': 'Paratyphusfieber A', 'FRE': 'Fièvre paratyphoïde A', 'POR': 'Febre paratifoide A'}, 'shortness of breath': {'FRE': 'Dyspnée', 'POR': 'Dispneia', 'GER': 'Dyspnoe'}}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path_relationships = '/home/mshahidul/project1/all_tran_data/prompt_info/keyword_synonymous_umls.json'\n",
    "\n",
    "with open(file_path_relationships, 'r', encoding='utf-8') as file:\n",
    "    keyword_relationships = json.load(file)\n",
    "\n",
    "print(keyword_relationships[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create --name unsloth_gemma python=3.11 -y\n",
    "conda activate unsloth_env\n",
    "pip3 install torch\n",
    "pip install \"unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "pip install evaluate sacrebleu\n",
    "pip install openai\n",
    "# pip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create --name gamma python=3.11 -y\n",
    "conda activate unsloth_om\n",
    "pip3 install torch\n",
    "pip install \"unsloth[cu124-ampere-torch250] @ git+https://github.com/unslothai/unsloth.git\" evaluate sacrebleu openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create --name unsloth_env python=3.11 -y\n",
    "conda activate unsloth_env\n",
    "pip3 install torch\n",
    "pip install \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "pip install evaluate sacrebleu openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir unsloth\n",
    "cd unsloth\n",
    "uv venv unsloth --python 3.11\n",
    "source unsloth/bin/activate\n",
    "uv pip install mysql-connector-python torch \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\" evaluate sacrebleu openai\n",
    "deactivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir unsloth\n",
    "cd unsloth\n",
    "uv venv unsloth --python 3.11\n",
    "source unsloth/bin/activate\n",
    "uv pip install mysql-connector-python torch \"unsloth[cu124-torch250] @ git+https://github.com/unslothai/unsloth.git\" evaluate sacrebleu openai\n",
    "deactivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for omega\n",
    "conda create --name gamma python=3.11 -y && conda activate gamma\n",
    "uv pip install mysql-connector-python evaluate sacrebleu openai\n",
    "uv pip install torch\n",
    "uv pip install \"unsloth[cu124-torch260] @ git+https://github.com/unslothai/unsloth.git\" \n",
    "unset LD_LIBRARY_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uv pip install torch --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
