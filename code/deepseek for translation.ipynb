{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"  # Replace with your model name\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"  # Replace with your desired cache path\n",
    "\n",
    "# Load the tokenizer and model with a custom cache directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI \n",
    "client = OpenAI(api_key=\"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\")\n",
    "def direct_translate(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from English into Spanish: {sentence}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    return (response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "def compute_bleu_chrf(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Computes the BLEU and chrF++ scores for a given reference and hypothesis.\n",
    "    \n",
    "    :param reference: List of reference translations (list of strings)\n",
    "    :param hypothesis: The hypothesis translation (a single string)\n",
    "    :return: A dictionary containing BLEU and chrF++ scores\n",
    "    \"\"\"\n",
    "    # Ensure reference is wrapped in a list as sacrebleu expects a list of references\n",
    "    # bleu_score = sacrebleu.corpus_bleu(hypothesis, [reference]).score\n",
    "    # bleu_score = sacrebleu.corpus_bleu(hypothesis, [reference], tokenize=\"13a\", lowercase=True).score\n",
    "    bleu_score=metric.compute(predictions=[hypothesis], references=[reference])\n",
    "    chrf_score = sacrebleu.corpus_chrf(hypothesis, [reference]).score\n",
    "\n",
    "    return {\"bleu_score\": bleu_score['score'],\"chrF++\": chrf_score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Function to translate text\n",
    "def translate(text, max_length=512):\n",
    "    import re\n",
    "    \n",
    "    retry_count = 0  # Counter for consecutive retries\n",
    "    max_retries = 4  # Maximum retries before skipping\n",
    "    \n",
    "    while retry_count < max_retries:  # Retry up to 4 times\n",
    "        # Prepare the input text with language tags (if required by the model)\n",
    "        input_text = f'''\n",
    "        <｜User｜>Translate this sentence English to Spanish: {text}<｜Assistant｜>\n",
    "        '''\n",
    "        # input_text = f'''\n",
    "        # ### Instruction:\n",
    "        #     Translate this sentence English to Spanish: {text}\n",
    "        # ### Response:\n",
    "        # ''' \n",
    "        # input_text=f'''\n",
    "        #         Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "        #         ### Instruction:\n",
    "        #         {instruction}\n",
    "\n",
    "        #         ### Input:\n",
    "        #         {input}\n",
    "\n",
    "        #         ### Response:\n",
    "        # '''\n",
    "        \n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        \n",
    "        # Generate translation\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs, \n",
    "                max_length=200, \n",
    "                num_beams=5, \n",
    "                early_stopping=True,\n",
    "                temperature=0.6, \n",
    "                no_repeat_ngram_size=2, \n",
    "                length_penalty=2.0\n",
    "            )\n",
    "        \n",
    "        # Decode the output tokens to text\n",
    "        translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(translated_text)\n",
    "        # Extract the text after \"### Response:\"\n",
    "        try:\n",
    "            txt = translated_text.split(\"### Response:\")[1]\n",
    "            match = re.search(r'\"(.*)\"', txt)\n",
    "            if match:  # If a match is found, return the translated text\n",
    "                return match.group(1)\n",
    "        except (IndexError, AttributeError):  # Catch splitting or regex errors\n",
    "            retry_count += 1  # Increment the retry counter\n",
    "\n",
    "    # Return None if max retries are reached\n",
    "    return None\n",
    "path = \"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs.txt\"\n",
    "f = open(path, \"r\").read().split(\"\\n\")\n",
    "print(f[0].split(\"\\t\")[0])\n",
    "tt=translate(f[0].split(\"\\t\")[0])\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If more pressure is put on a bone than it can stand, it will split or break.\n",
      "<｜User｜>Translate this sentence from English to Spanish: If more pressure is put on a bone than it can stand, it will split or break..<｜Assistant｜><think>\n",
      "Okay, so the user wants me to translate an English sentence into Spanish. Let me first read the sentence carefully. It's about bone structure under pressure. Hmm, I need to make sure the translation is accurate and natural in Spanish.\n",
      "\n",
      "Alright, the original sentence is: \"If morepressureisputonabone thanitcanstand,itwill splitorbreak.\" I notice that the words are run together without spaces, which might be a typo. I should correct that for clarity.\n",
      "\n",
      "Breaking it down, \"more pressure\" should be \"más presión.\" Then \"is put\" becomes \"se aplica.\" \"On abone\" is \"en un hueso.\" The phrase \"than itcan stand\" translates to \"de lo que puede soportar.\" \n",
      "\n",
      "Now, for the result part: if the bone can't ==> Si se aplica más presión sobre un hueso de la que puede soportar, éste se partirá o se romperá.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Function to translate text\n",
    "def translate(text, max_length=512):\n",
    "    import re\n",
    "    \n",
    "    # Prepare the input text with language tags (if required by the model)\n",
    "    input_text = f\"<｜User｜>Translate this sentence from English to Spanish: {text}.<｜Assistant｜>\"\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_length, truncation=True)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Generate translation\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_length=200, \n",
    "            num_beams=5, \n",
    "            early_stopping=True,\n",
    "            temperature=0.6, \n",
    "            no_repeat_ngram_size=2, \n",
    "            length_penalty=2.0\n",
    "        )\n",
    "    \n",
    "    # Decode the output tokens to text\n",
    "    translated_text= tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "path = \"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs.txt\"\n",
    "f = open(path, \"r\").read().split(\"\\n\")\n",
    "print(f[0].split(\"\\t\")[0])\n",
    "tt=translate(f[0].split(\"\\t\")[0])\n",
    "print(tt, \"==>\",f[0].split(\"\\t\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If more pressure is put on a bone than it can stand, it will split or break.\n"
     ]
    }
   ],
   "source": [
    "# print(f[:10])\n",
    "print(f[0].split(\"\\t\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# File path and processing\n",
    "path = \"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs.txt\"\n",
    "f = open(path, \"r\").read().split(\"\\n\")\n",
    "output_data = []\n",
    "\n",
    "for x in tqdm.tqdm(f[70:100]):\n",
    "    xx = x.split(\"\\t\")\n",
    "    sentence_eng = xx[0]\n",
    "    sentence_spa = xx[1]\n",
    "    print(sentence_eng)\n",
    "    \n",
    "    deepseek_tran = translate(sentence_eng)\n",
    "    if deepseek_tran is None:  # Skip processing if translation failed\n",
    "        print(f\"Skipping sentence due to repeated failures: {sentence_eng}\")\n",
    "        continue\n",
    "    \n",
    "    gpt4_translate = direct_translate(sentence_eng)\n",
    "    deepseek_score = compute_bleu_chrf(sentence_spa, deepseek_tran)\n",
    "    gpt4_score = compute_bleu_chrf(sentence_spa, gpt4_translate)\n",
    "    \n",
    "    output_data.append(\n",
    "        {\n",
    "            \"sentence_eng\": sentence_eng,\n",
    "            \"sentence_spa\": sentence_spa,\n",
    "            \"deepseek_translate\": deepseek_tran,\n",
    "            \"gpt4_translate\": gpt4_translate,\n",
    "            \"deepseek_score\": deepseek_score,\n",
    "            \"gpt4_score\": gpt4_score\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to an Excel file\n",
    "df = pd.DataFrame(output_data)\n",
    "df.to_excel(\"deepseek_testing.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
