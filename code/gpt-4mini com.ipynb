{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.47it/s]\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bariatric surgery in Spanish is 'Cirugía bariátrica'. Bariatric surgery in French is 'Chirurgie bariatrique'. Bariatric surgery in German is 'Bariatrische Chirurgie'. Bariatric surgery in Portuguese is 'Cirurgia bariátrica'. . . . .\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import OpenAI  # Assuming OpenAI API is set up\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n",
    "\n",
    "# Define the NLLB-200 model\n",
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# List of target languages\n",
    "languages = {\n",
    "    \"French\": \"fra_Latn\",\n",
    "    \"German\": \"deu_Latn\",\n",
    "    \"Portuguese\": \"por_Latn\",\n",
    "    \"Spanish\": 'spa_Latn'\n",
    "}\n",
    "\n",
    "# Load translation model\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_directory, torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# OpenAI API Client\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def extract_keywords(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract medical and non-medical keywords from the given sentence. return it as json format without extra things.\"},\n",
    "                  {\"role\": \"user\", \"content\": f\"{sentence}\"}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    # print(response.choices[0].message.content)\n",
    "    keywords = json.loads(response.choices[0].message.content)\n",
    "    return keywords  # Expected format: {\"medical\": [\"keyword1\", \"keyword2\"], \"non_medical\": [\"keyword3\", \"keyword4\"]}\n",
    "\n",
    "def search_umls(keyword):\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{keyword}%\",))\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            return None\n",
    "        cui = result[\"CUI\"]\n",
    "\n",
    "        cursor.execute(\"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                       (cui, 'FRE', 'POR', 'GER'))\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        translations = {row['LAT']: row['STR'] for row in rows}\n",
    "        return translations\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Database error: {err}\")\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            connection.close()\n",
    "\n",
    "def translate_non_medical(keyword):\n",
    "    translations = {}\n",
    "    for language, lang_code in languages.items():\n",
    "        output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "        translations[language] = output[0]['translation_text']\n",
    "    return translations\n",
    "\n",
    "# def convert_to_chained_format(dictionary, src_lang, target_lang):\n",
    "#     data={}\n",
    "#     for x, y in dictionary.items():\n",
    "#         for word,_ in dictionary[x].items():\n",
    "#             data[word]=[]\n",
    "#             data[word].append(word)\n",
    "#             for _, tran2 in dictionary[x][word].items():\n",
    "#                 data[word].append(tran2)\n",
    "#     chain=[]\n",
    "#     for word, translations in data.items():\n",
    "#         ch=\"\"\n",
    "#         for i,x in enumerate(translations):\n",
    "#             if i!=len(translations)-1:\n",
    "#                 ch+=f\"'{x}' means \"\n",
    "#             else:\n",
    "#                 ch+=f\"'{x}'\"\n",
    "#         chain.append(ch)\n",
    "#     chain=\". \".join(chain)\n",
    "#     chain+=\". \"\n",
    "#     # chain=f\"{chain}\\nTranslate the following text from {src_lang} into {target_lang}:\"\n",
    "#     return chain\n",
    "\n",
    "def convert_to_chained_format(dictionary, src_lang, target_lang):\n",
    "    chain = []\n",
    "    \n",
    "    for category, words in dictionary.items():\n",
    "        for word, translations in words.items():\n",
    "            formatted_translations = []\n",
    "            \n",
    "            # Ensure the source language is first, target language second, then others\n",
    "            ordered_languages = [\"English\", \"Spanish\", \"French\", \"German\", \"Portuguese\"]\n",
    "            \n",
    "            for lang in ordered_languages:\n",
    "                if lang in translations:\n",
    "                    formatted_translations.append(f\"{word} in {lang} is '{translations[lang]}'\")\n",
    "\n",
    "            chain.append(\". \".join(formatted_translations))\n",
    "    \n",
    "    chained_text = \". \".join(chain) + \".\"\n",
    "    return chained_text\n",
    "\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    keywords = extract_keywords(sentence)\n",
    "    medical_translations = {}\n",
    "    non_medical_translations = {}\n",
    "\n",
    "    # Process medical keywords\n",
    "    for keyword in keywords[\"medical_keywords\"]:\n",
    "        translation = search_umls(keyword)\n",
    "        if not translation:  # If keyword not found in UMLS, translate using NLLB\n",
    "            translation = {lang: translator(keyword, src_lang=\"eng_Latn\", tgt_lang=code, max_length=400)[0]['translation_text'] \n",
    "                           for lang, code in languages.items()}\n",
    "        elif \"SPA\" not in translation:  # If Spanish missing in UMLS, use NLLB for Spanish\n",
    "            translation[\"SPA\"] = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=\"spa_Latn\", max_length=400)[0]['translation_text']\n",
    "        \n",
    "        medical_translations[keyword] = translation\n",
    "\n",
    "    # Process non-medical keywords (always translated via NLLB)\n",
    "    for keyword in keywords[\"non_medical_keywords\"]:\n",
    "        non_medical_translations[keyword] = translate_non_medical(keyword)\n",
    "\n",
    "    result_json_temp = {\n",
    "        \"medical\": medical_translations,\n",
    "        \"non_medical\": non_medical_translations\n",
    "    }\n",
    "\n",
    "    src_language = \"English\"\n",
    "    target_language = \"Spanish\"\n",
    "    chained_output = convert_to_chained_format(result_json_temp, src_language, target_language)\n",
    "\n",
    "    return chained_output, result_json_temp\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Bariatric surgery is done when diet and exercise haven't worked or when you have serious health problems because of your weight.\"\n",
    "COD_prompt,result_json_temp = process_sentence(sentence)\n",
    "print(COD_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_cod_prompt(COD_prompt,sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"{COD_prompt}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from English into Spanish: {sentence}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    ans=(response.choices[0].message.content)\n",
    "    return ans\n",
    "\n",
    "## Direct translate in Spanish\n",
    "def direct_translate(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from English into Spanish: {sentence}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    return (response.choices[0].message.content)\n",
    "\n",
    "\n",
    "## Back translate in English\n",
    "def back_translate(spa_tran):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                # {\"role\": \"system\", \"content\": f\"\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from Spanish into English: {spa_tran}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    return (response.choices[0].message.content)\n",
    "\n",
    "## Evaluation\n",
    "import sacrebleu\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "def compute_bleu_chrf(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Computes the BLEU and chrF++ scores for a given reference and hypothesis.\n",
    "    \n",
    "    :param reference: List of reference translations (list of strings)\n",
    "    :param hypothesis: The hypothesis translation (a single string)\n",
    "    :return: A dictionary containing BLEU and chrF++ scores\n",
    "    \"\"\"\n",
    "    # Ensure reference is wrapped in a list as sacrebleu expects a list of references\n",
    "    # bleu_score = sacrebleu.corpus_bleu(hypothesis, [reference]).score\n",
    "    # bleu_score = sacrebleu.corpus_bleu(hypothesis, [reference], tokenize=\"13a\", lowercase=True).score\n",
    "    bleu_score=metric.compute(predictions=[hypothesis], references=[reference])\n",
    "    chrf_score = sacrebleu.corpus_chrf(hypothesis, [reference]).score\n",
    "\n",
    "    return {\"bleu_score\": bleu_score['score'],\"chrF++\": chrf_score}\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "output_data=[]\n",
    "import tqdm\n",
    "path=\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs.txt\"\n",
    "f=open(path, \"r\").read().split(\"\\n\")\n",
    "for x in tqdm.tqdm(f[:100]):\n",
    "    xx=x.split(\"\\t\")\n",
    "    sentence_eng=xx[0]\n",
    "    sentence_spa=xx[1]\n",
    "    COD_prompt,result_json_temp = process_sentence(sentence_eng)\n",
    "    spa_tran=translate_cod_prompt(COD_prompt,sentence_eng)\n",
    "    spa_tran_direct=direct_translate(sentence_eng)\n",
    "    back_tran=back_translate(spa_tran)\n",
    "    back_tran_direct=back_translate(spa_tran_direct)\n",
    "    reference_text = [sentence_eng]\n",
    "    hypothesis_text = back_tran\n",
    "    scores_cod_prompt = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    hypothesis_text = back_tran_direct\n",
    "    scores_direct = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    output_data.append({\n",
    "        \"Original_English_sentence\": sentence_eng,\n",
    "        \"Original_Spanish_sentence\": sentence_spa,\n",
    "        \"COD_prompt\": COD_prompt,\n",
    "        \"spanish_translation\": spa_tran,\n",
    "        \"spanish_translation_direct\": spa_tran_direct,\n",
    "        \"back_translation\": back_tran,\n",
    "        \"back_translation_direct\": back_tran_direct,\n",
    "        \"scores_cod_prompt(bleu and chrf)\": scores_cod_prompt,\n",
    "        \"scores_direct(bleu and chrf)\": scores_direct\n",
    "    })\n",
    "    # print(scores_cod_prompt,scores_direct)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(output_data)\n",
    "df.to_csv(\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs_gpt4_mini.csv\")\n",
    "\n",
    "df.to_excel(\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs_gpt4_mini.xlsx\")\n",
    "## Performance check dataset\n",
    "path=\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs.txt\"\n",
    "f=open(path, \"r\").read().split(\"\\n\")\n",
    "for x in f[:100]:\n",
    "    xx=x.split(\"\\t\")\n",
    "    sentence_eng=xx[0]\n",
    "    sentence_spa=xx[1]\n",
    "    COD_prompt,result_json_temp = process_sentence(sentence_eng)\n",
    "    spa_tran=translate_cod_prompt(COD_prompt, sentence_eng)\n",
    "    reference_text = [sentence_spa]\n",
    "    hypothesis_text = spa_tran\n",
    "    scores_cod_prompt = compute_bleu_chrf(reference_text, hypothesis_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLLB model only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.35it/s]\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Bariatric surgery' means 'Cirugía bariátrica' means 'Chirurgie bariatrique' means 'Bariatrische Chirurgie' means 'Cirurgia bariátrica'. 'diet' means 'dieta y' means 'régime alimentaire' means 'Ernährung' means 'dieta'. 'exercise' means 'ejercicio' means 'exercice physique' means 'Übung' means 'exercício'. 'serious health problems' means 'problemas de salud graves' means 'problèmes de santé graves' means 'schwere gesundheitliche Probleme' means 'problemas de saúde graves'. 'weight' means 'peso' means 'poids' means 'Gewicht' means 'peso'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import OpenAI  # Assuming OpenAI API is set up\n",
    "\n",
    "# Define the NLLB-200 model\n",
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# List of target languages (ensuring Spanish comes second)\n",
    "languages = {\n",
    "    \"Spanish\": 'spa_Latn',  # Target language\n",
    "    \"French\": \"fra_Latn\",\n",
    "    \"German\": \"deu_Latn\",\n",
    "    \"Portuguese\": \"por_Latn\"\n",
    "}\n",
    "\n",
    "# Load translation model\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_directory, torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# OpenAI API Client\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def extract_keywords(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract medical and non-medical keywords from the given sentence. return it as json format without extra things.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{sentence}\"}\n",
    "        ],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    keywords = json.loads(response.choices[0].message.content)\n",
    "    return keywords  \n",
    "\n",
    "def translate_keywords(keyword):\n",
    "    translations = {\"English\": keyword}  # Ensure English is first\n",
    "    for language, lang_code in languages.items():\n",
    "        output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "        translations[language] = output[0]['translation_text']\n",
    "    return translations\n",
    "\n",
    "def convert_to_chained_format(dictionary):\n",
    "    chain = []\n",
    "    \n",
    "    for category, keywords in dictionary.items():\n",
    "        for word, translations in keywords.items():\n",
    "            ordered_translations = [translations[\"English\"], translations[\"Spanish\"]]  # Keep English first, Spanish second\n",
    "            other_langs = {k: v for k, v in translations.items() if k not in [\"English\", \"Spanish\"]}\n",
    "            ordered_translations.extend(other_langs.values())  # Append remaining languages\n",
    "            \n",
    "            phrase = \"\"\n",
    "            for i, term in enumerate(ordered_translations):\n",
    "                if i != len(ordered_translations) - 1:\n",
    "                    phrase += f\"'{term}' means \"\n",
    "                else:\n",
    "                    phrase += f\"'{term}'\"\n",
    "            chain.append(phrase)\n",
    "    \n",
    "    chained_text = \". \".join(chain) + \".\"\n",
    "    return chained_text\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    keywords = extract_keywords(sentence)\n",
    "    medical_translations = {}\n",
    "    non_medical_translations = {}\n",
    "\n",
    "    # Translate medical keywords\n",
    "    for keyword in keywords[\"medical_keywords\"]:\n",
    "        medical_translations[keyword] = translate_keywords(keyword)\n",
    "\n",
    "    # Translate non-medical keywords\n",
    "    for keyword in keywords[\"non_medical_keywords\"]:\n",
    "        non_medical_translations[keyword] = translate_keywords(keyword)\n",
    "\n",
    "    result_json_temp = {\n",
    "        \"medical\": medical_translations,\n",
    "        \"non_medical\": non_medical_translations\n",
    "    }\n",
    "\n",
    "    chained_output = convert_to_chained_format(result_json_temp)\n",
    "\n",
    "    return chained_output, result_json_temp\n",
    "\n",
    "\n",
    "sentence = \"Bariatric surgery is done when diet and exercise haven't worked or when you have serious health problems because of your weight.\"\n",
    "COD_prompt, result_json_temp = process_sentence(sentence)\n",
    "print(COD_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate all the data using NLLB except cod prompt translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.53it/s]\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "100%|██████████| 100/100 [07:48<00:00,  4.69s/it]\n"
     ]
    }
   ],
   "source": [
    "# List of target languages\n",
    "languages = {\n",
    "    \"Spanish\": 'spa_Latn',  # Target language for translation\n",
    "    \"English\": \"eng_Latn\"   # Source language for back-translation\n",
    "}\n",
    "\n",
    "# Load translation model\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_directory, torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def translate_cod_prompt(COD_prompt,sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"{COD_prompt}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from English into Spanish: {sentence}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    ans=(response.choices[0].message.content)\n",
    "    return ans\n",
    "\n",
    "def direct_translate(sentence):\n",
    "    \"\"\"\n",
    "    Directly translates an English sentence to Spanish using NLLB.\n",
    "    \"\"\"\n",
    "    output = translator(sentence, src_lang=\"eng_Latn\", tgt_lang=\"spa_Latn\", max_length=400)\n",
    "    return output[0]['translation_text']\n",
    "\n",
    "def back_translate(spa_tran):\n",
    "    \"\"\"\n",
    "    Translates a Spanish sentence back into English using NLLB.\n",
    "    \"\"\"\n",
    "    output = translator(spa_tran, src_lang=\"spa_Latn\", tgt_lang=\"eng_Latn\", max_length=400)\n",
    "    return output[0]['translation_text']\n",
    "\n",
    "## Evaluation\n",
    "import sacrebleu\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "def compute_bleu_chrf(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Computes the BLEU and chrF++ scores for a given reference and hypothesis.\n",
    "    \n",
    "    :param reference: List of reference translations (list of strings)\n",
    "    :param hypothesis: The hypothesis translation (a single string)\n",
    "    :return: A dictionary containing BLEU and chrF++ scores\n",
    "    \"\"\"\n",
    "    # Ensure reference is wrapped in a list as sacrebleu expects a list of references\n",
    "    # bleu_score = sacrebleu.corpus_bleu(hypothesis, [reference]).score\n",
    "    # bleu_score = sacrebleu.corpus_bleu(hypothesis, [reference], tokenize=\"13a\", lowercase=True).score\n",
    "    bleu_score=metric.compute(predictions=[hypothesis], references=[reference])\n",
    "    chrf_score = sacrebleu.corpus_chrf(hypothesis, [reference]).score\n",
    "\n",
    "    return {\"bleu_score\": bleu_score['score'],\"chrF++\": chrf_score}\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "output_data=[]\n",
    "import tqdm\n",
    "path=\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs.txt\"\n",
    "f=open(path, \"r\").read().split(\"\\n\")\n",
    "for x in tqdm.tqdm(f[:100]):\n",
    "    xx=x.split(\"\\t\")\n",
    "    sentence_eng=xx[0]\n",
    "    sentence_spa=xx[1]\n",
    "    COD_prompt,result_json_temp = process_sentence(sentence_eng)\n",
    "    spa_tran=translate_cod_prompt(COD_prompt,sentence_eng)\n",
    "    spa_tran_direct=direct_translate(sentence_eng)\n",
    "    back_tran=back_translate(spa_tran)\n",
    "    back_tran_direct=back_translate(spa_tran_direct)\n",
    "    reference_text = [sentence_eng]\n",
    "    hypothesis_text = back_tran\n",
    "    scores_cod_prompt = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    hypothesis_text = back_tran_direct\n",
    "    scores_direct = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    output_data.append({\n",
    "        \"Original_English_sentence\": sentence_eng,\n",
    "        \"Original_Spanish_sentence\": sentence_spa,\n",
    "        \"COD_prompt\": COD_prompt,\n",
    "        \"spanish_translation\": spa_tran,\n",
    "        \"spanish_translation_direct\": spa_tran_direct,\n",
    "        \"back_translation\": back_tran,\n",
    "        \"back_translation_direct\": back_tran_direct,\n",
    "        \"scores_cod_prompt(bleu and chrf)\": scores_cod_prompt,\n",
    "        \"scores_direct(bleu and chrf)\": scores_direct\n",
    "    })\n",
    "    # print(scores_cod_prompt,scores_direct)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(output_data)\n",
    "\n",
    "df.to_excel(\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs_NLLB.xlsx\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## additional dictionary added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mshahidul/miniconda3/envs/unsloth_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.07s/it]\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import json\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import OpenAI  # Assuming OpenAI API is set up\n",
    "\n",
    "# Database connection details\n",
    "db_config = {\n",
    "    'host': '172.16.34.1',\n",
    "    'port': 3307,\n",
    "    'user': 'umls',\n",
    "    'password': 'umls',\n",
    "    'database': 'umls2024'\n",
    "}\n",
    "\n",
    "# Define the NLLB-200 model\n",
    "model_name = \"facebook/nllb-200-3.3B\"\n",
    "cache_directory = \"/data/data_user_alpha/public_models\"\n",
    "\n",
    "# List of target languages\n",
    "languages = {\n",
    "    \"French\": \"fra_Latn\",\n",
    "    \"German\": \"deu_Latn\",\n",
    "    \"Portuguese\": \"por_Latn\",\n",
    "    \"Spanish\": 'spa_Latn'\n",
    "}\n",
    "\n",
    "# Load translation model\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_directory, torch_dtype=torch.float16)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_directory)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# OpenAI API Client\n",
    "client = OpenAI(api_key=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract medical and non-medical keywords from the given sentence. return it as json format without extra things.\"},\n",
    "                  {\"role\": \"user\", \"content\": f\"{sentence}\"}],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    # print(response.choices[0].message.content)\n",
    "    keywords = json.loads(response.choices[0].message.content)\n",
    "    return keywords  # Expected format: {\"medical\": [\"keyword1\", \"keyword2\"], \"non_medical\": [\"keyword3\", \"keyword4\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_umls(keyword):\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**db_config)\n",
    "        cursor = connection.cursor(dictionary=True)\n",
    "        cursor.execute(\"SELECT CUI FROM MRCONSO WHERE STR LIKE %s LIMIT 1\", (f\"%{keyword}%\",))\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            return None\n",
    "        cui = result[\"CUI\"]\n",
    "\n",
    "        cursor.execute(\"SELECT LAT, STR FROM MRCONSO WHERE CUI = %s AND LAT IN (%s, %s, %s)\",\n",
    "                       (cui, 'FRE', 'POR', 'GER'))\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        translations = {row['LAT']: row['STR'] for row in rows}\n",
    "        return translations\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Database error: {err}\")\n",
    "    finally:\n",
    "        if connection.is_connected():\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate_non_medical(keyword):\n",
    "    translations = {}\n",
    "    for language, lang_code in languages.items():\n",
    "        output = translator(keyword, src_lang=\"eng_Latn\", tgt_lang=lang_code, max_length=400)\n",
    "        translations[language] = output[0]['translation_text']\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_chained_format(dictionary, src_lang, target_lang):\n",
    "    chain_umls = []\n",
    "    chain_nllb = []\n",
    "\n",
    "    # Process UMLS translations\n",
    "    for word, translations in dictionary[\"medical\"][\"UMLS\"].items():\n",
    "        formatted_translations = []\n",
    "        for lang, translation in translations.items():\n",
    "            formatted_translations.append(f\"{word} in {lang} is '{translation}'\")\n",
    "        chain_umls.append(\". \".join(formatted_translations))\n",
    "    \n",
    "    # Process NLLB translations\n",
    "    for word, translations in dictionary[\"medical\"][\"NLLB\"].items():\n",
    "        formatted_translations = []\n",
    "        for lang, translation in translations.items():\n",
    "            formatted_translations.append(f\"{word} in {lang} is '{translation}'\")\n",
    "        chain_nllb.append(\". \".join(formatted_translations))\n",
    "    \n",
    "    chained_text = \"UMLS Translations: \" + \". \".join(chain_umls) + \". \" + \"NLLB Translations: \" + \". \".join(chain_nllb) + \".\"\n",
    "    return chained_text\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    keywords = extract_keywords(sentence)\n",
    "    medical_translations_umls = {}\n",
    "    medical_translations_nllb = {}\n",
    "    non_medical_translations = {}\n",
    "\n",
    "    # Process medical keywords\n",
    "    for keyword in keywords[\"medical_keywords\"]:\n",
    "        translation = search_umls(keyword)\n",
    "        if translation:  # If found in UMLS\n",
    "            medical_translations_umls[keyword] = translation\n",
    "        \n",
    "        # Translate using NLLB regardless\n",
    "        medical_translations_nllb[keyword] = {\n",
    "            lang: translator(keyword, src_lang=\"eng_Latn\", tgt_lang=code, max_length=400)[0]['translation_text'] \n",
    "            for lang, code in languages.items()\n",
    "        }\n",
    "    \n",
    "    # Process non-medical keywords (always translated via NLLB)\n",
    "    for keyword in keywords[\"non_medical_keywords\"]:\n",
    "        non_medical_translations[keyword] = translate_non_medical(keyword)\n",
    "\n",
    "    result_json_temp = {\n",
    "        \"medical\": {\n",
    "            \"UMLS\": medical_translations_umls,\n",
    "            \"NLLB\": medical_translations_nllb\n",
    "        },\n",
    "        \"non_medical\": non_medical_translations\n",
    "    }\n",
    "\n",
    "    src_language = \"English\"\n",
    "    target_language = \"Spanish\"\n",
    "    chained_output = convert_to_chained_format(result_json_temp, src_language, target_language)\n",
    "\n",
    "    return {\n",
    "        \"chained_output\": chained_output,\n",
    "        \"translations\": result_json_temp\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans=process_sentence(\"Bariatric surgery is done when diet and exercise haven't worked or when you have serious health problems because of your weight.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"UMLS Translations: diet in POR is 'Paraoxônio'. diet in FRE is 'Paraoxon'. diet in GER is 'Paraoxon'. exercise in FRE is 'Entraînement respiratoire'. exercise in GER is 'Atemübungen'. exercise in POR is 'Exercícios Respiratórios'. health problems in POR is 'Problemas Internacionais de Saúde'. health problems in GER is 'Internationale Gesundheitsprobleme'. health problems in FRE is 'Problème international de santé'. weight in FRE is 'Nourrisson à petit poids de naissance'. weight in POR is 'Recém-Nascido de Baixo Peso'. weight in GER is 'Neugeborenes, geringes Geburtsgewicht'. NLLB Translations: Bariatric surgery in French is 'Chirurgie bariatrique'. Bariatric surgery in German is 'Bariatrische Chirurgie'. Bariatric surgery in Portuguese is 'Cirurgia bariátrica'. Bariatric surgery in Spanish is 'Cirugía bariátrica'. diet in French is 'régime alimentaire'. diet in German is 'Ernährung'. diet in Portuguese is 'dieta'. diet in Spanish is 'dieta y'. exercise in French is 'exercice physique'. exercise in German is 'Übung'. exercise in Portuguese is 'exercício'. exercise in Spanish is 'ejercicio'. health problems in French is 'problèmes de santé'. health problems in German is 'Gesundheitsprobleme'. health problems in Portuguese is 'problemas de saúde'. health problems in Spanish is 'problemas de salud'. weight in French is 'poids'. weight in German is 'Gewicht'. weight in Portuguese is 'peso'. weight in Spanish is 'peso'.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans['chained_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [17:43<00:00, 10.64s/it]\n"
     ]
    }
   ],
   "source": [
    "def translate_cod_prompt(COD_prompt,sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"{COD_prompt}\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from English into Spanish: {sentence}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    ans=(response.choices[0].message.content)\n",
    "    return ans\n",
    "\n",
    "## Direct translate in Spanish\n",
    "def direct_translate(sentence):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from English into Spanish: {sentence}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    return (response.choices[0].message.content)\n",
    "\n",
    "\n",
    "## Back translate in English\n",
    "def back_translate(spa_tran):\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                # {\"role\": \"system\", \"content\": f\"\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Translate the following text from Spanish into English: {spa_tran}\"}],\n",
    "            temperature=0.5\n",
    "        )\n",
    "    return (response.choices[0].message.content)\n",
    "\n",
    "## Evaluation\n",
    "import sacrebleu\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "def compute_bleu_chrf(reference, hypothesis):\n",
    "    \"\"\"\n",
    "    Computes the BLEU and chrF++ scores for a given reference and hypothesis.\n",
    "    \n",
    "    :param reference: List of reference translations (list of strings)\n",
    "    :param hypothesis: The hypothesis translation (a single string)\n",
    "    :return: A dictionary containing BLEU and chrF++ scores\n",
    "    \"\"\"\n",
    "    # Ensure reference is wrapped in a list as sacrebleu expects a list of references\n",
    "    # bleu_score = sacrebleu.corpus_bleu(hypothesis, [reference]).score\n",
    "    # bleu_score = sacrebleu.corpus_bleu(hypothesis, [reference], tokenize=\"13a\", lowercase=True).score\n",
    "    bleu_score=metric.compute(predictions=[hypothesis], references=[reference])\n",
    "    chrf_score = sacrebleu.corpus_chrf(hypothesis, [reference]).score\n",
    "\n",
    "    return {\"bleu_score\": bleu_score['score'],\"chrF++\": chrf_score}\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "output_data=[]\n",
    "import tqdm\n",
    "path=\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs.txt\"\n",
    "f=open(path, \"r\").read().split(\"\\n\")\n",
    "for x in tqdm.tqdm(f[:100]):\n",
    "    xx=x.split(\"\\t\")\n",
    "    sentence_eng=xx[0]\n",
    "    sentence_spa=xx[1]\n",
    "    COD_prompt = process_sentence(sentence_eng)\n",
    "    spa_tran=translate_cod_prompt(COD_prompt['chained_output'],sentence_eng)\n",
    "    spa_tran_direct=direct_translate(sentence_eng)\n",
    "    back_tran=back_translate(spa_tran)\n",
    "    back_tran_direct=back_translate(spa_tran_direct)\n",
    "    reference_text = [sentence_eng]\n",
    "    hypothesis_text = back_tran\n",
    "    scores_cod_prompt = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    hypothesis_text = back_tran_direct\n",
    "    scores_direct = compute_bleu_chrf(reference_text, hypothesis_text)\n",
    "    output_data.append({\n",
    "        \"Original_English_sentence\": sentence_eng,\n",
    "        \"Original_Spanish_sentence\": sentence_spa,\n",
    "        \"COD_prompt\": COD_prompt,\n",
    "        \"spanish_translation\": spa_tran,\n",
    "        \"spanish_translation_direct\": spa_tran_direct,\n",
    "        \"back_translation\": back_tran,\n",
    "        \"back_translation_direct\": back_tran_direct,\n",
    "        \"scores_cod_prompt(bleu and chrf)\": scores_cod_prompt,\n",
    "        \"scores_direct(bleu and chrf)\": scores_direct\n",
    "    })\n",
    "    # print(scores_cod_prompt,scores_direct)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(output_data)\n",
    "\n",
    "# df.to_excel(\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs_gpt4_mini.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_pairs_gpt4_mini_NLLB.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score (COD-Prompt-Based Translation): 52.46\n",
      "Average chrF++ Score (COD-Prompt-Based Translation): 2.03\n",
      "Average BLEU Score (Direct Translation): 51.93\n",
      "Average chrF++ Score (Direct Translation): 1.84\n"
     ]
    }
   ],
   "source": [
    "# Compute average scores for COD-prompt-based and direct translations\n",
    "total_bleu_cod = 0\n",
    "total_chrf_cod = 0\n",
    "total_bleu_direct = 0\n",
    "total_chrf_direct = 0\n",
    "count = len(output_data)\n",
    "\n",
    "for entry in output_data:\n",
    "    total_bleu_cod += entry[\"scores_cod_prompt(bleu and chrf)\"][\"bleu_score\"]\n",
    "    total_chrf_cod += entry[\"scores_cod_prompt(bleu and chrf)\"][\"chrF++\"]\n",
    "    total_bleu_direct += entry[\"scores_direct(bleu and chrf)\"][\"bleu_score\"]\n",
    "    total_chrf_direct += entry[\"scores_direct(bleu and chrf)\"][\"chrF++\"]\n",
    "\n",
    "# Calculate averages\n",
    "avg_bleu_cod = total_bleu_cod / count\n",
    "avg_chrf_cod = total_chrf_cod / count\n",
    "avg_bleu_direct = total_bleu_direct / count\n",
    "avg_chrf_direct = total_chrf_direct / count\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average BLEU Score (COD-Prompt-Based Translation): {avg_bleu_cod:.2f}\")\n",
    "print(f\"Average chrF++ Score (COD-Prompt-Based Translation): {avg_chrf_cod:.2f}\")\n",
    "print(f\"Average BLEU Score (Direct Translation): {avg_bleu_direct:.2f}\")\n",
    "print(f\"Average chrF++ Score (Direct Translation): {avg_chrf_direct:.2f}\")\n",
    "\n",
    "# Save the scores to a file\n",
    "evaluation_results = {\n",
    "    \"Average BLEU (COD-Prompt-Based Translation)\": avg_bleu_cod,\n",
    "    \"Average chrF++ (COD-Prompt-Based Translation)\": avg_chrf_cod,\n",
    "    \"Average BLEU (Direct Translation)\": avg_bleu_direct,\n",
    "    \"Average chrF++ (Direct Translation)\": avg_chrf_direct\n",
    "}\n",
    "\n",
    "df_scores = pd.DataFrame([evaluation_results])\n",
    "df_scores.to_excel(\"/home/mshahidul/project1/data2/extracted_files/eng_spa_pairs/medlineplus_translation_scores.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
