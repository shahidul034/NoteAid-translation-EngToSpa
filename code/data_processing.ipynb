{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eng_spa_pairs.tar.gz',\n",
       " 'medlineplus_alignment_result.tar.gz',\n",
       " 'EHR_data.tar.gz',\n",
       " 'README',\n",
       " 'extracted_files']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/data/data_user/annotations/machine_translation\"\n",
    "import os\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "# import os\n",
    "\n",
    "# # Define the path to the tar.gz file\n",
    "# tar_gz_path = \"/data/data_user/annotations/machine_translation/EHR_data.tar.gz\"\n",
    "\n",
    "# # Extract the contents of the tar.gz file\n",
    "# with tarfile.open(tar_gz_path, \"r:gz\") as tar:\n",
    "#     tar.extractall(path=\"/home/mshahidul/project1/all_tran_data/data3\")\n",
    "\n",
    "# # List the extracted files\n",
    "# extracted_files_path = \"/home/mshahidul/project1/all_tran_data/data3\"\n",
    "# extracted_files = os.listdir(extracted_files_path)\n",
    "# print(extracted_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eng_spa_pairs.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144155"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/home/mshahidul/project1/all_tran_data/data1/eng_spa_pairs/medlineplus_pairs.txt\"\n",
    "f = open(path, \"r\").read().split(\"\\n\")\n",
    "len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=[]\n",
    "for x in f[100:]:\n",
    "    xx=x.split(\"\\t\")\n",
    "    try:\n",
    "        data1.append(\n",
    "            {\n",
    "                \"english\":xx[0],\n",
    "                \"spain\":xx[1]\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/mshahidul/project1/all_tran_data/data1/eng_spa_pairs/umass_ehr_pairs.txt\"\n",
    "f = open(path, \"r\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in f:\n",
    "    xx=x.split(\"\\t\")\n",
    "    try:\n",
    "        data1.append(\n",
    "            {\n",
    "                \"english\":xx[0],\n",
    "                \"spain\":xx[1]\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## medlineplus_alignment_result.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/mshahidul/project1/all_tran_data/data2/alignment_result\"\n",
    "data2=[]\n",
    "import os\n",
    "for x in os.listdir(path):\n",
    "    f=open(path+\"/\"+x,\"r\").read().split(\"\\n\")\n",
    "    ## data format\n",
    "    # l1: Poisoning is caused by swallowing, injecting, breathing in, or otherwise being exposed to a harmful substance.\n",
    "    # l2: Una intoxicación (envenenamiento) es causada por la ingestión, inyección, inhalación o cualquier exposición a una sustancia dañina.\n",
    "\n",
    "    for xx in range(0,len(f)-1,2):\n",
    "        data2.append(\n",
    "            {\n",
    "                \"english\":f[xx].split(\"l1: \")[1],\n",
    "                \"spain\":f[xx+1].split(\"l2: \")[1]\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144154"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143760"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to /home/mshahidul/project1/all_tran_data/filtered_alignment_result.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define paths\n",
    "alignment_path = \"/home/mshahidul/project1/all_tran_data/data2/alignment_result\"\n",
    "sampled_file_path = \"/home/mshahidul/project1/all_tran_data/Sampled_100_MedlinePlus_eng_spanish_pair.json\"\n",
    "output_file_path = \"/home/mshahidul/project1/all_tran_data/filtered_alignment_result.json\"\n",
    "\n",
    "# Load sampled data\n",
    "with open(sampled_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    sampled_data = json.load(json_file)\n",
    "\n",
    "# Convert sampled_data to a set for quick lookup\n",
    "sampled_set = {(item[\"english\"], item[\"spanish\"]) for item in sampled_data}\n",
    "\n",
    "# Extract and filter data2\n",
    "data2 = []\n",
    "for filename in os.listdir(alignment_path):\n",
    "    with open(os.path.join(alignment_path, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.read().split(\"\\n\")\n",
    "\n",
    "        for i in range(0, len(lines) - 1, 2):\n",
    "            eng_text = lines[i].split(\"l1: \")[1]\n",
    "            spn_text = lines[i + 1].split(\"l2: \")[1]\n",
    "            entry = {\"english\": eng_text, \"spanish\": spn_text}\n",
    "\n",
    "            # Add to data2 only if it's not in sampled_set\n",
    "            if (eng_text, spn_text) not in sampled_set:\n",
    "                data2.append(entry)\n",
    "\n",
    "# Save filtered data2\n",
    "with open(output_file_path, 'w', encoding='utf-8') as json_out:\n",
    "    json.dump(data2, json_out, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Filtered data saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'Pyloroplasty is a surgical procedure to widen the opening in the lower part of the stomach (pylorus) so that the stomach contents can empty into the small intestine (duodenum).',\n",
       " 'spanish': 'Es un procedimiento quirúrgico para ensanchar la abertura en la parte inferior del estómago (píloro), de manera que los contenidos estomacales se puedan vaciar al intestino delgado (duodeno).'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EHR_data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/mshahidul/project1/all_tran_data/data3/EHR/google_result\"\n",
    "import os\n",
    "for x in os.listdir(path):\n",
    "    f=open(path+\"/\"+x,\"r\").read().split(\"\\n\")\n",
    "    \n",
    "\n",
    "    for xx in range(0,len(f)-1,3):\n",
    "        data3.append(\n",
    "            {\n",
    "                \"english\":f[xx].split(\"l1: \")[1],\n",
    "                \"spain\":f[xx+1].split(\"l2: \")[1]\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/mshahidul/project1/all_tran_data/data3/EHR/moses_result\"\n",
    "for x in os.listdir(path):\n",
    "    f=open(path+\"/\"+x,\"r\").read().split(\"\\n\")\n",
    "\n",
    "    for xx in range(0,len(f)-1,3):\n",
    "        data3.append(\n",
    "            {\n",
    "                \"english\":f[xx].split(\"l1: \")[1],\n",
    "                \"spain\":f[xx+1].split(\"l2: \")[1]\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data3 has been saved to data3.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert data3 to a DataFrame\n",
    "df_data3 = pd.DataFrame(data3)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df_data3.to_excel('/home/mshahidul/project1/all_tran_data/data3.xlsx', index=False)\n",
    "\n",
    "print(\"data3 has been saved to data3.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulate all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data1+data2+data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=[x['english'] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"/home/mshahidul/project1/all_tran_data/eng_spa_all_pairs.json\", \"w\") as outfile:\n",
    "#     json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## json load\n",
    "# import json\n",
    "# with open(\"/home/mshahidul/project1/all_tran_data/eng_spa_all_pairs.json\") as f:\n",
    "#     data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 1000 sentences with varied lengths to MedlinePlus.txt\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "output_file = 'MedlinePlus.txt'\n",
    "total_samples = 1000\n",
    "sentences = [x['english'] for x in data2]\n",
    "\n",
    "# Here, we use a simple binning: every 5 words constitutes a bin.\n",
    "bins = defaultdict(list)\n",
    "for sentence in sentences:\n",
    "    word_count = len(sentence.split())\n",
    "    bin_key = word_count // 5  # adjust the divisor to change bin width if desired\n",
    "    bins[bin_key].append(sentence)\n",
    "\n",
    "# Determine the number of bins and how many samples per bin (using equal allocation)\n",
    "num_bins = len(bins)\n",
    "samples_per_bin = total_samples // num_bins\n",
    "\n",
    "sampled_sentences = []\n",
    "\n",
    "# For each bin, sample up to samples_per_bin sentences.\n",
    "for bin_key, sents in bins.items():\n",
    "    if len(sents) <= samples_per_bin:\n",
    "        # If the bin has fewer sentences than desired, take them all.\n",
    "        sampled_sentences.extend(sents)\n",
    "    else:\n",
    "        sampled_sentences.extend(random.sample(sents, samples_per_bin))\n",
    "\n",
    "# If we haven't reached 1,000 sentences yet, sample additional sentences randomly\n",
    "if len(sampled_sentences) < total_samples:\n",
    "    remaining_needed = total_samples - len(sampled_sentences)\n",
    "    # Get the remaining sentences that were not sampled\n",
    "    already_sampled = set(sampled_sentences)\n",
    "    remaining = [s for s in sentences if s not in already_sampled]\n",
    "    if len(remaining) < remaining_needed:\n",
    "        sampled_sentences.extend(remaining)\n",
    "    else:\n",
    "        sampled_sentences.extend(random.sample(remaining, remaining_needed))\n",
    "elif len(sampled_sentences) > total_samples:\n",
    "    sampled_sentences = random.sample(sampled_sentences, total_samples)\n",
    "\n",
    "# Write the sampled sentences to the output file\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for sentence in sampled_sentences:\n",
    "        f.write(sentence + \"\\n\")\n",
    "\n",
    "print(f\"Successfully saved {len(sampled_sentences)} sentences with varied lengths to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from data2\n",
    "english_to_spanish = {item['english']: item['spain'] for item in data2}\n",
    "\n",
    "# Find the corresponding Spanish sentences for sampled_sentences\n",
    "sampled_spanish_sentences = [{\"english\":sentence,\"spanish\":english_to_spanish[sentence]} for sentence in sampled_sentences if sentence in english_to_spanish]\n",
    "\n",
    "# Print the corresponding Spanish sentences\n",
    "for sentence in sampled_spanish_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled translations saved successfully to /home/mshahidul/project1/code/Sampled_1000_MedlinePlus_eng_spanish_pair.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the output file path\n",
    "output_file = \"/home/mshahidul/project1/code/Sampled_1000_MedlinePlus_eng_spanish_pair.json\"\n",
    "\n",
    "# Save to JSON file\n",
    "with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(sampled_spanish_sentences, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Sampled translations saved successfully to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 100 sentence pairs to /home/mshahidul/project1/code/Sampled_100_MedlinePlus_eng_spanish_pair.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"/home/mshahidul/project1/all_tran_data/Sampled_1000_MedlinePlus_eng_spanish_pair.json\"\n",
    "output_file = \"/home/mshahidul/project1/code/Sampled_100_MedlinePlus_eng_spanish_pair.json\"\n",
    "\n",
    "total_samples = 100  # Number of samples needed\n",
    "\n",
    "# Load dataset\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract sentences\n",
    "sentences = [(x['english'], x['spanish']) for x in data]\n",
    "\n",
    "# Bin sentences based on English sentence length\n",
    "bins = defaultdict(list)\n",
    "for eng, span in sentences:\n",
    "    word_count = len(eng.split())\n",
    "    bin_key = word_count // 5  # adjust bin width as needed\n",
    "    bins[bin_key].append((eng, span))\n",
    "\n",
    "# Determine number of bins and samples per bin\n",
    "num_bins = len(bins)\n",
    "samples_per_bin = total_samples // num_bins\n",
    "\n",
    "sampled_pairs = []\n",
    "\n",
    "# Sample from each bin\n",
    "for bin_key, pairs in bins.items():\n",
    "    if len(pairs) <= samples_per_bin:\n",
    "        sampled_pairs.extend(pairs)\n",
    "    else:\n",
    "        sampled_pairs.extend(random.sample(pairs, samples_per_bin))\n",
    "\n",
    "# If additional samples are needed\n",
    "if len(sampled_pairs) < total_samples:\n",
    "    remaining_needed = total_samples - len(sampled_pairs)\n",
    "    already_sampled = set(sampled_pairs)\n",
    "    remaining = [pair for pair in sentences if pair not in already_sampled]\n",
    "    if len(remaining) < remaining_needed:\n",
    "        sampled_pairs.extend(remaining)\n",
    "    else:\n",
    "        sampled_pairs.extend(random.sample(remaining, remaining_needed))\n",
    "elif len(sampled_pairs) > total_samples:\n",
    "    sampled_pairs = random.sample(sampled_pairs, total_samples)\n",
    "\n",
    "# Save the sampled dataset\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump([{\"english\": eng, \"spanish\": span} for eng, span in sampled_pairs], f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Successfully saved {len(sampled_pairs)} sentence pairs to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
