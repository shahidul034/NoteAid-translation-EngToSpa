
## Phi-4 (Without Finetune) (Alpaca)
| **Context Information**                                               | **Metric**         | **Mean (μ)** | **Std Dev (σ)** | **Std Error (SE)** | **95% CI**                  |
|------------------------------------------------------------------------|--------------------|--------------|-----------------|--------------------|------------------------------|
| Translation dictionary based on UMLS                                   | BLEU               | 36.4711      | 0.0895          | 0.0400             | (36.3926, 36.5495)           |
|                                                                        | CHRF++             | 20.7827      | 0.0630          | 0.0282             | (20.7275, 20.8379)           |
|                                                                        | COMET              | 0.8325       | 0.0013          | 0.0006             | (0.8314, 0.8337)             |
| Direct translation without context                                     | BLEU               | 18.9240      | 0.1409          | 0.0630             | (18.8005, 19.0475)           |
|                                                                        | CHRF++             | 9.1569       | 0.2029          | 0.0907             | (8.9790, 9.3347)             |
|                                                                        | COMET              | 0.6341       | 0.0052          | 0.0023             | (0.6296, 0.6387)             |
| Multilingual translations of each concept obtained from GPT-4o Mini    | BLEU               | 24.6117      | 0.0765          | 0.0342             | (24.5447, 24.6788)           |
|                                                                        | CHRF++             | 12.6683      | 0.1240          | 0.0555             | (12.5595, 12.7770)           |
|                                                                        | COMET              | 0.7880       | 0.0011          | 0.0005             | (0.7870, 0.7890)             |
| Synonyms of each concept derived from GPT-4o Mini                      | BLEU               | 32.0276      | 0.6026          | 0.2695             | (31.4994, 32.5558)           |
|                                                                        | CHRF++             | 20.9581      | 0.5116          | 0.2288             | (20.5097, 21.4065)           |

---
## Qwen2.5 14B (Finetune) (GPT)


| **Context**                                                      | **Metric** | **Mean**  | **Std Dev** | **Std Error** | **CI Lower** | **CI Upper** |
|------------------------------------------------------------------|------------|-----------|-------------|----------------|--------------|--------------|
| Direct translation without context                               | BLEU       | 35.3830   | 0.1300      | 0.0581         | 35.2691      | 35.4970      |
|                                                                  | CHRF+      | 18.5851   | 0.1989      | 0.0890         | 18.4107      | 18.7595      |
|                                                                  | COMET      | 0.8563    | 0.0001      | 0.0001         | 0.8562       | 0.8564       |
| Translation dictionary based on UMLS                             | BLEU       | 35.4518   | 0.4980      | 0.2227         | 35.0153      | 35.8883      |
|                                                                  | CHRF+      | 21.6181   | 0.4335      | 0.1939         | 21.2381      | 21.9981      |
|                                                                  | COMET      | 0.8496    | 0.0020      | 0.0009         | 0.8478       | 0.8514       |
| Multilingual translations of each concept obtained from GPT-4o Mini | BLEU    | 37.8983   | 0.2962      | 0.1324         | 37.6387      | 38.1578      |
|                                                                  | CHRF+      | 23.5851   | 0.1394      | 0.0623         | 23.4629      | 23.7073      |
|                                                                  | COMET      | 0.8588    | 0.0003      | 0.0001         | 0.8585       | 0.8590       |

---

## Qwen2.5 14B (without finetune) (Alpaca)  

---

| **Context Information**                                               | **Metric** | **Mean (μ)** | **Standard Deviation (σ)** | **Standard Error (SE)** | **95% Confidence Interval (CI)**         |
|------------------------------------------------------------------------|------------|--------------|-----------------------------|--------------------------|------------------------------------------|
| **Multilingual translations of each concept obtained from GPT-4o Mini**| BLEU       | 31.5381      | 0.0909                      | 0.0407                   | (31.4584, 31.6178)                        |
|                                                                        | chrF++     | 16.8554      | 0.4368                      | 0.1954                   | (16.4725, 17.2383)                        |
|                                                                        | COMET      | 0.8214       | 0.0031                      | 0.0014                   | (0.8187, 0.8241)                          |
| **Translation dictionary based on UMLS**                               | BLEU       | 31.5182      | 0.4421                      | 0.1977                   | (31.1307, 31.9057)                        |
|                                                                        | chrF++     | 17.8943      | 0.4468                      | 0.1998                   | (17.5026, 18.2859)                        |
|                                                                        | COMET      | 0.8066       | 0.0054                      | 0.0024                   | (0.8018, 0.8113)                          |
| **Direct translation without context**                                 | BLEU       | 31.5937      | 0.1074                      | 0.0481                   | (31.4995, 31.6879)                        |
|                                                                        | chrF++     | 18.0681      | 0.1188                      | 0.0531                   | (17.9639, 18.1723)                        |
|                                                                        | COMET      | 0.8292       | 0.0004                      | 0.0002                   | (0.8288, 0.8295)                          |
| **Synonyms of each concept derived from GPT-4o Mini**                  | BLEU       | 31.4086      | 0.3103                      | 0.1388                   | (31.1365, 31.6806)                        |
|                                                                        | chrF++     | 17.7414      | 0.3499                      | 0.1565                   | (17.4347, 18.0481)                        |
|                                                                        | COMET      | 0.8305       | 0.0022                      | 0.0010                   | (0.8285, 0.8325)                          |

---
## Meta-Llama-3.1-8B-Instruct (finetune) (Alpaca)  

| **Context Information**                                               | **Metric** | **Mean (μ)** | **Standard Deviation (σ)** | **Standard Error (SE)** | **95% Confidence Interval (CI)**         |
|------------------------------------------------------------------------|------------|--------------|-----------------------------|--------------------------|------------------------------------------|
| **Multilingual translations of each concept obtained from GPT-4o Mini**| BLEU       | 34.7799      | 1.0541                      | 0.4714                   | (33.8559, 35.7038)                        |
|                                                                        | chrF++     | 21.3302      | 0.9015                      | 0.4032                   | (20.5400, 22.1204)                        |
|                                                                        | COMET      | 0.8531       | 0.0014                      | 0.0006                   | (0.8519, 0.8544)                          |
| **Translation dictionary based on UMLS**                               | BLEU       | 32.9844      | 0.8037                      | 0.3594                   | (32.2800, 33.6889)                        |
|                                                                        | chrF++     | 21.4367      | 0.9011                      | 0.4030                   | (20.6469, 22.2266)                        |
|                                                                        | COMET      | 0.8455       | 0.0024                      | 0.0011                   | (0.8434, 0.8476)                          |
| **Direct translation without context**                                 | BLEU       | 33.0801      | 1.0192                      | 0.4558                   | (32.1867, 33.9735)                        |
|                                                                        | chrF++     | 21.6181      | 0.4784                      | 0.2140                   | (21.1988, 22.0375)                        |
|                                                                        | COMET      | 0.8461       | 0.0015                      | 0.0007                   | (0.8448, 0.8475)                          |
| **Synonyms of each concept derived from GPT-4o Mini**                  | BLEU       | 33.5307      | 1.2281                      | 0.5492                   | (32.4542, 34.6072)                        |
|                                                                        | chrF++     | 21.9507      | 0.5198                      | 0.2325                   | (21.4951, 22.4063)                        |
|                                                                        | COMET      | 0.8509       | 0.0024                      | 0.0011                   | (0.8489, 0.8530)                          |

---
## Meta-Llama-3.1-8B-Instruct (without finetune) (Alpaca)  


| **Context Information**                                               | **Metric** | **Mean (μ)** | **Standard Deviation (σ)** | **Standard Error (SE)** | **95% Confidence Interval (CI)**         |
|------------------------------------------------------------------------|------------|--------------|-----------------------------|--------------------------|------------------------------------------|
| **Multilingual translations of each concept obtained from GPT-4o Mini**| BLEU       | 27.9658      | 1.2026                      | 0.5378                   | (26.9117, 29.0200)                        |
|                                                                        | chrF++     | 20.9455      | 1.4877                      | 0.6653                   | (19.6415, 22.2495)                        |
|                                                                        | COMET      | 0.7874       | 0.0049                      | 0.0022                   | (0.7831, 0.7917)                          |
| **Translation dictionary based on UMLS**                               | BLEU       | 29.1671      | 0.7036                      | 0.3146                   | (28.5504, 29.7838)                        |
|                                                                        | chrF++     | 19.3669      | 1.4175                      | 0.6339                   | (18.1244, 20.6094)                        |
|                                                                        | COMET      | 0.7991       | 0.0076                      | 0.0034                   | (0.7924, 0.8057)                          |
| **Direct translation without context**                                 | BLEU       | 27.2813      | 1.1677                      | 0.5222                   | (26.2577, 28.3048)                        |
|                                                                        | chrF++     | 15.1596      | 1.3664                      | 0.6111                   | (13.9619, 16.3573)                        |
|                                                                        | COMET      | 0.7878       | 0.0131                      | 0.0059                   | (0.7762, 0.7993)                          |
| **Synonyms of each concept derived from GPT-4o Mini**                  | BLEU       | 28.2573      | 1.1323                      | 0.5064                   | (27.2648, 29.2498)                        |
|                                                                        | chrF++     | 19.5345      | 0.9229                      | 0.4127                   | (18.7256, 20.3434)                        |
|                                                                        | COMET      | 0.7734       | 0.0118                      | 0.0053                   | (0.7630, 0.7838)                          |

---
## Qwen2.5 7B (finetune) (gpt)  


| **Context Information**                                               | **Metric** | **Mean (μ)** | **Standard Deviation (σ)** | **Standard Error (SE)** | **95% Confidence Interval (CI)**         |
|------------------------------------------------------------------------|------------|--------------|-----------------------------|--------------------------|------------------------------------------|
| **Direct translation without context**                                 | BLEU       | 37.7127      | 0.0440                      | 0.0197                   | (37.6741, 37.7512)                        |
|                                                                        | chrF++     | 15.9806      | 0.0819                      | 0.0366                   | (15.9088, 16.0524)                        |
|                                                                        | COMET      | 0.8521       | 0.0005                      | 0.0002                   | (0.8517, 0.8525)                          |
| **Multilingual translations of each concept obtained from GPT-4o Mini**| BLEU       | 38.7952      | 0.1401                      | 0.0627                   | (38.6724, 38.9181)                        |
|                                                                        | chrF++     | 23.1822      | 0.2766                      | 0.1237                   | (22.9398, 23.4247)                        |
|                                                                        | COMET      | 0.8570       | 0.0002                      | 0.0001                   | (0.8568, 0.8573)                          |
| **Translation dictionary based on UMLS**                               | BLEU       | 38.5423      | 0.2461                      | 0.1101                   | (38.3266, 38.7580)                        |
|                                                                        | chrF++     | 23.1491      | 0.2233                      | 0.0999                   | (22.9533, 23.3448)                        |
|                                                                        | COMET      | 0.8532       | 0.0008                      | 0.0004                   | (0.8525, 0.8539)                          |
| **Synonyms of each concept derived from GPT-4o Mini**                  | BLEU       | 38.6538      | 0.2572                      | 0.1150                   | (38.4283, 38.8792)                        |
|                                                                        | chrF++     | 23.7857      | 0.0538                      | 0.0241                   | (23.7385, 23.8329)                        |
|                                                                        | COMET      | 0.8573       | 0.0004                      | 0.0002                   | (0.8570, 0.8576)                          |

---
## Qwen2.5 7B (without finetune) (gpt)  


| **Context Information**                                               | **Metric** | **Mean (μ)** | **Std Dev (σ)** | **Std Error (SE)** | **95% Confidence Interval (CI)**         |
|------------------------------------------------------------------------|------------|--------------|------------------|--------------------|------------------------------------------|
| **Multilingual translations of each concept obtained from GPT-4o Mini**| BLEU       | 33.3799      | 0.0005           | 0.0002             | (33.3795, 33.3803)                        |
|                                                                        | CHRF++     | 20.3273      | 0.0008           | 0.0003             | (20.3266, 20.3279)                        |
|                                                                        | COMET      | 0.8450       | 0.0001           | 0.0000             | (0.8449, 0.8451)                          |
| **Direct translation without context**                                 | BLEU       | 31.8525      | 0.0000           | 0.0000             | (31.8525, 31.8525)                        |
|                                                                        | CHRF++     | 18.8863      | 0.0000           | 0.0000             | (18.8863, 18.8863)                        |
|                                                                        | COMET      | 0.8413       | 0.0000           | 0.0000             | (0.8413, 0.8413)                          |
| **Synonyms of each concept derived from GPT-4o Mini**                  | BLEU       | 31.1615      | 0.0507           | 0.0227             | (31.1171, 31.2059)                        |
|                                                                        | CHRF++     | 19.1172      | 0.0056           | 0.0025             | (19.1122, 19.1221)                        |
|                                                                        | COMET      | 0.8222       | 0.0004           | 0.0002             | (0.8218, 0.8225)                          |
| **Translation dictionary based on UMLS**                               | BLEU       | 24.4723      | 0.0371           | 0.0166             | (24.4398, 24.5048)                        |
|                                                                        | CHRF++     | 16.5740      | 0.0252           | 0.0113             | (16.5519, 16.5962)                        |
|                                                                        | COMET      | 0.7575       | 0.0002           | 0.0001             | (0.7573, 0.7576)                          |

---

## gemma-3-4b-it (finetune) 

| **Context**                          | **Metric** | **Mean**  | **Std Dev** | **Std Error** | **CI Lower** | **CI Upper** |
|---------------------------------------|------------|-----------|-------------|---------------|--------------|--------------|
| Multilingual (GPT-4o Mini)            | BLEU       | 30.2860   | 0.4733      | 0.2117        | 29.8711      | 30.7009      |
|                                       | CHRF+      | 21.0973   | 0.7134      | 0.3191        | 20.4720      | 21.7226      |
|                                       | COMET      | 0.7890    | 0.0030      | 0.0013        | 0.7864       | 0.7917       |
| Direct translation without context    | BLEU       | 30.2933   | 0.4322      | 0.1933        | 29.9144      | 30.6721      |
|                                       | CHRF+      | 20.0370   | 0.6646      | 0.2972        | 19.4544      | 20.6196      |
|                                       | COMET      | 0.8024    | 0.0016      | 0.0007        | 0.8010       | 0.8038       |
| GPT-4o Mini Synonyms                  | BLEU       | 31.4359   | 0.2893      | 0.1294        | 31.1823      | 31.6894      |
|                                       | CHRF+      | 20.3261   | 0.3901      | 0.1744        | 19.9842      | 20.6680      |
|                                       | COMET      | 0.8023    | 0.0006      | 0.0003        | 0.8018       | 0.8029       |
| UMLS Dictionary                       | BLEU       | 31.1249   | 0.3317      | 0.1484        | 30.8341      | 31.4157      |
|                                       | CHRF+      | 20.1438   | 1.0364      | 0.4635        | 19.2354      | 21.0523      |
|                                       | COMET      | 0.7994    | 0.0015      | 0.0007        | 0.7981       | 0.8008       |

---

## gemma-3-4b-it (without finetune) 


| **Context**                          | **Metric** | **Mean**  | **Std Dev** | **Std Error** | **CI Lower** | **CI Upper** |
|---------------------------------------|------------|-----------|-------------|---------------|--------------|--------------|
| Synonyms (GPT-4o Mini)                | BLEU       | 31.4764   | 0.4339      | 0.1941        | 31.0961      | 31.8568      |
|                                       | CHRF+      | 20.7797   | 0.6585      | 0.2945        | 20.2025      | 21.3569      |
|                                       | COMET      | 0.8028    | 0.0009      | 0.0004        | 0.8020       | 0.8036       |
| Direct translation without context    | BLEU       | 30.1866   | 0.3467      | 0.1551        | 29.8827      | 30.4905      |
|                                       | CHRF+      | 19.6613   | 0.3098      | 0.1385        | 19.3897      | 19.9328      |
|                                       | COMET      | 0.8026    | 0.0006      | 0.0003        | 0.8020       | 0.8031       |
| Multilingual (GPT-4o Mini)             | BLEU       | 30.8988   | 0.4514      | 0.2019        | 30.5031      | 31.2945      |
|                                       | CHRF+      | 21.5671   | 0.5060      | 0.2263        | 21.1236      | 22.0106      |
|                                       | COMET      | 0.7942    | 0.0024      | 0.0011        | 0.7921       | 0.7963       |
| Translation Dictionary (UMLS)          | BLEU       | 31.2125   | 0.6748      | 0.3018        | 30.6210      | 31.8040      |
|                                       | CHRF+      | 20.5500   | 0.2534      | 0.1133        | 20.3279      | 20.7721      |
|                                       | COMET      | 0.8002    | 0.0014      | 0.0006        | 0.7990       | 0.8015       |

---


