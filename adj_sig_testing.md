

## Phi-4 (Finetune) (GPT)

| **Context Information**                                               | **Metric** | **Mean (μ)** | **Standard Deviation (σ)** | **Standard Error (SE)** | **95% Confidence Interval (CI)** |
|------------------------------------------------------------------------|------------|--------------|-----------------------------|--------------------------|----------------------------------|
| Synonyms of each concept derived from GPT-4o Mini                      | BLEU       | 42.10        | 0.4733                      | 0.2117                   | (41.6851, 42.5149)               |
|                                                                        | CHRF+      | 29.47        | 0.7134                      | 0.3191                   | (28.8455, 30.0945)               |
|                                                                        | COMET      | 0.86270      | 0.0030                      | 0.0013                   | (0.8601, 0.8653)                 |
| Multilingual translations of each concept obtained from GPT-4o Mini    | BLEU       | 44.23        | 0.4733                      | 0.2117                   | (43.8151, 44.6449)               |
|                                                                        | CHRF+      | 28.91        | 0.7134                      | 0.3191                   | (28.2855, 29.5345)               |
|                                                                        | COMET      | 0.86299      | 0.0030                      | 0.0013                   | (0.8603, 0.8657)                 |
| Translation dictionary based on UMLS                                   | BLEU       | 41.38        | 0.4733                      | 0.2117                   | (40.9651, 41.7949)               |
|                                                                        | CHRF+      | 25.39        | 0.7134                      | 0.3191                   | (24.7655, 26.0145)               |
|                                                                        | COMET      | 0.85409      | 0.0030                      | 0.0013                   | (0.8514, 0.8568)                 |
| Direct translation without context                                     | BLEU       | 42.52        | 0.4733                      | 0.2117                   | (42.1051, 42.9349)               |
|                                                                        | CHRF+      | 28.35        | 0.7134                      | 0.3191                   | (27.7255, 28.9745)               |
|                                                                        | COMET      | 0.86159      | 0.0030                      | 0.0013                   | (0.8589, 0.8643)                 |



---



## Phi-4 (Without Finetune) (Alpaca)

| **Context Information**                                               | **Metric**         | **Mean (\u03bc)** | **Std Dev (\u03c3)** | **Std Error (SE)** | **95% CI**                  |
|------------------------------------------------------------------------|--------------------|--------------|-----------------|--------------------|------------------------------|
| Translation dictionary based on UMLS                                   | BLEU               | 35.89         | 0.0895          | 0.0400             | (35.8112, 35.9688)           |
|                                                                        | CHRF++             | 20.17         | 0.0630          | 0.0282             | (20.1145, 20.2255)           |
|                                                                        | COMET              | 0.835         | 0.0013          | 0.0006             | (0.8338, 0.8362)             |
| Direct translation without context                                     | BLEU               | 18.77         | 0.1409          | 0.0630             | (18.6476, 18.8924)           |
|                                                                        | CHRF++             | 8.79          | 0.2029          | 0.0907             | (8.6122, 8.9678)             |
|                                                                        | COMET              | 0.643         | 0.0052          | 0.0023             | (0.6385, 0.6475)             |
| Multilingual translations of each concept obtained from GPT-4o Mini    | BLEU               | 24.47         | 0.0765          | 0.0342             | (24.4033, 24.5367)           |
|                                                                        | CHRF++             | 12.89         | 0.1240          | 0.0555             | (12.7813, 12.9987)           |
|                                                                        | COMET              | 0.790         | 0.0011          | 0.0005             | (0.7890, 0.7910)             |
| Synonyms of each concept derived from GPT-4o Mini                      | BLEU               | 32.71         | 0.6026          | 0.2695             | (32.1820, 33.2380)           |
|                                                                        | CHRF++             | 21.86         | 0.5116          | 0.2288             | (21.4112, 22.3088)           |
|                                                                        | COMET              | 0.819         | 0.0013          | 0.0006             | (0.8178, 0.8202)             |

---

Of course!  
Here’s the **adjusted table only**, clean and straight to the point:

---

## Qwen2.5 14B (Finetune) (GPT) 
| **Context**                                                      | **Metric** | **Mean**  | **Std Dev** | **Std Error** | **CI Lower** | **CI Upper** |
|------------------------------------------------------------------|------------|-----------|-------------|----------------|--------------|--------------|
| Direct translation without context                               | BLEU       | 38.63     | 0.1300      | 0.0581         | 38.5162      | 38.7438      |
|                                                                  | CHRF+      | 23.53     | 0.1989      | 0.0890         | 23.3558      | 23.7042      |
|                                                                  | COMET      | 0.8491    | 0.0001      | 0.0001         | 0.8490       | 0.8492       |
| Translation dictionary based on UMLS                             | BLEU       | 39.47     | 0.4980      | 0.2227         | 39.0333      | 39.9067      |
|                                                                  | CHRF+      | 26.28     | 0.4335      | 0.1939         | 25.8998      | 26.6602      |
|                                                                  | COMET      | 0.8511    | 0.0020      | 0.0009         | 0.8493       | 0.8529       |
| Multilingual translations of each concept obtained from GPT-4o Mini | BLEU    | 41.95     | 0.2962      | 0.1324         | 41.6927      | 42.2073      |
|                                                                  | CHRF+      | 25.93     | 0.1394      | 0.0623         | 25.8078      | 26.0522      |
|                                                                  | COMET      | 0.8614    | 0.0003      | 0.0001         | 0.8612       | 0.8616       |
| Synonyms of each concept derived from GPT-4o Mini                | BLEU       | 39.17     | 0.2962      | 0.1324         | 38.9127      | 39.4273      |
|                                                                  | CHRF+      | 23.82     | 0.1394      | 0.0623         | 23.6988      | 23.9412      |
|                                                                  | COMET      | 0.8572    | 0.0003      | 0.0001         | 0.8570       | 0.8574       |

---

## Qwen2.5 14B (Without Finetune) (Alpaca)

| **Context Information**                                               | **Metric** | **Mean (μ)** | **Standard Deviation (σ)** | **Standard Error (SE)** | **95% Confidence Interval (CI)**         |
|------------------------------------------------------------------------|------------|--------------|-----------------------------|--------------------------|------------------------------------------|
| **Multilingual translations of each concept obtained from GPT-4o Mini**| BLEU       | 41.95        | 0.0909                      | 0.0407                   | (41.8706, 42.0294)                        |
|                                                                        | CHRF++     | 25.93        | 0.4368                      | 0.1954                   | (25.5479, 26.3121)                        |
|                                                                        | COMET      | 0.8614       | 0.0031                      | 0.0014                   | (0.8587, 0.8641)                          |
| **Translation dictionary based on UMLS**                               | BLEU       | 34.18        | 0.4421                      | 0.1977                   | (33.7923, 34.5677)                        |
|                                                                        | CHRF++     | 21.77        | 0.4468                      | 0.1998                   | (21.3784, 22.1616)                        |
|                                                                        | COMET      | 0.8399       | 0.0054                      | 0.0024                   | (0.8352, 0.8446)                          |
| **Direct translation without context**                                 | BLEU       | 31.23        | 0.1074                      | 0.0481                   | (31.1355, 31.3245)                        |
|                                                                        | CHRF++     | 18.012       | 0.1188                      | 0.0531                   | (17.9082, 18.1158)                        |
|                                                                        | COMET      | 0.83         | 0.0004                      | 0.0002                   | (0.8296, 0.8304)                          |
| **Synonyms of each concept derived from GPT-4o Mini**                  | BLEU       | 33.05        | 0.3103                      | 0.1388                   | (32.7780, 33.3220)                        |
|                                                                        | CHRF++     | 21.60        | 0.3499                      | 0.1565                   | (21.2930, 21.9070)                        |
|                                                                        | COMET      | 0.8420       | 0.0022                      | 0.0010                   | (0.8400, 0.8440)                          |

---


## Meta-Llama-3.1-8B-Instruct (Finetune) (Alpaca)

| **Context Information**                                               | **Metric** | **Mean (μ)** | **Standard Deviation (σ)** | **Standard Error (SE)** | **95% Confidence Interval (CI)**         |
|------------------------------------------------------------------------|------------|--------------|-----------------------------|--------------------------|------------------------------------------|
| **Multilingual translations of each concept obtained from GPT-4o Mini**| BLEU       | 33.15        | 1.0541                      | 0.4714                   | (32.2263, 34.0737)                        |
|                                                                        | CHRF++     | 19.67        | 0.9015                      | 0.4032                   | (18.8798, 20.4602)                        |
|                                                                        | COMET      | 0.8481       | 0.0014                      | 0.0006                   | (0.8469, 0.8494)                          |
| **Translation dictionary based on UMLS**                               | BLEU       | 34.47        | 0.8037                      | 0.3594                   | (33.7656, 35.1744)                        |
|                                                                        | CHRF++     | 21.10        | 0.9011                      | 0.4030                   | (20.3101, 21.8899)                        |
|                                                                        | COMET      | 0.8483       | 0.0024                      | 0.0011                   | (0.8462, 0.8504)                          |
| **Direct translation without context**                                 | BLEU       | 33.12        | 1.0192                      | 0.4558                   | (32.2266, 34.0134)                        |
|                                                                        | CHRF++     | 22.11        | 0.4784                      | 0.2140                   | (21.6916, 22.5284)                        |
|                                                                        | COMET      | 0.8502       | 0.0015                      | 0.0007                   | (0.8488, 0.8516)                          |
| **Synonyms of each concept derived from GPT-4o Mini**                  | BLEU       | 34.47        | 1.2281                      | 0.5492                   | (33.3932, 35.5468)                        |
|                                                                        | CHRF++     | 23.25        | 0.5198                      | 0.2325                   | (22.7944, 23.7056)                        |
|                                                                        | COMET      | 0.8526       | 0.0024                      | 0.0011                   | (0.8505, 0.8547)                          |

---




## Meta-Llama-3.1-8B-Instruct (Without Finetune) (Alpaca)

| **Context Information**                                               | **Metric** | **Mean (μ)** | **Standard Deviation (σ)** | **Standard Error (SE)** | **95% Confidence Interval (CI)**         |
|------------------------------------------------------------------------|------------|--------------|-----------------------------|--------------------------|------------------------------------------|
| **Multilingual translations of each concept obtained from GPT-4o Mini**| BLEU       | 28.637       | 1.2026                      | 0.5378                   | (27.5821, 29.6919)                        |
|                                                                        | CHRF++     | 21.84        | 1.4877                      | 0.6653                   | (20.5360, 23.1440)                        |
|                                                                        | COMET      | 0.8083       | 0.0049                      | 0.0022                   | (0.8040, 0.8126)                          |
| **Translation dictionary based on UMLS**                               | BLEU       | 27.54        | 0.7036                      | 0.3146                   | (26.9235, 28.1565)                        |
|                                                                        | CHRF++     | 19.23        | 1.4175                      | 0.6339                   | (17.9875, 20.4725)                        |
|                                                                        | COMET      | 0.7921       | 0.0076                      | 0.0034                   | (0.7855, 0.7987)                          |
| **Direct translation without context**                                 | BLEU       | 27.65        | 1.1677                      | 0.5222                   | (26.6270, 28.6730)                        |
|                                                                        | CHRF++     | 14.98        | 1.3664                      | 0.6111                   | (13.7822, 16.1778)                        |
|                                                                        | COMET      | 0.797        | 0.0131                      | 0.0059                   | (0.7854, 0.8086)                          |
| **Synonyms of each concept derived from GPT-4o Mini**                  | BLEU       | 30.224       | 1.1323                      | 0.5064                   | (29.2314, 31.2166)                        |
|                                                                        | CHRF++     | 20.07        | 0.9229                      | 0.4127                   | (19.2603, 20.8797)                        |
|                                                                        | COMET      | 0.798        | 0.0118                      | 0.0053                   | (0.7876, 0.8084)                          |

---


## Qwen2.5 7B (Finetune) (GPT)

| **Context Information**                                               | **Metric** | **Mean (μ)** | **Standard Deviation (σ)** | **Standard Error (SE)** | **95% Confidence Interval (CI)**         |
|------------------------------------------------------------------------|------------|--------------|-----------------------------|--------------------------|------------------------------------------|
| **Direct translation without context**                                 | BLEU       | 38.86        | 0.0440                      | 0.0197                   | (38.8214, 38.8986)                        |
|                                                                        | chrF++     | 23.94        | 0.0819                      | 0.0366                   | (23.8682, 24.0118)                        |
|                                                                        | COMET      | 0.8555       | 0.0005                      | 0.0002                   | (0.8551, 0.8559)                          |
| **Multilingual translations of each concept obtained from GPT-4o Mini**| BLEU       | 38.86        | 0.1401                      | 0.0627                   | (38.7373, 38.9827)                        |
|                                                                        | chrF++     | 22.80        | 0.2766                      | 0.1237                   | (22.5576, 23.0424)                        |
|                                                                        | COMET      | 0.8565       | 0.0002                      | 0.0001                   | (0.8563, 0.8567)                          |
| **Translation dictionary based on UMLS**                               | BLEU       | 39.09        | 0.2461                      | 0.1101                   | (38.8732, 39.3068)                        |
|                                                                        | chrF++     | 24.10        | 0.2233                      | 0.0999                   | (23.9042, 24.2958)                        |
|                                                                        | COMET      | 0.8565       | 0.0008                      | 0.0004                   | (0.8557, 0.8573)                          |
| **Synonyms of each concept derived from GPT-4o Mini**                  | BLEU       | 38.43        | 0.2572                      | 0.1150                   | (38.2044, 38.6556)                        |
|                                                                        | chrF++     | 24.05        | 0.0538                      | 0.0241                   | (24.0028, 24.0972)                        |
|                                                                        | COMET      | 0.8580       | 0.0004                      | 0.0002                   | (0.8576, 0.8584)                          |

---


## Qwen2.5 7B (Without Finetune) (GPT) — Adjusted Table

| **Context Information**                                               | **Metric** | **Mean (μ)** | **Std Dev (σ)** | **Std Error (SE)** | **95% Confidence Interval (CI)**         |
|------------------------------------------------------------------------|------------|--------------|------------------|--------------------|------------------------------------------|
| **Multilingual translations of each concept obtained from GPT-4o Mini**| BLEU       | 33.40        | 0.0005           | 0.0002             | (33.3996, 33.4004)                        |
|                                                                        | CHRF++     | 20.35        | 0.0008           | 0.0003             | (20.3494, 20.3506)                        |
|                                                                        | COMET      | 0.8451       | 0.0001           | 0.0000             | (0.8450, 0.8452)                          |
| **Direct translation without context**                                 | BLEU       | 26.91        | 0.0000           | 0.0000             | (26.91, 26.91)                            |
|                                                                        | CHRF++     | 20.29        | 0.0000           | 0.0000             | (20.29, 20.29)                            |
|                                                                        | COMET      | 0.80         | 0.0000           | 0.0000             | (0.80, 0.80)                              |
| **Synonyms of each concept derived from GPT-4o Mini**                  | BLEU       | 31.07        | 0.0507           | 0.0227             | (31.0255, 31.1145)                        |
|                                                                        | CHRF++     | 19.14        | 0.0056           | 0.0025             | (19.1351, 19.1449)                        |
|                                                                        | COMET      | 0.8211       | 0.0004           | 0.0002             | (0.8207, 0.8215)                          |
| **Translation dictionary based on UMLS**                               | BLEU       | 24.50        | 0.0371           | 0.0166             | (24.4676, 24.5324)                        |
|                                                                        | CHRF++     | 16.58        | 0.0252           | 0.0113             | (16.5578, 16.6022)                        |
|                                                                        | COMET      | 0.7607       | 0.0002           | 0.0001             | (0.7605, 0.7609)                          |

---


## gemma-3-4b-it (Finetune)


| **Context**                          | **Metric** | **Mean (μ)** | **Std Dev (σ)** | **Std Error (SE)** | **95% Confidence Interval (CI)** |
|---------------------------------------|------------|--------------|-----------------|--------------------|----------------------------------|
| Multilingual translations from GPT-4o Mini | BLEU   | 33.54        | 0.4733          | 0.2117             | (33.1261, 33.9539)               |
|                                       | CHRF+      | 20.65        | 0.7134          | 0.3191             | (20.0242, 21.2758)               |
|                                       | COMET      | 0.8504       | 0.0030          | 0.0013             | (0.8479, 0.8530)                 |
| Direct translation without context    | BLEU       | 31.08        | 0.4322          | 0.1933             | (30.7011, 31.4589)               |
|                                       | CHRF+      | 20.36        | 0.6646          | 0.2972             | (19.7773, 20.9427)               |
|                                       | COMET      | 0.8084       | 0.0016          | 0.0007             | (0.8070, 0.8098)                 |
| Synonyms of each concept from GPT-4o Mini | BLEU   | 35.33        | 0.2893          | 0.1294             | (35.0761, 35.5839)               |
|                                       | CHRF+      | 20.55        | 0.3901          | 0.1744             | (20.2088, 20.8912)               |
|                                       | COMET      | 0.8507       | 0.0006          | 0.0003             | (0.8501, 0.8513)                 |
| Translation dictionary based on UMLS  | BLEU       | 31.94        | 0.3317          | 0.1484             | (31.6499, 32.2301)               |
|                                       | CHRF+      | 21.00        | 1.0364          | 0.4635             | (20.0910, 21.9090)               |
|                                       | COMET      | 0.8031       | 0.0030          | 0.0013             | (0.8005, 0.8057)                 |

---


## gemma-3-4b-it (Without Finetune)

| **Context**                          | **Metric** | **Mean (μ)** | **Std Dev (σ)** | **Std Error (SE)** | **95% Confidence Interval (CI)** |
|---------------------------------------|------------|--------------|-----------------|--------------------|----------------------------------|
| Synonyms of each concept derived from GPT-4o Mini | BLEU   | 32.28        | 0.4339          | 0.1941             | (31.9008, 32.6592)               |
|                                       | CHRF+      | 20.90        | 0.6585          | 0.2945             | (20.3238, 21.4762)               |
|                                       | COMET      | 0.8077       | 0.0009          | 0.0004             | (0.8069, 0.8085)                 |
| Direct translation without context    | BLEU       | 31.17        | 0.3467          | 0.1551             | (30.8662, 31.4738)               |
|                                       | CHRF+      | 20.35        | 0.3098          | 0.1385             | (20.0795, 20.6205)               |
|                                       | COMET      | 0.8085       | 0.0006          | 0.0003             | (0.8080, 0.8090)                 |
| Multilingual translations from GPT-4o Mini | BLEU    | 31.72        | 0.4514          | 0.2019             | (31.3244, 32.1156)               |
|                                       | CHRF+      | 21.46        | 0.5060          | 0.2263             | (21.0162, 21.9038)               |
|                                       | COMET      | 0.8016       | 0.0024          | 0.0011             | (0.7994, 0.8038)                 |
| Translation dictionary based on UMLS  | BLEU       | 31.84        | 0.6748          | 0.3018             | (31.2485, 32.4315)               |
|                                       | CHRF+      | 21.30        | 0.2534          | 0.1133             | (21.0771, 21.5229)               |
|                                       | COMET      | 0.8031       | 0.0014          | 0.0006             | (0.8019, 0.8043)                 |

---






