{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "\n",
    "def translate_medical_notes(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Create the output file in jsonl format\n",
    "    with jsonlines.open(output_file, mode='w') as writer:\n",
    "        count = 0  # Track the number of entries written\n",
    "        \n",
    "        for entry in data:\n",
    "            output_entry = {\n",
    "                \"Original clinical note\": entry['original_english'],\n",
    "                \"translation\": entry['translated_spanish']\n",
    "            }\n",
    "            writer.write(output_entry)\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            # Stop after writing 3 entries\n",
    "            if count == 3:\n",
    "                break\n",
    "\n",
    "# Example usage\n",
    "translate_medical_notes(\"/project/pi_hongyu_umass_edu/zonghai/hospital_translation/IFT_Metrics/IFT_Metrics/data/NLLB_direct.json\", \n",
    "                        \"/project/pi_hongyu_umass_edu/zonghai/hospital_translation/self-refine/data/tasks/ML/translated_inital.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code to generate fed_data.jsonl:\n",
    "import json\n",
    "import jsonlines\n",
    "def translate_medical_notes(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    output_data = []\n",
    "    for entry in data:\n",
    "        output_entry = {\n",
    "            \"Original clinical note\": entry['english'],\n",
    "            # \"translation\": \"\",\n",
    "            # \"annotations\": {\n",
    "                \n",
    "            #     # \"Clinical usefulness\":[],\n",
    "            #     # \"Clinical Accuracy\":[],\"\n",
    "            #     # \"Overall Clarity\":[],\n",
    "            #     # \"Coverage\":[],\n",
    "            #     # \"Fluency\":[],\n",
    "            #     # \"total_score\"\n",
    "            # }\n",
    "        }\n",
    "        output_data.append(output_entry)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Example usage\n",
    "translate_medical_notes(\"/project/pi_hongyu_umass_edu/zonghai/hospital_translation/self-refine/data/Sampled_100_MedlinePlus_eng_spanish_pair.json\", \"/project/pi_hongyu_umass_edu/zonghai/hospital_translation/self-refine/data/tasks/ML/fed_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data has been written to /project/pi_hongyu_umass_edu/zonghai/hospital_translation/IFT_Metrics/IFT_Metrics/data/gpt4o_self_refine_trans_5.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Define the input and output file paths\n",
    "input_filepath = \"/project/pi_hongyu_umass_edu/zonghai/hospital_translation/self-refine/trans_gpt5.json\"\n",
    "output_filepath = '/project/pi_hongyu_umass_edu/zonghai/hospital_translation/IFT_Metrics/IFT_Metrics/data/gpt4o_self_refine_trans_5.json'\n",
    "original_spa_filepath = '/project/pi_hongyu_umass_edu/zonghai/hospital_translation/self-refine/data/Sampled_100_MedlinePlus_eng_spanish_pair.json'\n",
    "\n",
    "# Function to remove the score part and anything after it from the translation\n",
    "\n",
    "def remove_score_and_after(translation):\n",
    "    return re.sub(r'\\s*\\[score:.*$', '', translation, flags=re.DOTALL).strip()\n",
    "\n",
    "\n",
    "# Read the original Spanish translations JSON file\n",
    "with open(original_spa_filepath, 'r', encoding='utf-8') as infile:\n",
    "    original_spa_data = {entry[\"english\"]: entry[\"spanish\"] for entry in json.load(infile)}\n",
    "\n",
    "# Read the input JSON file\n",
    "with open(input_filepath, 'r', encoding='utf-8') as infile:\n",
    "    data = [json.loads(line) for line in infile]\n",
    "\n",
    "# Process the data to extract the required fields\n",
    "cleaned_data = []\n",
    "for entry in data:\n",
    "    original_eng = entry.get(\"Original clinical note\", \"\")\n",
    "    generated_translation = entry.get(\"generated_translations\", \"\")\n",
    "    cleaned_translation = remove_score_and_after(generated_translation)\n",
    "    original_spa = original_spa_data.get(original_eng, \"\")\n",
    "    cleaned_data.append({\n",
    "        \"original_eng\": original_eng,\n",
    "        \"original_spa\": original_spa,\n",
    "        \"tran_spa\": cleaned_translation\n",
    "    })\n",
    "\n",
    "# Write the cleaned data to the output JSON file\n",
    "with open(output_filepath, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(cleaned_data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Cleaned data has been written to {output_filepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
